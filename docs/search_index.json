[["index.html", "Data science with R: Applied Predictive Modelling Compendium for STAT623 Course overview Learning outcomes and objectives Lecture overview Litterature", " Data science with R: Applied Predictive Modelling Compendium for STAT623 Sondre Hølleland and Kristian Gundersen 2022-05-22 18:15:38 Course overview Learning outcomes and objectives The course presents various advanced methods within data science for predictive modelling and the use of R. Methods for regression, including non-linear regression and generalized additive models, and methods for classification, including trees, boosting and Support Vector Machines, will be examined. The course will focus on practical use in r, without going into details of the mathematical theory of the methods. On completion of the course the student should have the following learning outcomes: Knowledge Knows the basic ideas underpinning carious methods in data science/predictive modelling Skills Can implement various models within data science/predictive modelling in R Use data science methods on real data sets and perform predictions General competence Have an overview of how data science methods can be used to analyze larger data sets Lecture overview Lecture Subject Exercises Datacamp 1 Introduction and short recap of R and Data preprocessing Recap of R Introduction to Regression in R (ch 1-2) 2 Over-fitting and model tuning, selection and evaluation and multiple regression Multiple regression Supervised Learning in R: Regression (ch 1-2) 3 Non-linear regression GAMs Nonlinear modeling in R with GAMs (ch 1-2) 4 Classification methods Supervised Learning in R: Classification (ch 1-3) 5 Decision Trees and Bagged trees Machine Learning with Tree-Based Models in R (ch 1-3) 6 Random forrest and boosting xgboost Machine Learning with Tree-Based Models in R (ch 3-4) 7 Support vector machines Support Vector Machines in R (ch 1-2) 8 Neural Networks Introduction to TensorFlow in R (ch 1-3) 9 Feature selection/Explainable AI Supervised Learning in R: Classification (ch 3, video on automatic feature selection Litterature We will use many different sources for teaching you applied predictive modelling. Kuhn and Johnson (2016) and James et al. (2013) are the main references. References "],["lecture1.html", " 1 Introduction and short recap of R 1.1 Introduction 1.2 Recap of R 1.3 Data preprocessing 1.4 Case study", " 1 Introduction and short recap of R Goals: In this Lecture 1 we will give a introduction to the course and a short recap of using R. 1.1 Introduction This course is called applied predictive modelling. What do we actually mean by these three words? Well, applied is as opposed to theoretical - indicating that this will not be a very theoretical course. We will focus on usage of the different methods that you will learn throughout the course. Predictive modelling means that the overall goal of the modelling is to predict something or make a prediction. Gkisser (1993) defines predictive modelling as “the process by which a model is created or chosen to try to best predict the probability of an outcome”, while Kuhn and Johnson (2016) define it as “the process of developing a mathematical tool or model that generates an accurate prediction” and the give the following examples of types of questions one could be interested in predicting: How many copies will a book sell? Will this customer move their buisness to a different company? How much will my house sell for in the current market? Does a patient have a specific disease? Based on past choices, which movies will interest this viewer? Should I sell this stock? Which people should we match in our online dating service? Is an email spam? Will this patient respond to this therapy? Examples of stakeholders or users of predictive modelling can be insurance companies. They need to quantify individual risk of potential policy holders. If the risk is too high, they may not offer the potential customer insurance or they may use the quantified risk to set the insurance premium. Governments may use predictive models to detect fraud or identify terror suspects. When you go to a grocery store and sign up for some discounts, usually either by creating an account or registering your credit card, your purchase information is being collected and analyzed in an attempt to who you are and what you want. Predictive modelling can often a good thing. When for instance Netflix learns what kind of TV-shows you prefer to watch, they can come up with suggestions for other TV-shows that you are likely to also enjoy. This can often be recognized by statements like “people who liked TV-series A, also liked TV-series B”. For the streaming provider the goal is to keep you as an entertained, satisfied and paying customer, but it is also in your interest to find entertainment that you like. But you have probably also noticed that sometimes, these predictions are quite inaccurate and provide the wrong answer. For instance, you did not like the suggested series or you did not receive an important email because it was wrongly classified as spam by (predictive) spam filter. Kuhn and Johnson (2016) give four reasons for why predictive models fail: Inadequate pre-processing of the data. Inadequate model validation. Unjustified extrapolation. Over-fitting the model to existing data. They also mention the fact that modellers tend to explore realtively few models when searching for predictive realationships. This is usually because the modeler has a preference for a certain type of model (the one he/she has used before and know well). It can also be due to software availability. We will cover (at least) some of these aspects in this course. 1.1.1 Prediction or interpretation? In the examples listed above, the main goal is to predict something and there are likely data available to train a statistical model to do so in most of the cases. Note that we are not so interested in answering questions of why something happens or not. Our primary interest is to accurately predict the probability that something will, or will not, happen. In situations like these, we should not be worried with having interpretative models, and focus our attention on prediction accuracy. If your spam filter classifies an email as spam, you would not care why the filter did so, as long as you receive the emails you care about and not the ones you do not. An interpretative model can be a model for a certain stock’s value that can support statements like “the stock value prediction went up, because the company released information about a big contract.” One can explain why the model behaves the way it does. The alternative is often called a “black box”, meaning that you put your data in on one side of the box and on the other the prediction pops out, without you knowing what happened inside the box. So, our primary interest is prediction accuracy. Cannot interpretability be our secondary target, so we can understand why it works? Kuhn and Johnson (2016) writes that “the unfortunate reality is that as we push towards higher accuracy, models become more complex and their interpretability becomes more difficult”. 1.1.2 Terminology In the predictive modelling terminology there are quite a lot of things that mean the same thing. We will list some terminology here with some explanations (see Kuhn and Johnson (2016, p6)): The terms sample, data point, observation, or instance refer to a single, independent unit of data, e.g. a customer, a patient, a transaction, an individual. Sample can also refer to a collection of data points (a subset). The training set consists of data used to develop the models while the test and validation sets are used solely for evaluating the performance of the final model. The predictors, idendependent variables, attributes, descriptors, or covariates are the data used as input for the prediction equation (e.g. what you feed into the black box). Outcome, dependent variable, target, class, or response refer to the outcome of the event or qunatity that is being predicted (e.g. what comes out of the black box). Continuous data have natural, numeric scales (e.g. blood pressure, price of an item, a persons body mass index, number of bathrooms, etc). Categorical, nominal, attribute or discrete data all mean the same thing. These are data types that take on specific values that have no scale (e.g. credit status (“good” or “bad”) or color (“red”, “white”, “blue”)) Model building, model training, and parameter estimation all refer to the process of using data to determine values of model equations. 1.2 Recap of R 1.2.1 Installing R and Rstudio If you have not already installed the newest version of R and Rstudio on your personal computer, we will now tell you how. This installation guide will be based on Windows, but iOS and Linux are very similar and we will indicate where to deviate. The video below shows the same steps as listed. First we install R: Go to https://cran.uib.no. Select Download R for Windows (or macOS/Linux) (this will be the newest R version). Go to your “Downloads” folder and run the .exe file. Use standard settings throughout the installation steps (click next-next-next-etc.). R is now installed. Now that R is installed, we can install Rstudio: Go to https://www.rstudio.com/products/rstudio/download/. Press “Download” under the Rstudio Desktop - Open source licenece - Free. Press “Download Rstudio for Windows”. I guess if you have macOS/Linux it will detect that automatically. If not, select it manually from the list below. Go to your “Downloads” folder and run the Rstudio.exe file. Use standard settings throughout the installation steps (click next-next-next-etc.). RStudio is now installed. Open RStudio and check that the print-out in the console matches the R version you just installed. At the time when this is written the newest version is 4.1.1 (2021-08-10) “Kick Things”. The printout at the top looks like this: R version 4.1.1 (2021-08-10) – “Kick Things” Copyright (C) 2021 The R Foundation for Statistical Computing Platform: i386-w64-mingw32/i386 (32-bit) If you had R and Rstudio already, but updated to the newest version, remember that you will have to also install all your packages again. 1.2.2 R community and packages Currently, there are over 18 000 packages available on the official CRAN server - open source and free. If you have a problem and need a smart function for solving that, you will most likely find that someone else have had the same problem, already solved it and made an R package that you can use. The large R community is also very active on the net forum stackexchange.com. If you have a programming issue and google it, you will typically end up in one of these forums where someone else has posted a question similar to yours and others from the community has provided a solution. I have been using R for over 10 years now and this strategy for solving programming issues has not failed me yet. Once you have found an R package you want to install, you can install it using the install.packages function. We will use the tidyverse package developed by Wickham et al. (2019) as an example. This is a bundling package built up of many packages, which we will use throughout the course. install.packages(&quot;tidyverse&quot;) To load a package into you R environment, making all its functionality available to you, you can use the library or require functions. The two are quite equal, but if you do not have the package installed require will cast a warning and continue, while library will cast an error and stop. library(tidyverse) As part of the output here, you can see all the packages that tidyverse attaches to your working environment. We will be using many of these. 1.2.3 Datacamp As a part of the course, you will be given access to Datacamp. We will suggest courses to take there in combination with the lectures provided. If you want a more thorough recap of using R, you can take the course Introduction to R or test yourself with the practice module Introduction to R. 1.3 Data preprocessing Data pre-processing techniques typically involve addition, deletion or transformation of training set data. Preparing the data in a good way can make or break the predictive ability of a model. How the predictors enter the model is important. By transforming the data before feeding them to the model one may reduce the impact of data skewness or outliers. This can signifcantly improve the model’s performance. The need for data pre-processing may depend on the method you are using. A tree-based model is notably insensitive to the characteristics of the predictor data, while a linear regression is not. How the predictors are encoded, called Feature engineering, can also have a huge impact on the model performance. There are many ways to encode the same information. Combining multiple predictors can sometimes be more effective than using the individual ones. For example, using the body mass index (BMI = weight/length\\({}^2\\)) instead of using length and weight as separate predictors. The modeler’s understanding of the problem can often help in choosing the most effective encoding. Different ways of encoding a date predictor can be The number of days since a reference date Using the month, year and day of the week as separate predictors The numeric day of the year (julian day) Whether the date was within the school year (as opposed to holiday) 1.4 Case study We will use the same example as Kuhn and Johnson (2016) from the R package install.packages(&quot;AppliedPredictiveModeling&quot;) 1.4.1 Data transformations for individual predictors library(AppliedPredictiveModeling) data(&quot;segmentationOriginal&quot;) segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;) 1.4.2 Centering and scaling The most straightforward and common transformation of data is to center and scale the predictor variables. Let \\(\\textbf{x}\\) be the predictor, \\(\\overline{x}\\) its average value and \\(sd(x)\\) it’s standard deviation. Centering means to substract the average predictor value from all the predictor values (\\(\\textbf{e} = \\textbf{x}-\\overline{x}\\)) and scaling means dividing all values by the empirical standard deviation of the predictor (\\(\\textbf{e}/sd(x)\\)). By centering the predictor, the predictor has zero mean and by scaling, you coerce the value to have a common standard deviation of one. A common term for a centered and scaled predictor is a standarized predictor, indicating that if you do this to all predictors they are all on a common scale. In the Figure 1.1 below, we have simulated 1000 normally distributed observations with mean 22 and standard deviation 4 and plotted a histogram of them (left panel) and a histogram of them standardized (right panel). As you can see, the distribution looks the same, but the scale on the x-axes has changed. library(ggplot2) set.seed(123) x &lt;- rnorm(1000, mean = 22, sd = 4) df &lt;- data.frame(x = c(x, (x-mean(x))/sd(x)), mean = rep(c(22, 0),each = 1000), type = rep(c(&quot;Original&quot;, &quot;Standardized&quot;), each = 1000)) ggplot(data= df,aes(x = x, fill = type)) + geom_histogram() + facet_wrap(~type, scales = &quot;free&quot;) + geom_vline(aes(xintercept = mean), col = 1, lty = 2) + scale_y_continuous(expand =c(0,0)) + theme(legend.title = element_blank(), legend.position = &quot;none&quot;)+ xlab(&quot;&quot;) Figure1.1: Simulated normally distributed variables on original- and standardized scale. 1.4.3 Tranformations to resolve skewness As noted above, standardizing preserves the distribution, but sometimes we need to remove distributional skewness. An un-skewed distribution is roughly symmetric around the mean. A right-skewed distribution has a larger probability of falling on the left side (i.e. small values) than on the right side (large values). We have illustrated left-, right- and un-skewed normal distributions in Figure 1.2. x &lt;- seq(-4,4,by = .1) alpha &lt;- c(-5,0,5) df &lt;- expand.grid(&quot;x&quot; = x,&quot;alpha&quot; = alpha) df &lt;- df %&gt;% mutate(case = ifelse(alpha == -5, &quot;Left skewed&quot;, ifelse(alpha ==5, &quot;Right skewed&quot;, &quot;Unskewed&quot;)), delta = alpha/sqrt(1+alpha^2), omega = 1/sqrt(1-2*delta^2/pi), dens = sn::dsn(x,xi = -omega*delta*sqrt(2/pi), alpha =alpha, omega = omega)) ggplot(df, aes(x = x,y = dens)) + geom_line() + facet_wrap( ~case, scales = &quot;fixed&quot;, nrow = 3, strip.position = &quot;right&quot;) + geom_vline(xintercept = 0, lty = 2) + xlab(&quot;&quot;) + ylab(&quot;Density&quot;) Figure1.2: Skewed normal distributions with mean 0 and standard deviation 1. A rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. One can calculate the skewness statistic and use that as a diagnostic. Is it close to zero, the distribution if roughly symmetric. Is it large, indicates right-skewness and negative values indicates left-skewness. The formula for the sample skewness statistic is given by \\[\\text{skewness} = \\frac{\\sum_{i=1}^n (x_i-\\overline{x})^3}{(n-1)v^{3/2}},\\quad \\text{where}\\quad v = (n-1)^{-1}\\sum_{i=1}^n(x_i-\\overline{x})^2,\\] \\(x\\) is the predictor variable, \\(n\\) the number of values and \\(\\overline{x}\\) is sample mean. Transforming the predictor with the log, square root or inverse may help remove the skew. Alternatively, a Box-Cox transformation can be used. The Box-Cox family of transformations are indexed by a parameter denoted \\(\\lambda\\) and defined by \\[x^\\star = \\begin{cases} \\frac{x^\\lambda-1}\\lambda, &amp; \\text{if }\\lambda\\neq 0\\\\ \\log(x), &amp; \\text{if }\\lambda = 0. \\end{cases}\\] In addition to the log transformation, the family can identify square transformation (\\(\\lambda = 2\\)), square root (\\(\\lambda = 0.5\\)), inverse (\\(\\lambda = -1\\)) and other in-between. By using maximum likelihood estimation on the training data, \\(\\lambda\\) can be estimated for each predictor that contain values greater than zero. References "],["lecture2.html", " 2 Over-fitting and model tuning, selection and evaluation and multiple regression 2.1 Overfitting 2.2 Training, validation and test split 2.3 Multiple regression", " 2 Over-fitting and model tuning, selection and evaluation and multiple regression In this lecture we will cover the terms overfitting, training-, validation- and test sets, model selection and -evaluation. These terms are part of the chargong of predictive modelling and is something we need to get familiar with before learning about specific models. Towards the end of this lecture, we will learn about multiple regression. 2.1 Overfitting Overfitting is the phenomenon when you build a model that fits (almost) “perfectly” to the training set, but when applied to new data the performance is low. For the modeller there is a balance between choosing a model that has a good fit to the training data, but also generalize well to new data. The goal for the model should be to discover the underlying signal and not fit to all the noise. We will illustrate overfitting by an example with simulated data and GAM models (topic for the lecture Lecture 3) set.seed(3) x &lt;- seq(0,2*pi,0.1) z &lt;- sin(x) y &lt;- z + rnorm(mean=0, sd=0.5*sd(z), n=length(x)) df &lt;- cbind.data.frame(x,y,z) p &lt;- ggplot(df, aes(x = x, y = y)) + geom_point() + geom_line(aes(y=z), lwd = .8, col = 2) p In the figure above, the “true” underlying signal is the red curve while the black dots are the observations. If we fit a model with a high level of flexibility, we can make it fit well to the observations. p + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k = 50, sp = 0)) We will come back to the details of the model setup when discussing GAM models in the next lecture, but the point here is that we have “turned off” the likelihood penalization by setting sp=0 in the smoother function s and set the order of the smoother to a high number (k = 50). If we set the order to k = 4 and let the GAM procedure set the penalization itself, we get a much smoother curve that lies closer to the true signal. p + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k = 4)) The first model explains 96.3% of the deviance, while the second explains 81.7%. In a normal situation, we would of course not know the true underlying signal, but overfitting can be suspected when you get such a wiggly prediction curve that seems to follow every little move in the observations. Judging by the fit to the points, the first GAM model is much closer to the observations, but we can check how well it would perform on a new set of data with the same structure. library(mgcv) # package for fitting gam models # Fit model with high level of complexity: fit1 &lt;- gam(y ~ s(x, k= 50, sp = 0), data = df) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x, k = 50, sp = 0) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02501 0.04060 -0.616 0.549 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 49 49 6.895 0.000262 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.823 Deviance explained = 96.3% ## GCV = 0.50314 Scale est. = 0.10382 n = 63 # Fit simpler model: fit2 &lt;- gam(y ~ s(x, k= 4), data = df) summary(fit2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x, k = 4) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02501 0.04238 -0.59 0.557 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 2.988 3 87.32 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.807 Deviance explained = 81.7% ## GCV = 0.12081 Scale est. = 0.11317 n = 63 # simulate new set of data: set.seed(4) newdata &lt;- data.frame(x, z, y = z+rnorm(mean=0, sd=0.5*sd(z), n=length(x))) # Make predictions from the two models: newdata$pred1 &lt;- predict(fit1, newdata=newdata) newdata$pred2 &lt;- predict(fit2, newdata=newdata) # Calculate sum of squared residuals: with(newdata, c(&quot;SS1&quot; = sum((y-pred1)^2), &quot;SS2&quot; = sum((y-pred2)^2))) ## SS1 SS2 ## 14.160736 7.734603 As you can see, the sum of squared residuals is almost twice as high for the complex model when applied to new observations, compared to the simpler model. In a preditive modelling situation, it is the prediction performance that is important and not as much the deviance explained on training data. Below is a figure with the two predicition curves on the new data. Since \\(x\\) is the only input to the models, the two lines are equal to the curves above, but the data is new. # Plot the new data: ggplot(data= newdata, aes(x=x,y=y)) + geom_point() + geom_line(aes(y=pred1, col = &quot;fit1&quot;))+ geom_line(aes(y=pred2, col = &quot;fit2&quot;)) 2.2 Training, validation and test split In order to evaluate a predictive model’s performance, we should test its predictive abilities on data the model has never seen before (during model fitting). Usually, you only have one dataset to start with, so it is common to split the original data into three subsets; a training set, a validation set and a test set. The training set is the data used to “train” the model, meaning estimating its parameters. If your original data is split, this will typically be the biggest portion of the data. One should measure the model performance on the training data, but this should not be used solely for model selection. If your model has a very good fit to the training data, this can often lead to overfitting issues when applying the model to new data. This is one of the reasons for using a validation set. The validation set is used for tackling overfitting and doing model selection. This is a smaller portion of the data not seen during training, which is key. We can therefore measure the models performance on these unseen data and if the model performs similarly as on the traning data, this means it generalizes well to new situations and overfitting is likely not a big issue. If the performance is high in training and low for validation, this is a sign of overfitting. If you are in a scenario where you have many different candidate models, either from different model families or the same model but with different setups, you can use the validation set to select “the best” model according to the performance criteria you have chosen. Once you have found a model that does not overfit to training data and performs well on the validation set, you can apply it to the test set. This should only be used to measure the models performance. In some cases, one sees modellers only splitting the data in two, a training and testing set. If you use the validation set to measure the models performance, this will give a biased estimate of model performance, since you have selected the model based on the validation set. Basically, you will not know whether your model performs better because of changes you made or because it just happened to fit the validation set better. Therefore, we need a separate test set. The performance on the test set should ideally be similar to the performance on the validation set. If it is significantly lower, this can indicate overfitting to the validation set. Figure2.1: Summary of the three-way data split. We will illustrate how this three-way split can be used on an example, after we have learned about multiple regression. 2.3 Multiple regression Multiple regression is an extension of simple linear regression, where more than one covariate is used to predict the value of the dependent variable (the Y). The form of the predictor is a linear combination of the set of explanatory variables, i.e. \\[\\widehat Y_i = \\beta_0 + \\beta_1X_{i1} + \\ldots + \\beta_p X_{ip},\\quad i=1,\\ldots,n,\\] where \\(\\widehat Y_i\\) is the predictor of observation \\(i\\), \\(\\beta_0, \\ldots, \\beta_p\\) are the parameters and \\(X_{\\cdot 1}, \\ldots, X_{\\cdot p}\\) are the vectors of explanatory variables. It is quite common to write the equation above on vector form. Define the vectors \\(\\widehat{\\mathbf{Y}}=(\\widehat Y_1, \\ldots, \\widehat Y_n)&#39;\\) and \\(\\boldsymbol{\\beta} = (\\beta_0, \\ldots, \\beta_p)&#39;\\), and the design matrix \\(\\mathbb X = (\\mathbf{1}, \\mathbf X_{\\cdot 1},\\ldots, \\mathbf X_{\\cdot p})\\). Then the equation above can be written as \\[\\widehat{\\mathbf{Y}} = \\mathbb X\\,\\boldsymbol\\beta.\\] The assumptions are that the residuals, \\(Z_i = Y_i - \\widehat Y_i\\), \\(i=1,\\ldots, n\\) are independent and normally distributed. The explanatory variables should not be correlated. Parameters are estimated using maximum likelihood estimation. Note that we are using capital letters here. This is a statistical convention when we are talking about variables. Once we introduce observations, we switch to small letters for the same quantities. To learn more about multiple regression, take the datacamp course Multiple and Logistic Regression in R. For now you can focus on the multiple regression part. We will simply go on to illustrate usage by an example. 2.3.1 Example In this example, we consider a dataset containing the impact of three advertising medias (youtube, facebook and newspaper) on sales for different companies. The advertising budgets and sales are in thousands of dollars and the advertising experiment has been repeated 200 times. We will use multiple regression to model the relationship between sales and the advertising budgets from the different medias. In the video below we walk you through the example, but you can also read it below. We start by looking at the data: # load data: data(&quot;marketing&quot;, package = &quot;datarium&quot;) head(marketing) ## youtube facebook newspaper sales ## 1 276.12 45.36 83.04 26.52 ## 2 53.40 47.16 54.12 12.48 ## 3 20.64 55.08 83.16 11.16 ## 4 181.80 49.56 70.20 22.20 ## 5 216.96 12.96 70.08 15.48 ## 6 10.44 58.68 90.00 8.64 plot(marketing) From looking at the plot above, it does not seem to be a very strong correlation between the three covariates: youtube, facebook and newspaper. This is based on that the first 3x3 scatters plots seem to be randomly distributed without any clear patterns. Looking at the last row of panels we see the marginal relationships between the covariates and the response; sales. Here it looks like the marginal relationship between youtube and facebook variables is close to linear, while for newspaper it does not look like a linear relationship is well suited. We will therefore use facebook and youtube to predict sales. We will also check if including newspaper can improve the fit. We create a train and a test set, by doing a 80-20 random split (80% for training set - 20% test set): set.seed(123) train.ind &lt;- sample(1:nrow(marketing), nrow(marketing)*.8, replace = FALSE) trainset &lt;- marketing[train.ind, ] testset &lt;- marketing[-train.ind, ] We start by fitting the following model: \\[\\mathrm{sales} = \\beta_0 + \\beta_1\\cdot \\mathrm{youtube} + \\beta_2\\cdot \\mathrm{facebook}+\\beta_3\\cdot \\mathrm{youtube}\\cdot\\mathrm{facebook}\\] This model can be set up by different formula arguments in R. The different model calls below are equivalent. mod1 &lt;- lm(sales ~ youtube + facebook + youtube:facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ youtube + facebook + youtube:facebook, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 mod1 &lt;- lm(sales ~ 1 + youtube + facebook + youtube:facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ 1 + youtube + facebook + youtube:facebook, ## data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 mod1 &lt;- lm(sales ~ youtube * facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ youtube * facebook, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 As you can see from the outputs, all models are equivalent. If you want to learn more about setting the formula argument, check out the helpsite for the formula function: ?stats::formula We will choose the model that has the highest predictive ability. We will therefore suggest several models and choose the one that has the lowest value of Akaike’s information criteria (AIC). We will also evaluate the different models on the test set. mod2 &lt;- lm(sales ~ youtube * facebook * newspaper, data = trainset) mod3 &lt;- lm(sales ~ youtube * facebook + newspaper, data = trainset) mod4 &lt;- lm(sales ~ youtube, data = trainset) mod5 &lt;- lm(sales ~ facebook, data = trainset) mod6 &lt;- lm(sales ~ newspaper, data = trainset) AIC(mod1, mod2, mod3, mod4, mod5, mod6) ## df AIC ## mod1 5 511.4668 ## mod2 9 517.5961 ## mod3 6 512.9814 ## mod4 3 894.2953 ## mod5 3 986.9928 ## mod6 3 1043.6815 We choose the model with lowest AIC value. As you can see from the output above, this is the model we named mod1. Adding newspaper as covariate will provide the model with more information, but the cost of adding more parameters to be estimated is deemed higher than the benefit of including this information in the model according to AIC. We can also use the predictive abilities of the models on the test set to choose model. We will restrict ourselves to the top three model based on AIC and use root mean square error (RMSE) to assess the quality of the predictions. First we evaluate the in-sample prediction. That is, we calculate predictions on the training set and summarize by RMSE. trainset %&gt;% bind_cols( mod1 = predict(mod1, newdata = trainset), mod2 = predict(mod2, newdata = trainset), mod3 = predict(mod3, newdata = trainset) ) %&gt;% pivot_longer(cols = 5:7, names_to = &quot;models&quot;, values_to = &quot;pred&quot;) %&gt;% group_by(models) %&gt;% summarize(RMSE = sqrt(mean((sales-pred)^2))) ## # A tibble: 3 × 2 ## models RMSE ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 1.16 ## 2 mod2 1.15 ## 3 mod3 1.16 Perhaps not very surpising, the most complex model has the lowest in-sample, but the differences seem small. Let’s evaluate the models on the test set as well. predictions = testset %&gt;% bind_cols( mod1 = predict(mod1, newdata = testset), mod2 = predict(mod2, newdata = testset), mod3 = predict(mod3, newdata = testset) ) %&gt;% pivot_longer(cols = 5:7, names_to = &quot;models&quot;, values_to = &quot;pred&quot;) head(predictions) ## # A tibble: 6 × 6 ## youtube facebook newspaper sales models pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 240. 3.12 25.4 12.7 mod1 13.4 ## 2 240. 3.12 25.4 12.7 mod2 13.4 ## 3 240. 3.12 25.4 12.7 mod3 13.4 ## 4 245. 39.5 55.2 22.8 mod1 22.7 ## 5 245. 39.5 55.2 22.8 mod2 22.7 ## 6 245. 39.5 55.2 22.8 mod3 22.7 predictions %&gt;% group_by(models) %&gt;% summarize(RMSE = sqrt(mean((sales-pred)^2))) ## # A tibble: 3 × 2 ## models RMSE ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 0.957 ## 2 mod2 0.916 ## 3 mod3 0.967 We see that the RMSE is a bit lower for the test set. This indicates that we are not overfitting our models at least, since the models perform better on the test set. Solely based on the prediction on the test set, we would choose mod2, which is the full model, including youtube, facebook and newspaper with all interaction terms included. Depending on what one will use the model for, one may choose mod1 or mod2. We can look at the summary output of mod2. summary(mod2) ## ## Call: ## lm(formula = sales ~ youtube * facebook * newspaper, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0556 -0.4672 0.2466 0.7246 1.7399 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.933e+00 6.277e-01 12.640 &lt; 2e-16 *** ## youtube 1.937e-02 2.996e-03 6.466 1.3e-09 *** ## facebook 1.860e-02 1.872e-02 0.994 0.322 ## newspaper 8.807e-03 1.945e-02 0.453 0.651 ## youtube:facebook 9.622e-04 9.108e-05 10.564 &lt; 2e-16 *** ## youtube:newspaper -2.513e-05 8.613e-05 -0.292 0.771 ## facebook:newspaper 2.382e-05 4.514e-04 0.053 0.958 ## youtube:facebook:newspaper -4.273e-07 2.092e-06 -0.204 0.838 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.183 on 152 degrees of freedom ## Multiple R-squared: 0.9659, Adjusted R-squared: 0.9644 ## F-statistic: 615.9 on 7 and 152 DF, p-value: &lt; 2.2e-16 Based on classical statistics, you would fix the non-significant estimates to zero (p-values below 5%) and choose a more parsimoneous model, like mod1. This example shows the difference between modelling for prediction (choose mod2) and modelling for interpretation (choose mod1). We can also look at the observations vs predictions plots for the three models we compared in terms of RMSE. It seems to be very small differences between the predictions from the different models. ggplot(predictions, aes(x = pred, y = sales)) + geom_point() + geom_abline(intercept = 0, slope = 1, col = 2, lty = 2) + facet_wrap(~models, ncol= 3) + xlab(&quot;Predicted&quot;) + ylab(&quot;Observed&quot;) "],["lecture3.html", " 3 Non-linear regression 3.1 GAM example", " 3 Non-linear regression As we saw in the previous lecture in the example illustrating the overfitting concept, we do not always have a linear relationship between the response and the covariates. In that example, the relationship was a periodic sine function. Other examples of non-linear relations can be polynomial-, exponential- and logistical functions. There are many non-linear regression models designed for solving different problems, but we will mainly focus on the generalized additive models (GAMs). These are quite closely related to generalized linear models (GLMs). There are a few options for R packages for GAMs, but we will focus on the mgcv package (Wood 2017). (#fig:examples of nonlinear relations)Examples of relationsships between x and y. In the figure above, we illustrate different types of relationships between x and y. The green one is a linear one, while the others are non-linear. The red curve is even an additive combination of some of the other types of curves, i.e.  \\[ y(x) = -0.3 x + 0.5 \\sin(x)+ 0.5 \\log(x+1) - 0.02 x^2 + \\exp((x+1)/10).\\] This is actually not so different from the principal idea behind GAMs. A GAM uses a set of basis functions, often called smoothers, to map the dependent variables and use the transformed variables as covariates in an additive manner. We will explain this further. For a simple linear model, we have that the expected value of the \\(i\\)th variable is \\(\\mathrm{E} Y_i = \\mu_i = \\beta_0+\\beta_1 X_i\\). For a generalized linear model, the relationship is mapped using a link function \\(g\\), such that \\(g(\\mu_i) = \\beta_0+\\beta_1X_i\\). For a generalized additive model we take it one step further, by also mapping the dependent variable: \\[g(\\mu_i) = \\beta_0 + \\sum_{j=0}^k \\beta_j f_j(X_i),\\] where \\(\\{f_j:\\, j = 1,\\ldots, k\\}\\) is a set of basis functions. The standard in the mgcv package is to use thin plate splines. 3.1 GAM example In the video below, Sondre goes through this section’s example in a bit more detail and with more in-depth explanations of the code. You can also read the example below the video. We will simulate a dataset where the “true” relationship between X and Y is given by (for observation \\(i\\)), \\[Y_i = 0.5X_i + \\sin(X_i)+Z_i, \\quad \\text{where } Z_i\\sim N(0,0.29^2).\\] We start by generating the data in a tibble data frame: set.seed(123) # seed for reproducibility dat &lt;- tibble( x = seq(0, pi * 2, 0.1), sin_x = .5*x+sin(x), # expected value y = sin_x + rnorm(n = length(x), mean = 0, sd = sd(sin_x / 2))) # add noise head(dat) # print first 6 observations ## # A tibble: 6 × 3 ## x sin_x y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 -0.162 ## 2 0.1 0.150 0.0833 ## 3 0.2 0.299 0.749 ## 4 0.3 0.446 0.466 ## 5 0.4 0.589 0.627 ## 6 0.5 0.729 1.22 We can plot the true underlying signal (black line) and the observations (black dots) adding a linear regression line (blue) ontop. Clearly a linear model would fit poorly to these data. ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + geom_line(aes(y = sin_x)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Instead, we will use a GAM model. As you will see below the syntax is very similar to the glm or even lm function in base R. library(mgcv) # gam package mod &lt;- gam(y ~ s(x), data = dat, method = &quot;REML&quot;) The mgcv package uses the syntax y ~ s(x) to specify the model. This means that y is modelled as a smooth function of x. If nothing else is specified, the procedure will use a thin plate spline and determine the number of basis functions to use itself. You can also fix this by setting the \\(k\\) argument of the \\(s()\\) function. When problems with overfitting occur, you can tackle this by setting a lower value for \\(k\\) or setting the smoothing parameter. This argument is \\(sp\\) in the \\(s()\\) function. We will not go into details about this here, but in general it is recommended to use method = “REML”. This means the model is using restricted maxmimum likelihood for the estimation and use this algorithm to set of the smoothing parameter for you. Let’s look at the model summary. summary(mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.56585 0.03236 48.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 6.367 7.517 37.88 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.821 Deviance explained = 83.9% ## -REML = 15.726 Scale est. = 0.065972 n = 63 Here you can see we have a significant intercept term and a approximate significance of the smooth term. The procedure uses an approximation here because the smooth term not really just one covariate. It is important to check that the estimated degree of freedom (edf) is not very close to the reference degree of freedom (Ref.df). This indicates that model has not been given enough flexibility and you could consider setting a higher \\(k\\) value for the smoother. Let’s have a closer look at the smoothing terms being used in this model. In the code below we extract the model matrix and plot the basis function. To have a slightly deeper understanding of what the GAM model does is that it creates a weighted sum of these basis functions to, as optimally as possible, fit the underlying curve of the data. # Extract model matrix: MM &lt;- model.matrix(mod) MM &lt;- as_tibble(MM) MM$x &lt;- dat$x MM &lt;- pivot_longer(MM, -x) # Plot basis functions ggplot(MM, aes(x = x, y = value, group = name, col = name)) + geom_line() + theme_bw(12)+ theme(legend.position = &quot;none&quot;) + labs(y = &quot;Basis functions&quot;) If we multiply these basis functions with the weights for the fitted model, we get the additive terms that constitutes the full model prediction. # Extract coefficient from the model and merge model matrix: coefs &lt;- tibble(coef = coef(mod)) coefs$name &lt;- names(coef(mod)) MM &lt;- left_join(MM,coefs, by = &quot;name&quot;) # Plot model weigthed basis functions ggplot(MM, aes(x = x, y = value*coef, group = name, col = name)) + geom_line() + theme_bw(12)+ theme(legend.position = &quot;none&quot;) + labs(y = &quot;Model weigthed basis functions&quot;) Here you can see how the different curves contribute the full signal. Lets have a look at the final prediction and plot that along with a 95% confidence interval, the observations and the true signal. pred.mod &lt;- predict(mod, se.fit = TRUE) # predict values with standard error dat &lt;- dat %&gt;% mutate(pred = pred.mod$fit, lwr = pred - 1.96 * pred.mod$se.fit, upr = pred + 1.96 * pred.mod$se.fit) ggplot(data = dat, aes(x = x)) + geom_point(aes(y=y)) + geom_line(aes(y = pred), col = &quot;magenta&quot;)+ geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .3, fill = &quot;magenta&quot;) + theme_bw(12) + labs( x = &quot;x&quot;, y = &quot;y&quot; ) Normally, we would do the train-test split, but in this example we know the truth and the point of the exercise is to reproduce the underlying signal. As a last element of this example, we will see how well the model extrapolates as we try to predict the signal for values the model has not “seen”. For the training we used values of \\(x\\) between 0 and \\(2\\pi\\). Let’s see how it performs for values of x between \\(2\\pi\\) and \\(3\\pi\\). # Setting up new data: dat.ext &lt;- tibble( x = seq(2*pi, 3*pi, 0.1), sin_x = 0.5*x+sin(x) # underlying signal ) # Predicting pred.ext &lt;- predict(mod, newdata = dat.ext, se.fit = TRUE, type =) # adding predictions to data frame: dat.ext &lt;- dat.ext %&gt;% mutate(pred = pred.ext$fit, lwr = pred - 1.96 * pred.ext$se.fit, upr = pred + 1.96 * pred.ext$se.fit) # Plotting results: ggplot(dat.ext, aes(x = x))+ geom_line(aes(y = pred), col = &quot;magenta&quot;)+ geom_ribbon(aes(ymin = lwr, ymax = upr), fill = &quot;magenta&quot;, alpha = 0.3)+ geom_line(aes(y = sin_x), col = &quot;red&quot;)+ theme_bw(12) First of all, note the width of the prediction intervals. As the model has not been trained on these values of \\(x\\), it is expected that these are quite wide. It is comforting that the underlying signal (in red) is inside the intervals, but as you can see the shape of the curve is not very near the true signal. Hence, such a prediction will not be very informative. Finally, let’s make a plot, combining the prediction with the training data. Note that we add the y=NA since this column is not included in the dat.ext data frame. We also add the type column to separate the two data sources and split the coloring based on this column. dat.combined &lt;- rbind( dat %&gt;% mutate(type = &quot;training&quot;), # adding colum type to split dat.ext %&gt;% mutate(y=NA, type = &quot;extrapolation&quot;)# the two data sources by ) ggplot(dat.combined, aes(x=x, col = type, fill = type))+ geom_point(aes(y=y), color = &quot;black&quot;) + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .4)+ geom_line(aes(y=pred))+ geom_line(aes(y=sin_x), col = &quot;yellow&quot;, lwd = 1.2, lty =2)+ theme_bw(12) ## Warning: Removed 32 rows containing missing values (geom_point). The warning about the 32 missing values are because we added y=NA to the dat.ext data frame. Data camp There is a very informative and useful GAM module on data camp that we can highly recommend called Nonlinear modeling in R with GAMs - especially chapter 1 and 2. References "],["lecture4.html", " 4 Classification methods 4.1 k-nearest neighbor 4.2 Logistic regression 4.3 Naive bayes 4.4 Wrap-up", " 4 Classification methods Sometimes we are not interested in predicting a continuous output, but rather a factor or a class. We use classes or categories for many things and often we want a model to make a prediction into discrete categories. Is this an image of a cat or a dog? Given information of each passenger, is it likely that a certain individual would survive the shipwrecking of Titanic? Based on the score of certain hand-ins in a predictive modelling course, what final grade (A-F) is the student likely to get on the final exam? In this chapter we will learn about the classification methods k-nearest neighbor (knn), naive bayes and logistic regression. We will briefly consider all three methods here, and refer to the data camp course for further details (see data camp section below). We will also use the same case as an example for all three methods, and compare them all together at the end. In the video at the end, we walk through all the example code. 4.1 k-nearest neighbor The method k-nearest neighbor (knn) is a very simple classificiation method, where for predicting a class of observations in the test set one find the \\(k\\) observations in the training set that has covariates nearest to the covariates of the test case in terms of Euclidean distance. There are many different implementations of knn, for instance in the class package (see ?class::knn). But since we will be using the package bundle tidymodels in the next lecture we will also use it here. Tidymodels is, similar to the tidyverse, a combination of many packages using a kind of pipe notation to build models. It is very well integrated with tidyverse and provides a general framework for many different models. We will use knn as an example of how we set up a model in the tidymodels setup. Example As an example for classification we will use the famous Iris dataset of Edgar Anderson (see ?iris). We start by loaded the packages and the data containing inforation of sepal- and petal- lengths and widths of three different iris flower species (Iris setosa, Iris versicolor and Iris virginica). The goal will be to make a model that can tell us which Iris flower species we are dealing with, based on the given lengths and widths. library(tidyverse) library(tidymodels) df &lt;- as_tibble(iris) head(df) ## # A tibble: 6 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa We make our train-test split using the tidymodels::initial_split function to create the split and respecitvely the training and testing functions to extract the train and test sets. Note that we use strata = Specices. This means we are doing stratified subsampling so that we have the (roughly) the same proportion of the different species in the test and train sets. set.seed(999) split &lt;- initial_split(df, strata = Species) df_train &lt;- training(split) df_test &lt;- testing(split) To fit a model in tidymodels we must first specify which type of model (nearest neighbor) we want to create and set an engine and a mode. The engine is the package used to fit the model and the mode is usually “classification” or “regression”, depending on what type of y variable you are considering. knn_spec &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% # requires &quot;kknn&quot; package installed set_mode(&quot;classification&quot;) Then we are ready to fit the model using the fit function, where we specify a formula and which data to use. We will here use all covariates in the train set to predict Species. knn_fit &lt;- knn_spec %&gt;% fit(Species ~., data = df_train) knn_fit ## parsnip model object ## ## ## Call: ## kknn::train.kknn(formula = Species ~ ., data = data, ks = min_rows(5, data, 5)) ## ## Type of response variable: nominal ## Minimal misclassification: 0.05405405 ## Best kernel: optimal ## Best k: 5 As you can see from the output, the best number of neighbors to be used was 5. The knn model will thus look for the 5 flowers that has the nearest covariates and then predict based on the majority vote of the five. For example if three of them are Iris Virginica and two are Iris versicolor, the prediction will be Iris Virginica. Let us predict on the test set and chech the accuracy and \\(\\kappa\\) using the metrics function. knn_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) metrics(truth = Species, estimate = .pred_class) # calculate metrics ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 The accurcay metric is simply the number of correct classifcation divided by the total number of predictions made. We get (roughly) 95% correct, which seems very acceptable. The Cohen’s kappa metric is useful when the data is imbalanced, e.g. you have many more Iris versicolor than Iris setosa in the data, such that predicting all flowers as Iris versicolor would also give a high accuracy. We can also check the confusion matrix. This is a matrix with frequencies of the true classes as columns and the predicted classes as rows. If all cells except the diagonal is zero, it means the classifier has 100% correct classification. Otherwise, we can see where the “mistakes” happen. To make the confusion matrix, we only need to change the last line of the previous code snippet: knn_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) conf_mat(truth = Species, estimate = .pred_class) # calculate metrics ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 4.2 Logistic regression Logistic regression is a classical method for estimating the conditional probability of certain discrete outcomes given the value of covariates. In the Iris example, we can get estimates of the probability of the flower species being setosa, vercicolor or viriginca given the length and width of sepal and petal. As a first step, we will only assume we have two outcomes, either the flower is a setosa or it is something else. THe response variable is then 1 for setosa and 0 for other. For logistic regression the prediction is thus a value between 0 and 1, and this is achieved by mapping the linear predictors using the inverse logit link function, defined by \\[\\rm{logit}^{-1}(x) = \\frac{e^x}{1+e^x}, \\quad x\\in\\mathbb R.\\] We can also define the logit function directly, mapping probabilites \\(p\\in (0,1)\\) to \\(\\mathbb R\\), by \\[\\rm{logit}(p) = \\ln \\bigg(\\frac{p}{1-p}\\bigg).\\] The right hand side of this definition is often referred to as the log-odds and p would in our case be the probability of a certain flower being an iris setosa. Thus, if \\(x\\) is the vector of covariates for a certain flower and \\(\\beta\\) the parameter vector, we will model the probability of an Iris flower belonging to the species setosa given the set of lengths and widths, by \\[p=P(Y=1|x) = \\rm{logit}^{-1}(x^\\prime \\beta)=\\frac{\\exp(x^\\prime \\beta)}{1+\\exp(x^\\prime \\beta)}.\\] We can also express it in terms of the log-odds \\[\\log \\frac p{1-p} = x^\\prime \\beta = \\beta_0 + \\beta_1x_1 + \\cdots + x_p\\beta_p,\\] which you may note is linear in the parameters. Since the outcome here is binary, we can assume a Binomial distribution and estimate the parameters using maxmimum likelihood estimators. In a prediction setting, a logistic regression model will output a probabilty of flower belonging to class setosa, i.e. a number between zero and one. To translate this into a classification we need to set a threshold, e.g. 50%. Is it more than a 50% chance that the flower is setosa, we will predict that it is. We have tried to explain logistic regression for a binary classification setting, but it can also be used in multinomial settings. We will not go into details about this now, but move over to the practicle implementation in R. If you want to learn more about the theory of logistic regression, see Dobson and Barnett (2018). Example In the practicle example, we will use all three categories of iris flowers. We will use the same train and test sets as we used for knn. We will again, stick to the tidymodels set-up. There are simpler ways to set up a logistic regression using basic functions in R (see ?stats::glm). lr_spec &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% # requires &quot;kknn&quot; package installed set_mode(&quot;classification&quot;) lr_spec ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm Let us fit the model: lr_fit &lt;- lr_spec %&gt;% fit(Species ~., data = df_train) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred As you can see from the output, the algorithm did not converge and the fitted probabilities are at the extremeties 0 and 1, which is not good. We could try to centralize (subtract the mean value) and standardize (divide by the standard deviation) the covariates or to take away some of them to see if we can a converging algorithm. lr_fit &lt;- lr_spec %&gt;% fit(Species ~ Sepal.Length, data = df_train) Using only Sepal.Length seems to converge at least. Let us look at the performance on the test set. lr_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.564 ## 2 kap multiclass 0.346 As expected the accuracy is poor. We can also look at the confusion matrix to see why: lr_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = Species, estimate = .pred_class) ## Truth ## Prediction setosa versicolor virginica ## setosa 12 3 1 ## versicolor 1 10 12 ## virginica 0 0 0 The logistic regression model we built cannot separate Iris Virginica from Iris Versicolor. 4.3 Naive bayes Naive Bayes is the third and last classification method we will consider in this lecture. It is a probabilistic classifier based on applying Bayes theorem with independence assumptions between the features (making it “naive”). They are a type of Bayesian network, but in combination with kernel density estimation, you will not notice the Bayesian framework they are usually wrapped in. In Bayesian methods, you have to specify priors for your parameters, but the R packages we will be using uses Kernel densities instead and you only need to tune them with smoothness parameters. In its essence, a naive bayes classifier finds the probability that a certain flower belongs to any of the classes and select the one with highest probability. We will derive the objective function using Bayes theorem below and use the methods on our Iris data with tidymodels and the discrim package. As the name describes, Naive bayes classifiers uses Bayes theorem to assign a conditional probability given the covariates \\(X_1, \\ldots, X_p\\) that the instance in question belongs to category \\(C_k\\), i.e. \\[P(C_k|X_1,\\ldots, X_p),\\quad k = 1,\\ldots, K,\\] for each of K outcomes or classes \\(C_k\\). According to Bayes theorem, we can write this probability as \\[P(C_k|X_1\\ldots, X_p) = \\frac{P(C_k)P(X_1,\\ldots, X_p|C_k)}{P(X_1,\\ldots, X_p)}.\\] As is often the case for Bayesian methods, the denominator is not of interest here, since it does not depend on \\(C_k\\) - it is only a normalizing constant. Now, the “naive” property of naive bayes is that we assume all covariates to be conditionally independent of each other given \\(C_k\\). Meaning, \\(P(X_i|X_1,\\ldots, X_{i-1}, X_{i+1}, \\ldots, X_p, C_k) = P(X_i|C_k)\\) for all i and any \\(k\\). The numerator above is the joint probability of \\(C_k\\) and \\(X_1,\\ldots, X_p\\). We can write it as \\[\\begin{align} P(C_k)P(X_1,\\ldots, X_p) &amp;= P(X_1, \\ldots, X_p, C_k) \\\\ &amp;= P(X_1|X_2, \\ldots, X_p, C_k)P(X_2, \\ldots, X_p, C_k)\\\\ &amp;= P(X_1|X_2, \\ldots, X_p, C_k)P(X_2|X_3, \\ldots, X_p, C_k)P(X_3, \\ldots, X_p, C_k)\\\\ &amp;=\\ldots\\\\ &amp;= P(X_1|X_2, \\ldots, X_p, C_k)P(X_2|X_3, \\ldots, X_p, C_k)\\cdots \\\\&amp;\\hspace{50pt}\\cdots P(X_{p-1}|X_p, C_k) P(X_p| C_k)P(C_k) \\end{align}\\] Using the Naive conditional independence property, we get \\[\\begin{align} P(C_k)P(X_1,\\ldots, X_p) &amp;= P(C_k)\\prod_{i=1}^p P(X_i|C_k) \\end{align}\\] Thus, we can write that the conditional probability of \\(C_k\\) given the covariates (commonly known as the posterior distribution in Bayesian theory) as \\[P(C_k|X_1,\\ldots, X_p) \\propto P(C_k)\\prod_{i=1}^p P(X_i|C_k),\\] where \\(\\propto\\) mean proportional to, because we have neglected the denominator. We can then construct a classifier based on this posterior distribution, selecting the category or class \\(k\\) that has the highest posterior probability. Formallly, we can define the classifier \\(\\widehat y\\) as \\[\\widehat y = \\mathrm{argmax}_k\\, P(C_k)\\prod_{i=1}^p P(X_i|C_k)\\] There are different ways of selecting priors, i.e. the probability functions \\(P(C_k)\\) and \\(P(X_i|C_k)\\). For the \\(P(C_k)\\) it is common to either use the proportion of each class in the training set or set all probabilities equal \\(1/K\\). For the feature distributions one can select Mutinomial or Bernoulli distributions for discrete covariates or Gaussian for continous, for instance. The package we will be using is using Kernel density estimation for the priors, which is a non-parametric procedure. Example Turning back to the flowers one last time, we will be using the package discrim with tidymodels. For hyper parameters there is a smoothness term one can set in the set_engine that determines how smooth the kernel should be. We will only use standard values here and see how it goes. The procedure is more or less the same as for the other two approaches we have considered - as you will see in the code below. # Load naive bayes pacakge: library(discrim) # Set up model specification: nb_spec &lt;- naive_Bayes() %&gt;% set_engine(&quot;naivebayes&quot;) %&gt;% set_mode(&quot;classification&quot;) # Fit the model: nb_fit &lt;- nb_spec %&gt;% fit(Species ~ ., data = df_train) # Evaluate performance on the test set: nb_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 # Confusion matrix: nb_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = Species, estimate = .pred_class) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 4.4 Wrap-up We have now fitted three different classfiers to the iris data set. In this setting and without doing any in-depth fine tuning of the models, we found that the logistic regression did not perform very well, while k-nearest neighbour and naive bayes seemed to do a good job of splitting the flowers into the correct categories. In fact, on this particular example and without model tuning, KNN and naive bayes performed exactly equal. Let’s do a quick check that they are 100% in agreement by making a confusion matrix between the predictions from KNN and naive bayes. # KNN prediction on test set knn_pred &lt;- knn_fit %&gt;% predict(new_data = df_test) %&gt;% rename(&quot;knn&quot;=&quot;.pred_class&quot;) # Renaming column for predictions # Logistic regression prediction on test set lr_pred &lt;- lr_fit %&gt;% predict(new_data = df_test) %&gt;% rename(&quot;lr&quot;=&quot;.pred_class&quot;) # Naive Bayes prediction on test set nb_pred &lt;- nb_fit %&gt;% predict(new_data = df_test) %&gt;% rename(&quot;nb&quot;=&quot;.pred_class&quot;) predictions &lt;- bind_cols(knn_pred, nb_pred, lr_pred) conf_mat(data = predictions, truth = knn, estimate = nb) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 15 0 ## virginica 0 0 11 Let us also compare one of them to the logistic regression predictions conf_mat(data = predictions, truth = knn, estimate = lr) ## Truth ## Prediction setosa versicolor virginica ## setosa 12 4 0 ## versicolor 1 11 11 ## virginica 0 0 0 For this partitular problem, logsitic regression was not the method to choose, but remember that it may be different in other settings. Data camp We highly recommend the data camp course Supervised Learning in R: Classification - chapters 1-3. The subject of chapter 4 is covered separately in the next lecture 5. 4.4.1 Sources rpubs References "],["lecture5.html", " 5 Decision Trees and Bagged Trees 5.1 Decision trees 5.2 Bagged trees", " 5 Decision Trees and Bagged Trees In this lecture we will learn about decision trees and bagged trees for classification or regression models. As in the previous lecture, we will use the tidymodels framework (Kuhn and Wickham 2020). If you have not installed tidymodels, this can be done by install.packages(&quot;tidymodels&quot;) This lecture is highly inspired by the data camp course Machine learning with three based models in R (see link at the buttom). We start with decision trees. 5.1 Decision trees A decision tree is a classifier that can be represented as a flow chart that looks like a tree. In each prediction, we start at the root asking a question deciding to which branch we should go for the prediction. As an example, say we want to predict whether or not a person likes computer games and make a decision tree for solving that problem. We have illustrated such a tree below. First you must ask: Is the person younger than 15 years old? If yes, predict that the person likes computer games. If the answer is no, is the person a boy? If yes, predict YES, he likes computer games, or if no, then NO, she does not like computer games. (#fig:decisiontree_computergames)A simple decision tree In the illustration we have tried to make the decision tree “grow from the ground-up”, but it is most common to print them the other way around. The pros of decision trees are: Easy to explain and intuitive to understand Possible to capture non-linear relationships Require no standardization or normalization of numeric features No need for dummy variables for categoric features Robust to outliers Fast for large datasets Can be used for regression and classification The cons are: Hard to interpret if large, deep, or ensembled High variance, complex trees are prone to overfitting To set up a tidymodels decision tree, we start by loading the package library(tidymodels) To make a decision tree, there are three elements we need. First a decision tree object, then the engine and finally the mode. tree_spec &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) Here decision_tree() makes the decision tree object. There are different packages we can use for decision trees, but we select a package called rpart, which is a package for recursive partitioning. This is called the engine in the tidymodels framework. Finally, we must decide if we are to do classification or regression, and this is the mode. Here we use classification. The tree_spec object is just a skeleton, we need data to make it useful. For example purposes, let use the same data as in lecture 4 (the iris data). library(tidyverse) df &lt;- as_tibble(iris) set.seed(999) split &lt;- initial_split(df, strata = Species) df_train &lt;- training(split) df_test &lt;- testing(split) Then we are ready to fit a decition tree to our training data. You will notice the procedure is very similar to the procedure we used in lecture 4. This is the largest benefit of using tidymodels. tree_fit &lt;- tree_spec %&gt;% fit(Species ~., data = df_train) tree_fit ## parsnip model object ## ## n= 111 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 111 74 setosa (0.33333333 0.33333333 0.33333333) ## 2) Petal.Length&lt; 2.6 37 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.6 74 37 versicolor (0.00000000 0.50000000 0.50000000) ## 6) Petal.Width&lt; 1.75 39 3 versicolor (0.00000000 0.92307692 0.07692308) * ## 7) Petal.Width&gt;=1.75 35 1 virginica (0.00000000 0.02857143 0.97142857) * Using the rpart.plot package, we can visualize the decision tree we have fitted by library(rpart.plot) rpart.plot(tree_fit$fit,roundint=FALSE) So, to make a prediction, the decision tree asks first: “Is the petal length below 2.6?” If yes, predict Setosa. If no: “Is the petal width below 1.75 (rounded off to 1.8 in the illustration)?” If the answer is yes, it is a vercicolor and a virginica if no. We can also make predictions on the test set and calculate the accuracy tree_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) metrics(truth = Species, estimate = .pred_class) # calculate metrics ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 and the confusion matrix tree_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) conf_mat(truth = Species, estimate = .pred_class) # confusion matrix ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 If you remember from the last lecture, this is the same result as we got using knn and naive bayes. 5.2 Bagged trees Bagged threes is an ensemble method. This means, instead of fitting just one decision tree, we fit many, and aggregate the predictions from all of them to improve our final prediction. Bagging is short for bootstrap aggregation. Bootstrap is a method for resampling that we use on the training set to get many training sets with the same properties as the original one. We sample random rows of the training set with replacement to make these bootstrapped datasets and fit a decision tree to each of these. (#fig:bagged_ensemble)A bagged tree This will give us many predictions that we then aggregate to arrive at our bagged prediction. If we are in a regression setting, the aggregated prediction is simply the mean, while in a classification setting the majority vote becomes the final prediction. Bagging can reduce the variance of our prediction significantly. (#fig:bagged_ensemble_pred)A bagged tree prediction Let us fit a bagged tree to the iris data. Here we need to install an additional package called baguette. install.packages(&quot;baguette&quot;) This package contains the function bag_tree that we will use in our model spec. library(baguette) bag_spec &lt;- bag_tree() %&gt;% set_engine(&quot;rpart&quot;, times = 100) %&gt;% set_mode(&quot;classification&quot;) As you have learned by now, the structure is the same, but notice in the engine specification we have set times = 100. This is how we specify how many trees we want to include in our bag. set.seed(123) bag_fit &lt;- bag_spec %&gt;% fit(Species ~., data = df_train) bag_fit ## parsnip model object ## ## Bagged CART (classification with 100 members) ## ## Variable importance scores include: ## ## # A tibble: 4 × 4 ## term value std.error used ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Petal.Length 66.8 0.320 100 ## 2 Petal.Width 66.7 0.277 100 ## 3 Sepal.Length 48.2 0.489 100 ## 4 Sepal.Width 27.9 0.487 100 In the output, we see the variable importance scores. As you can see the Petal.Length is the most important covariate. We can also make predictions on the test set and calculate the accuracy bag_fit %&gt;% predict(df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 and the confusion matrix bag_fit %&gt;% predict(df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = Species, estimate = .pred_class) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 It seems this is the best we can do with the data that we have - same result as for knn, naive bayes and a simple decision tree. On data camp you will learn more about decision trees and bagged trees - also how to use them in a regression setting. Data camp We highly recommend the data camp course Machine Learning with Tree-Based Models in R chapters 1-3. References "],["lecture6.html", " 6 Random forrest and boosting 6.1 Hyperparameter tuning 6.2 Random Forest 6.3 Boosted trees", " 6 Random forrest and boosting In this lecture we will continue where we left off in the previous lecture by considering the two methods Random forest and Boosted trees. Both bagging and boosting are general methods one can use for many predictive models and trees is first and foremost a convenient example of such procedures. This lecture is highly inspired by the data camp course Machine learning with three based models in R (see link at the buttom). For this lecture we have not made a video. Before we go into the new models, we will have a quick look at how one can automatically tune hyperparameters with the tidymodels R package. 6.1 Hyperparameter tuning All the models we have considered so far have hyperparameters one can tune - meaning finding the optimial set of hyperparameters. We have sticked to using standard values, but one can also use the tune (Kuhn 2021) package which is included in the tidymodels package. We will simply show you how it is done, but in principle you set up a set of candidate hyperparameters for the procedure to consider and then it fits set of model candidates and selects the best one. For this example we have already loaded the iris data and set up the test and training. We set up the model specification setting the hyperparameters min_n (minimum number of data points in a node for the node to be split further) and tree_depth (maximimum depth of the tree) to tune() indicating to the procedure that these will be tuned. tree_spec &lt;- decision_tree(min_n = tune(), # to be tuned tree_depth = tune()) %&gt;% # to be tuned set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) There are several ways of setting up the grid of model candidates, but we will consider the simplest here; namely the grid_regular. This allows you to simply specify how many grid points you want for each parameter, and it creates an equidistant grid. We use 4 for each, giving a \\(4\\times 4\\) grid. tree_grid &lt;- grid_regular(parameters(tree_spec), levels = 4) ## Warning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003. ## Please use `hardhat::extract_parameter_set_dials()` instead. tree_grid ## # A tibble: 16 × 2 ## tree_depth min_n ## &lt;int&gt; &lt;int&gt; ## 1 1 2 ## 2 5 2 ## 3 10 2 ## 4 15 2 ## 5 1 14 ## 6 5 14 ## 7 10 14 ## 8 15 14 ## 9 1 27 ## 10 5 27 ## 11 10 27 ## 12 15 27 ## 13 1 40 ## 14 5 40 ## 15 10 40 ## 16 15 40 Before we can start tuning, we need to set up the cross validation folds. We use 5 partitions for the cross validation. folds &lt;- vfold_cv(df_train, v = 5) Then we are ready to set up the tuning using the tune_grid function. It fits the model using each of the model candiates in the tuning grid and evaluates the model using cross validation for out-of-sample performance with a user-specified metric. tune_results &lt;- tune_grid( tree_spec, Species ~ ., resamples = folds, grid = tree_grid, metrics = metric_set(accuracy) ) We can visualize the tuning using the autoplot function: autoplot(tune_results) Selecting the best model is done in a few steps. First we extract the best model setup and create a new (final) model specification. # Best hyperparameters: final_params &lt;- select_best(tune_results) final_params ## # A tibble: 1 × 3 ## tree_depth min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 5 2 Preprocessor1_Model02 # Best model spec: best_spec &lt;- finalize_model(tree_spec, final_params) best_spec ## Decision Tree Model Specification (classification) ## ## Main Arguments: ## tree_depth = 5 ## min_n = 2 ## ## Computational engine: rpart It seems the best model (among our candidates) has tree_depth of 5 and min_n of 2. 6.2 Random Forest Random forest is really a bagged tree with additional randomness added to it. That is, it is an ensemble method with trained trees on bootstrap samples. The additional randomness stems from using a random sub-sample of the available predictors in each tree in the ensemble. It may be non-intuitive that not using all predictors in each tree would provide the best prediction, but it is beneficial for the ensemble that the individual trees are less correlated. They are Well-suited for high dimensional data Easy to use Implemented in several R packages (ranger, randomForest) In tidymodels, the function rand_forest() provides a user interface to these implementations. The hyperparameters are mtry: predictors seen at each node trees:Number of trees in your forest min_n: smallest node size allowed Increasing the number of trees can be one way of improving the model performance. There are also different options for algorithms to be used for the node split. If you use the ranger implementation the options are impurity or permutation. This is set in the set_engine call. We will not go into the details here, but use the impurity option as default. Example In this lecture, we will move away from the Iris data set and consider the problem of predicting the sex of abalones. The data is produced by Nash et al. (1994) and downloaded here. Here you will also find the following explanation of the columns of the data: Name / Data Type / Measurement Unit / Description Sex / nominal / – / M, F, and I (infant) Length / continuous / mm / Longest shell measurement Diameter / continuous / mm / perpendicular to length Height / continuous / mm / with meat in shell Whole weight / continuous / grams / whole abalone Shucked weight / continuous / grams / weight of meat Viscera weight / continuous / grams / gut weight (after bleeding) Shell weight / continuous / grams / after being dried Rings / integer / – / +1.5 gives the age in years We load the data and call it df. Here are the first 6 rows: head(df) ## # A tibble: 6 × 9 ## Sex Length Diameter Height `Whole weight` `Shucked weight` `Viscera weight` `Shell weight` Rings ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.455 0.365 0.095 0.514 0.224 0.101 0.15 15 ## 2 M 0.35 0.265 0.09 0.226 0.0995 0.0485 0.07 7 ## 3 F 0.53 0.42 0.135 0.677 0.256 0.142 0.21 9 ## 4 M 0.44 0.365 0.125 0.516 0.216 0.114 0.155 10 ## 5 I 0.33 0.255 0.08 0.205 0.0895 0.0395 0.055 7 ## 6 I 0.425 0.3 0.095 0.352 0.141 0.0775 0.12 8 We set up the test-train split: set.seed(1991) split &lt;- initial_split(df, strata = Sex) df_train &lt;- training(split) df_test &lt;- testing(split) Now, let us specify a random forest model. We will now tune the min_n hyperparameter and fix the others to 50 trees and an mtry of 6. rf_spec &lt;- rand_forest(trees = 50, min_n = tune(), mtry = 6) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) %&gt;% set_mode(&quot;classification&quot;) rf_spec ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = 6 ## trees = 50 ## min_n = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: ranger To set up the tuning, we use 10-fold cross validation and use 5 levels for the grid. Then we tune the models using all avaliable predictors to best predict the sex of the abalones. We use accuracy as our metric of performance. df_fold &lt;- vfold_cv(df_train, v = 10) param_grid &lt;- grid_regular(parameters(rf_spec), levels = 5) ## Warning: `parameters.model_spec()` was deprecated in tune 0.1.6.9003. ## Please use `hardhat::extract_parameter_set_dials()` instead. tuning &lt;- tune_grid( rf_spec, Sex ~ ., resamples = df_fold, grid = param_grid, metrics = metric_set(accuracy) ) autoplot(tuning) It seems a small minimum sample size for splitting is beneficial and choose the best among our candidate models: par_best &lt;- select_best(tuning) rf_spec_final &lt;- finalize_model(rf_spec, par_best) # fit final model: rf_fit &lt;- rf_spec_final %&gt;% fit(Sex ~ ., data = df_train) An interesting function to be aware of is the vip function in the vip package (Greenwell and Boehmke 2020), which produces a variable of importance plot. rf_fit %&gt;% vip::vip() From this plot, it seems the Viscera weight is the most important variable. Let’s measure the out-of-sample performance on our test set: predict(rf_fit, new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Sex, estimate = .pred_class) %&gt;% filter(.metric == &quot;accuracy&quot;) %&gt;% select(.estimate) ## # A tibble: 1 × 1 ## .estimate ## &lt;dbl&gt; ## 1 0.565 It seems to be close to the in-sample performance. 6.3 Boosted trees For bagged threes and random forest, the different trees in the ensemble are independent. This is a computation benefit, because it makes it possible to fit multiple models in the ensemble in parallel. The idea behind boosting, or in our case boosted trees, is that the second model in the ensemble learns from the mistakes of first model, and so on. This means we can no longer estimate the models in parallel, but intuitively it is smarter to allow the models to learn from their colleuges mistakes - even if it takes a bit longer to estimate. This is why boosted trees very often outperformed bagged trees or random forest. There are several boosting algorithms. One of the is called AdaBoost (Adaptive Boosting). The idea behind this algorithm is that you change the weight of wrongly classified training instances in the next ensemble traning - making them more important for the algorithm to get right in the next tree. The final ensemble prediction is a weighted sum of the preceding models. The AdaBoost algorithm has been further developed by adding a technique called gradient descent and is the called gradient boosting. Instead of changing the weights of the observations, gradient boosting uses a loss function that is optimized using gradient descent. Boosted methods has been shown to be among the best-performing machine learning models and it turns out that it is a good option for unbalanced data, but be aware - it is prone to overfitting! In random forest, adding too many trees will not overfit the model, it will just not improve anymore, while boosted trees will in the asymptote try to “correct all preceding tree’s mistakes” and thus overfit to the data. Training can also slow and there are quite many hyperparameters to select or tune. The hyperparameters are: min_n: Minimum number of data points in a node that is required to be split further tree_depth: Maximum tree depth Sample size: Amount of data exposed to the fitting routine trees: Number of trees in the ensemble mtry: Number of predictors randomly sampled at each split learn_rate: Rate at which the boosting algorithm adapts from iteration to iteration loss_reduction: Reduction in the loss function required to split further stop_iter: The number of iterations without improvement before stopping Let us fit a boosted tree to predict the sex of abalones. You are by now very familiar with tidymodels, but we start by specifying the model spec. We will not tune hyperparameters now, but stick to the defaults. boost_spec &lt;- boost_tree() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;xgboost&quot;) boost_fit &lt;- boost_spec %&gt;% fit(Sex ~ ., df_train) boost_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% select(Sex, .pred_class) %&gt;% conf_mat(truth = &quot;Sex&quot;, estimate = &quot;.pred_class&quot;) ## Truth ## Prediction F I M ## F 119 27 118 ## I 39 267 74 ## M 169 42 190 boost_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% select(Sex, .pred_class) %&gt;% metrics(truth = &quot;Sex&quot;, estimate = &quot;.pred_class&quot;) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.551 ## 2 kap multiclass 0.324 The accuracy is very similar to that of the random forest performance on this example (using the same number of trees and min_n). Let’s tune the hyperparameters learn_rate, tree_depth and sample_size using random tuning grid (with 8 candidate models). boost_spec &lt;- boost_tree( trees = 500, learn_rate = tune(), tree_depth = tune(), sample_size = tune() ) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;xgboost&quot;) boost_spec ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## trees = 500 ## tree_depth = tune() ## learn_rate = tune() ## sample_size = tune() ## ## Computational engine: xgboost # -- Random tuning grid --: tunegrid_boost &lt;- grid_random(parameters(boost_spec), size = 8) tunegrid_boost ## # A tibble: 8 x 3 ## tree_depth learn_rate sample_size ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 0.0000969 0.219 ## 2 5 0.000187 0.576 ## 3 5 0.00000321 0.492 ## 4 5 0.0392 0.963 ## 5 3 0.000000682 0.175 ## 6 4 0.000383 0.679 ## 7 8 0.0000000113 0.673 ## 8 13 0.000000175 0.284 # Tuning: tune_results &lt;- tune_grid( boost_spec, Sex ~ . , resamples = vfold_cv(df_train, v = 6), grid = tunegrid_boost, metrics = metric_set(accuracy) ) autoplot(tune_results) # Select best configuration: best_par &lt;- select_best(tune_results) final_spec &lt;- finalize_model(boost_spec, best_par) # Fit the best model: boost_fit &lt;- final_spec %&gt;% fit(Sex ~ ., df_train) ## [20:00:45] WARNING: amalgamation/../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. # Evaluate out-of-sample performance on test set: boost_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = &quot;Sex&quot;, estimate = &quot;.pred_class&quot;) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.558 ## 2 kap multiclass 0.331 We see we go from 0.551 accuracy to 0.558 - so very slight improvement. Iris data For the sake of continuity, let’s us also see if the boosted tree can manage to correctly predict the last three iris flowers: df &lt;- as_tibble(iris) set.seed(999) split &lt;- initial_split(df, strata = Species) df_train &lt;- training(split) df_test &lt;- testing(split) boost_spec &lt;- boost_tree( trees = 500, learn_rate = tune(), tree_depth = tune(), sample_size = tune() ) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;xgboost&quot;) boost_spec ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## trees = 500 ## tree_depth = tune() ## learn_rate = tune() ## sample_size = tune() ## ## Computational engine: xgboost # -- Random tuning grid --: tunegrid_boost &lt;- grid_random(parameters(boost_spec), size = 8) tunegrid_boost ## # A tibble: 8 x 3 ## tree_depth learn_rate sample_size ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 8.46e- 7 0.581 ## 2 11 2.22e- 3 0.592 ## 3 2 2.19e- 8 0.203 ## 4 14 5.77e- 9 0.292 ## 5 12 3.47e- 3 0.418 ## 6 6 4.75e- 7 0.298 ## 7 3 9.69e-10 0.922 ## 8 9 8.41e- 7 0.542 # Tuning: tune_results &lt;- tune_grid( boost_spec, Species ~ ., resamples = vfold_cv(df_train, v = 6), grid = tunegrid_boost, metrics = metric_set(roc_auc) ) autoplot(tune_results) # Select best configuration: best_par &lt;- select_best(tune_results) final_spec &lt;- finalize_model(boost_spec, best_par) # Fit the best model: boost_fit &lt;- final_spec %&gt;% fit(Species ~ ., df_train) ## [20:27:46] WARNING: amalgamation/../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. # Evaluate out-of-sample performance on test set: boost_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = &quot;Species&quot;, estimate = &quot;.pred_class&quot;) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 Same as for the other methods - should have used another example data from the start! Data camp We highly recommend the data camp course Machine Learning with Tree-Based Models in R chapters 3-4. References "],["lecture7.html", " 7 Support vector machines 7.1 Linear SVMs 7.2 Polynomial SVM", " 7 Support vector machines Support vector machines is, similarly to the tree based models we have looked at already, a supervised learning method for classification. They can also be used in regression settings, but my impression is that they are mostly applied for classification in cases with continuous predictors. The reason for this is that with a support vector machine (often abbreviated svm) the outcome is a (continuous) decision boundary splitting the p-dimensional predictor space into regions where the y has different classes. We will only cover the case with two possible outcomes for y here and also only two predictors, x1 and x2. We will also only use simulated data. We will not go into the mathematical details of how the algorithm works, but focus on applications and how it may look like for a user to use svm’s. The lecture is highly inspired by the data camp course Support Vector Machines in R (see link at the buttom) and the main R package for this lecture implementing all the svm related functions is called e1071 (Meyer et al. 2021). For this lecture we have not made a video. Say you have a data frame with two numerical explanatory variables, x1 and x2, and a response variable, y, that takes the values -1 and 1. You want to make a model for predicting the outcome of y given x1 and x2. Since y only takes two values and x1 and x2 are continuous numerical predictors, a classical decision tree would find some threshold for x1 and x2 and say that all x1 &gt; c1 is classified as Y=-1, for instance. A support vector machine will find a continuous line that splits x1 and x2 into regions where Y=-1 and y=1. In our first example the separating line is linear and our svm will be using a linear kernel, but there are other options also (in the e1071 package: polynomial, radial basis, sigmoid). Situations with two predictors are most interesting from a pedagogical perspective, because it is easy to visualize the separating line. In three dimensions, it will be a separating plane, while in one dimension it will only be a threshold (much like a simple decision three). 7.1 Linear SVMs We start illustrating how it works with a simple example. The first 6 rows of the data frame looks like this: head(df) ## # A tibble: 6 × 3 ## x1 x2 y ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.288 0.354 -1 ## 2 0.788 0.366 1 ## 3 0.409 0.287 1 ## 4 0.883 0.0800 1 ## 5 0.940 0.365 1 ## 6 0.0456 0.178 -1 Let us do a train-test split. The following code should be very familiar by now. library(tidymodels) split &lt;- initial_split(df, prop = .8, strata = y) df_train &lt;- training(split) df_test &lt;- testing(split) We can plot the training data with x1 on the x axis and x2 on the y axis and color the dots by the categorical y response variable. p &lt;- ggplot(df_train, aes(x=x1,y=x2,col = y)) + geom_point()+ scale_color_manual(values = c(4,6)) p Here it is clear that a linear split will fit perfectly from just looking at the figure. But let us use the e1071 package and fit an svm for this problem and see the line. The different arguments in the function call below are quite self-explanatory, but we specify the formula and the data first. Then we specify that it is classification we want to do and that the kernel should be linear. This means we want the curve that divides the data set to be a straight line. The scale argument is set to false, as we do not want to scale the predictors nor response variables here. If scale is true, the algorithm will standardize both the x and y variables. library(e1071) svm_fit &lt;- svm(y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE) summary(svm_fit) ## ## Call: ## svm(formula = y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 95 ## ## ( 48 47 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 There is not a lot of information here, but note especially the number of support vectors (95). We will come back to this later. Now, let us plot the points again and highlight the support vectors from the model fit. # Extract support vectors df_sv &lt;- df_train[ svm_fit$index, ] p &lt;- p + geom_point(data = df_sv, color = &quot;green&quot;, alpha = .2, size = 4) p As you can see, the support vectors are clustered around the decision boundary. One could say that they support the decision boundary, hence the name. In general, the support vectors are data points that are closer to the hyperplane and influence the postion and orientation of the hyperplane. The idea is to use the support vectors to find the hyperplane that maximizes the margin of the classifier. By looking at the figure you can imagine drawing many straight lines that will split the blue and purple dots, but by maximizing the margin we get closer to a unique solution. Next we will add in the decision boundary and margins. It is not as straight forward as one might imagine to extract the intercept and slope from the fitted object. The reason for this is likely that the implementation is more general than a linear kernel in two dimensions. Still we can derive this information from the information in the fitted object. First we must build the weight vector, \\(w\\), from the coefficients and the support vectors matrix. w &lt;- t(svm_fit$coefs) %*% svm_fit$SV w ## x1 x2 ## [1,] -5.180669 5.60003 The slope is then given by the negative ratio between the first and second weight svm_slope &lt;- -w[1]/w[2] svm_slope ## [1] 0.9251146 and the intercept is found by the ratio between the rho element and the second weight svm_intercept &lt;- svm_fit$rho / w[2] svm_intercept ## [1] -0.02130264 Let us add it to our figure p &lt;- p + geom_abline(slope = svm_slope, intercept = svm_intercept, col = &quot;red&quot;) p The margins are parallel to the decision boundary with an offset of 1/w[2]. p &lt;- p + geom_abline(slope = svm_slope, intercept = svm_intercept-1/w[2], col = &quot;red&quot;, linetype = &quot;dashed&quot;)+ geom_abline(slope = svm_slope, intercept = svm_intercept+1/w[2], col = &quot;red&quot;, linetype = &quot;dashed&quot;) p Note that the decision boundary is supported by roughly the same number of support vectors on either side. In this example we do not have any instances violating the boundary, so our accuracy on the test set will be perfect (100%), but let us test it anyway. svm_fit %&gt;% predict(df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = &quot;y&quot;, estimate = &quot;...1&quot;) # default name of new column is &quot;...1&quot; ## New names: ## • `` -&gt; `...1` ## Truth ## Prediction -1 1 ## -1 51 0 ## 1 0 40 As we can see, there is no need to calculate the accuracy as all predictions are correct it will be 1. There is also a built-in plotting function for svm’s in the e1071 package. It highlights the support vectors by x’es and observations by o’s, and separate the decision boundary by the different colored areas. It takes the fitted object and the data set you want to predict on. Here we use the training set, but you can also use the test set. plot(svm_fit, data = df_train) Note that here x1 is on the y-axis and x2 on the x-axis (opposite of what we did in ggplot above). Now, we will play a little bit with the hyperparameters. Remember that we had 95 support vectors in our first attempt. Now we will increase the cost hyperparameter, i.e. the cost of constraint violations. svm_fit &lt;- svm(y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE, cost = 100) summary(svm_fit) ## ## Call: ## svm(formula = y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, cost = 100, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 100 ## ## Number of Support Vectors: 7 ## ## ( 3 4 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 As you can see from the model output, the number of support vectors has drastically reduced from 95 to 7. We can also look at the equivalent plot (code as above is not included here). Here you see that the margin is much narrower. Increasing the cost like this for linear support vector machines can be useful in situations where the decision boundary is known to be linear. We will now consider a situation where the decision boundary is non-linear - in fact a circle. 7.2 Polynomial SVM To generate circles for plotting, we introduce the function circle. This creates a tibble data frame with npoints rows containing x1 and x2 coordinates for a circle of radius r. circle &lt;- function(x1_center, x2_center, r, npoints = 100){ theta = seq(0,2*pi,length.out=npoints) tibble( x1c = x1_center + r*cos(theta), x2c = x2_center + r*sin(theta) ) } We generate the data and plot it with the decision boundary being a circle of radius 0.7. radius &lt;- .7 set.seed(54321) # generate data: df &lt;- tibble( x1 = runif(200, min = -1, max = 1), x2 = runif(200, min = -1, max = 1), y = factor(ifelse(x1^2+x2^2 &lt; radius^2, -1,1), levels = c(-1,1)) ) # train-test split split &lt;- initial_split(df, prop = .8, strata = &quot;y&quot;) df_train &lt;- training(split) df_test &lt;- testing(split) # Generate decision boundary circle: boundary = circle(0,0,r = radius) # Plot p &lt;- ggplot(df, aes(x=x1,y=x2,col=y)) + geom_point() + geom_path(data = boundary, aes(x=x1c, y= x2c), inherit.aes=FALSE) p Let us first try to fit a linear svm to these data. We expect it not to perform very well, but let us see how bad it will be. For a quick check we just plot the result using the plot function. svm_fit &lt;- svm(y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE, cost = 1) plot(svm_fit, data = df_train) As expected, the linear svm performs bad and predicts all observations to belong to the class \\(y=1\\). The number of support vectors (114) is also very high, which is a sign of poor performance. We can try to increase the cost, as we did before. svm_fit &lt;- svm(y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE, cost = 100) plot(svm_fit, data = df_train) More or less the same result. Increasing the cost reduced the number of support vectors to 119. The solution to this problem is to use another kernel than the linear one, but before we to that, we will use the transformation trick. As is often the case, a smart transformation can often take a non-linear problem and make it linear. In this case, we know the boundary is a circle and the formula for a circle is \\[ x_1^2 + x_2^2 = r^2, \\] where \\(r\\) is the radius. If we transform the data, by squaring the observations, say \\[z_1 = x_1^2 \\quad\\text{and}\\quad z_2=x_2^2,\\] we get that \\[x_1^2+x_2^2 = z_1+z_2 = r^2 \\quad\\Rightarrow \\quad z_2 = r^2-z_1.\\] That is the relationship between \\(z_1\\) and \\(z_2\\) is linear with a slope of \\(-1\\) and an intercept of \\(r^2\\). Since we know the true radius, we can plot this directly: df_train2 &lt;- df_train %&gt;% mutate(z1=x1^2, z2 = x2^2) %&gt;% select(z1,z2,y) ggplot(df_train2, aes(x=z1,y=z2,col=y)) + geom_point() + geom_abline(slope = -1, intercept = radius^2) Let us fit a linear svm to the transformed data. svm_fit &lt;- svm(y ~ ., data = df_train2, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE, cost = 100) plot(svm_fit, data = df_train2) Clearly, this is more satisfying. The next step we will test is to use the original data without transforming, but using a different kernel. Instead of squaring the data, we will use a polynomial kernel of 2nd degree. Using the polynomial kernel option, there are three hyperparameters: degree, gamma and coef0. The polynomial kernel is parameterized as \\((\\text{gamma*u&#39;*v}+ \\text{coef0})\\text{^degree}\\), where \\(u\\) and \\(v\\) are data vectors. We set the kernel argument to “polynomial” and degree to 2, and for now we use standard options for gamma and coef0. svm_fit &lt;- svm(y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;polynomial&quot;, degree = 2, scale = FALSE, cost = 1) plot(svm_fit, data = df_train) mean(df_train$y == predict(svm_fit, df_train)) #Accuracy ## [1] 0.9874214 The accuracy is certainly very high (98.7%), but for the sake of learning, we will try to improve this further by tuning the hyperparameters instead of using the defaults. We do this by using the tune.svm function in e1071. The syntax is very similar to the svm function, but we need to feed in the x’s and y’s separately as below and also feed in a vector for each of the hyperparaemters with the values we want to consider. We use costs of 0.1,1,10 and 100, gamma values of 0.1,1 and 10 and coef0 of 0,0.1,1 and 10. The tuning can take some time, but on this example it is quite fast. df_train &lt;- as.data.frame(df_train) tuning &lt;- tune.svm( x = df_train[,-3], y = df_train[,3], type = &quot;C-classification&quot;, kernel = &quot;polynomial&quot;, degree = 2, #Suggestions for the hyperparameters: cost = 10^(-1 : 2), gamma = c(0.1, 1, 10), coef0 = c(0, 0.1,1,10) ) # Extract the &quot;best&quot; parametrs: tuning$best.parameters ## degree gamma coef0 cost ## 6 2 10 0.1 0.1 We fit the model using the best parameters from the tuning: svm_fit &lt;- svm(y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;polynomial&quot;, degree = 2, scale = FALSE, cost = tuning$best.parameters$cost, gamma = tuning$best.parameters$gamma, coef0 = tuning$best.parameters$coef0) plot(svm_fit, data = df_train) mean(df_train$y == predict(svm_fit, df_train)) #Accuracy ## [1] 0.9937107 The tuning resulted in a slight improvement of roughly half a percent of accuracy going from 98.7% to 99.4%, but we are already very close to 100%. On the data camp course recommended below you will learn more about this and in chapter 4 you will also go a bit further than we have here. Data camp We highly recommend the data camp course Support Vector Machines in R chapters 1-4. References "],["lecture8.html", " 8 Artificial Neural networks 8.1 Feed Forward Neural Networks 8.2 Objective functions and training 8.3 Validation of trained models 8.4 Implementation of Feed Forward Neural Network in Keras", " 8 Artificial Neural networks This section describes the basics theory and classical optimization of an artificial neural network. This includes defining a 2-layer feed forward neural network, cost and objective functions, regularization and the manner in which a network can be trained to produce desired output. The section will be limited to so-called supervised learning, i.e. where know the true state of an output. The main references for this chapter are the textbook “Deep Learning” by Goodfellow et al. (2016). 8.1 Feed Forward Neural Networks Let \\(\\mathbf{X}^{(i)}\\) be an instance of data which is input to a deep learning model, with associated target values \\(\\mathbf{y}^{(i)}\\). A target value can be a class or as we will use later the data \\(\\mathbf{X}^{(i)}\\) itself. All the instances of \\(\\mathbf{X}^{(i)},\\) \\(i=1, \\ldots N,\\) constitute the data set \\(\\mathbf{X}^{(i)}\\), and all the target values \\(\\mathbf{y}^{(i)},\\) \\(i=1, \\ldots N,\\) comprise the data set $. A feed forward neural network is a hierarchical model that consists of nodes or computational units divided into subsequent layers. For each node, a non-linear activation function is applied. The nodes between each layer are connected, so that the input to a node is totally dependent on the output from the nodes of the previous layer. The model is called a if there are multiple hidden layers; see . The simplest deep learning model has at least one hidden layer: an input layer and an output layer. This hierarchical structure makes it possible to formulate the deep learning model as a linear system of equations. Illustration of a deep neural network with three hidden layers The model input to a neural network is here defined as vector \\(\\mathbf{x}^{(i)}\\) with \\(Q\\) elements. The input is transformed linearly by \\(\\mathbf{W}_1\\) and \\(\\mathbf{b}\\) such that \\(f(\\mathbf{x}^{(i)}) = \\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b}\\). \\(\\mathbf{W}_1\\) transforms the input to a vector of \\(K\\) elements and is often called the weight matrix, while the translation \\(\\mathbf{b}\\) is referred to as the bias. The bias can be interpreted as a threshold for when the neuron activates. A nonlinear activation function is applied element wise to the transformed data. The activation function is typically given as a rectified linear unit (ReLU) Nair and Hinton (2010) \\(Relu(z) = max(0, z)\\) or \\(\\tanh\\) function. This activation introduces non-linearity to the other linear operations. The superposition of the linear and nonlinear transformation is in combination with the activation function and is what we refer to as a hidden layer. Applying another linear transformation \\(\\mathbf{W}_2\\) to the hidden layer results in this case to the model output or output layer. The size of the output layer is a row vector with \\(D\\) elements. Generally, many transformations and activations can be applied consecutively which will result in a more complex hierarchical model. A generalization to a network with several hidden layers is straightforward; to make this clear we here limit the notation to a single hidden layer. We note that \\(\\mathbf{x}^{(i)}\\) is vector of size \\(Q\\), \\(\\mathbf{W}_1\\) is a \\(K \\times Q\\) matrix that transforms the input to \\(K\\) elements, \\(\\mathbf{W}_2\\) is a \\(D \\times K\\) matrix, transforming the vector into \\(D\\) elements and \\(\\mathbf{b}\\) consists of \\(K\\) elements. We write this as linear system of equations transformed with the activation function \\(\\sigma\\) \\[\\widehat{\\mathbf{y}}^{(i)} = \\mathbf{W}_2 (\\sigma(\\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b})) := \\mathbf{f}^{\\omega}, \\qquad \\omega = \\{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}\\}\\] Depending on how the output layer is defined, we can use the network for classification or regression tasks. For classification purposes, the number of nodes in the output layer equals the number of classes, and typically transformed with a softmax function Goodfellow, Bengio, and Courville (2016). The softmax function is a generalization of the logistic map that normalizes the output relative to the different classes: \\[ \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K \\] In regression problems we want to estimate relations between variables; we want to predict a continuous output based on some input (variables). To use a linear activation function on the output layer will serve this purpose. It has been shown that ANN is a universal approximation Hornik, Stinchcombe, and White (1989); thus, our goal is to find the weights of the given network to best approximate the map from the input to the output. This means that we want to estimate the weights of the ANN \\(\\mathbf{\\omega}\\), given the input data \\(\\mathbf{x}^{(i)}\\), the target \\(\\mathbf{y}^{(i)}\\) such that the predictions \\(\\widehat{\\mathbf{y}}^{(i)}\\) is minimized towards the true target values \\(\\mathbf{y}^{(i)}\\). This is a typical optimization problem, which can be minimized with an objective function and optimization procedure. 8.2 Objective functions and training An objective function for use in deep learning typically contains two terms: cost function and regularization. The cost function takes the predicted and the true values as input. Depending on the task and what one wants to minimize, the cost function maximizes a likelihood. In classification problems this is can be the negative cross entropy \\[\\mathcal{C}_1^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) = - \\frac{1}{N}\\sum\\limits_{j=1}^{N}\\mathbf{y}^{(i)}_j\\log(\\widehat{\\mathbf{y}}^{(i)}_j) = -\\log p(\\mathbf{Y}|\\mathbf{f}^{\\mathbf{\\omega}}(\\mathbf{X})),\\] and in regression problems the Mean Squared Error (MSE) \\[\\mathcal{C}_2^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(\\mathbf{y}^{(i)} - \\widehat{\\mathbf{y}}^{(i)})^2 = - \\log p(\\mathbf{Y}|\\mathbf{f}^\\mathbf{\\omega}(\\mathbf{X})),\\] Minimization of the negative cross entropy and the MSE is well known to be equivalent to minimize the negative log likelihood of the parameter estimation Tishby, Levin, and Solla (1989) for neural networks. Depending on the task, minimizing or with respect to the parameters \\(\\omega = \\{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}\\}\\) maximizes the likelihood of these parameters. The choice of the cost function is not restricted to those given above, and depend on the data, the model structure and what one wants to predict with the model. One of the key problems in deep learning is a phenomenon called over-fitting (See also Lecture 3). Over-fitting occurs if the optimized model performs poorly on new unseen data, i.e. it does not generalize well. To address this problem, regularization is added to the cost function. Regularization is a general technique, where the goal is to make an ill posed problem well-posed. Over-fitting is basically one example of an ill-posed problem. For optimization problems, you could add a penalizing functional: L2 or L1 norm for the parameters; or use dropout Srivastava et al. (2014). Regularization in neural networks work by penalizing the cost function, e.g. forcing the weights to become small. The idea behind a specific regularization term could be to minimize the weights of the ANN to generate a simpler model that helps against over-fitting. \\(L2\\) regularization multiplied with some penalizing factor \\(\\lambda_i\\) is one of the most common regularization techniques. The cost function with regularization is called the objective function. Adding \\(L2\\) regularization to equation or result in the objective function \\[\\mathcal{L}(\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}) = \\mathcal{C}^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) + \\lambda_1||\\mathbf{W}_1||^2 + \\lambda_2||\\mathbf{W}_2||^2 + \\lambda_3||\\mathbf{b}||^2\\] Another common way of regularizing the cost function is through dropout, which is a stochastic regularization technique. In the example later we will utelize dropout as regularization. Minimizing the objective in \\(\\eqref{NN_cost_function}\\) with respect to the weights \\(\\mathbf{\\omega}\\) with an objective function and a gradient descent optimization method has proven to give good results in a wide range of applications. The gradient descent method Curry (1944) updates the parameter \\(\\mathbf{\\omega}\\) using the entire data set \\[\\mathbf{\\omega}_t = \\mathbf{\\omega}_{t-1} - \\eta \\nabla \\mathcal{L}(\\mathbf{\\omega}_{t-1}). \\] Here \\(\\omega_t\\) represents the current configuration of the weights, while \\(\\omega_{t-1}\\) represents the previous one. The parameter \\(\\eta\\) is referred to as the learning rate, i.e. how large the step in the negative gradient direction the update of the weights should be. Too small steps can lead to poor convergence, while to large steps can lead to overshooting, i.e. missing local/global minimums. Usually it is too expensive to calculate the gradient over the entire dataset. This is solved by a technique called stochastic gradient descent Robbins and Monro (1951). Stochastic gradient descent performs a parameter update for each training example. A natural extension and a more cost-efficient approach is the mini-batch gradient descent approach. In mini-batch optimization, the gradient is approximated by calculating the mean of the gradients on sub-sets or batches of the entire data set, \\[\\mathbf{\\omega}_t = \\mathbf{\\omega}_{t-1} - \\frac{\\eta}{n}\\sum\\limits_{i=1}^{n}\\nabla \\mathcal{L}_i(\\mathbf{\\omega}_{t-1}).\\] The mini-batch gradient descent iterative process can be implemented in the neural network with the back-propagation algorithm Rumelhart et al. (1988). In back-propagation, the weights are updated through a forward and backward pass. In the forward pass, we predict with the current weight configuration and compare towards the target values. In the backward pass, we use the chain rule successively from the output to the input to calculate the gradient of \\(\\omega\\). Based on the gradient direction and the learning rate, the configuration of the weights is updated. To find the correct learning rate is difficult, hence, several methods has been developed to adjust the learning rate adaptively. One of the disadvantages of the vanilla gradient descent approach to the ANN optimization problem, is that it has a fixed leaning rate \\(\\eta.\\) In line with the development of ANN, methods dedicated to deep learning and adaptive adjustment of the learning rate have been developed. Besides SGD with momentum Sutskever et al. (2013), the two most used optimization methods for ANNs are ADAM Kingma and Ba (2014) and Root Mean Square Propagation (RMSProp) Tieleman and Hinton (2012). RMSProp adaptively adjusts the learning rate of the gradients based on a running average for each of the individual parameters. The ADAM-algorithm individually adjusts the weights in terms of both the running average, but also with respect to the running variance. The use of back-propagation together with a stochastic gradient descent method, increase in available data and hardware have been the successes of deep learning during the past decade. 8.3 Validation of trained models To validate and ensure that the predictions of the deep learning model also performs well on new unseen instances, the data is split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. \\ After each epoch, we predict with this temporally model on the validation data set. Usually we put criteria on the performance on the validation data set for when to stop the optimization. We can use a so called early stopping regime, where the model stops training if it does not see improvement on the validation score after a certain number of epochs without improvement. The purpose of the test data set is to validate on new unseen data that has not been part of the training or the continuous validation of the model. Lets look at an example of how to implement a classifier on the mnist data set in keras. 8.4 Implementation of Feed Forward Neural Network in Keras In this section we will show you how to implement a Feed Forward Neural Network with dropout as regularization with the low level API for tensorflow called Keras. A lot of the development within deep learning is concentrated around the python programming language. Keras is also a tool that is originally developed in python, but also available in R. Lets start by installing tensorflow: 8.4.1 Installing Keras and Tensorflow For people who use Windows you must install miniconda first. You can do this through the reticulate package (The reticulate package provides a comprehensive set of tools for interoperability between Python and R.). You can install reticulate and subsequently miniconda with the following commands: install.packages(&quot;reticulate&quot;) reticulate::install_miniconda() If you have a linux or mac operating system, you should be able to run the subsequent commands directly. This is because you have python already installed on your computer. You can now install tensorflow with the following commands: install.packages(&#39;tensorflow&#39;) library(&#39;tensorflow&#39;) install_tensorflow() Then install keras with the following commands: install.package(&#39;keras&#39;) library(keras) install_keras() 8.4.2 Importing MNIST We will use the famous MNIST data set Deng (2012) as an example in this section. The MNIST dataset contains 60000 handwritten digits for training the model, and 10000 for testing. The digits has a shape of 28x28 pixels in grey scale, that is, they only have one channel, describing the black/white intensity of the pixel value. Each picture/instance is associated with a label, that is a number from 0-9. The keras package contains a function for downloading the MNIST dataset from the source. library(keras) ## ## Attaching package: &#39;keras&#39; ## The following object is masked from &#39;package:yardstick&#39;: ## ## get_weights mnist &lt;- dataset_mnist() ## Loaded Tensorflow version 2.8.0 To ensure that the predictions of the neural network model also performs well on new unseen instances, the data is often split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize/train the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. We will come back to validation and testing of the neural network later in this section. The MNIST dataset are split into a test and train data set from source # x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y We can check out some of the training data #checking the dimension of the train/test data library(ggplot2) # visualize the digits par(mfcol=c(5,5)) par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) for (idx in 1:25) { im &lt;- x_train[idx,,] im &lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=&#39;n&#39;, main=paste(y_train[idx])) } #checking the dimension of the train/test data dim(y_train) ## [1] 60000 head(y_train) ## [1] 5 0 4 1 9 2 Here we see that the training dataset has a shape of \\(60000 \\times 28 \\times28\\). This means that there are \\(60000\\) instances, or pictures of handwritten digits, where each of the pictures has \\(28 \\times 28\\) dimension in the horizontal and vertical direction. The dimension of the y_train variable represents the class or label of the x_train variable. 8.4.3 Pre-processing Pre-processing is the step where we transform, scale, removes, imputes etc. the data before we feed it to the deep learning model. Removing erroneous data, or impute NA values may be of crucial importance to get the algorithm run at all. Transforming or scaling the data helps improve the so called conditioning problem. Conditioning of a problem say something about the sensitivity of the solution to changes in the problem data. To create an easier problem to optimize, we pre-process the MNIST-data before we are feeding it to the neural network model. Here we reshape and scale the data. # reshape x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784)) x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784)) # rescale x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 We also need to convert the y_train and y_test to categorical variables. Instead of having a vector with a number from 0-9, we convert it to a binary representation. That is, each label is represented by a vector of length 10 with one element is 1 and the rest 0. The element that is 1 represent the label of the digit. y_train &lt;- to_categorical(y_train, num_classes = NULL) y_test &lt;- to_categorical(y_test, num_classes = NULL) head(y_train) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 0 0 0 1 0 0 0 0 ## [2,] 1 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 1 0 0 0 0 0 ## [4,] 0 1 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 0 1 ## [6,] 0 0 1 0 0 0 0 0 0 0 8.4.4 Feed Forward Neural Network model In this example we want to create a so called dense neural network, or feed forward neural network. That is, each node are influenced from all of the nodes in the previous layer. This is contrast to e.g. convolutional neural networks that uses convolutions to reduce the connectivity between layers, thus reducing the connections. We construct a dense neural network. After reshaping the input, the network has a shape of \\(28 \\times 28 = 784\\) per instance. This is our input shape. This shape has to be specified in the model, as shown below. The first layer has \\(256\\) nodes, and we use the rectified linear unit as activations function. The next layer is a so called dropout layer. This layer drops randomly a proportion of the nodes out during a forward and backward pass. This will help avoiding overfitting, i.e. it is a form of regularization of the network. The next hidden layer has \\(128\\) nodes, and also ReLU activation function. The last layer of the network is the output. The output in this case, is to classify the label of the digit. There are 10 possible classes, 0-9 and we thus use a softmax activation function, with 10 units. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(784)) %&gt;% layer_dropout(rate = 0.4) %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;) First we construct a sequential model object. Then we can use the pipe syntax to specify the layers of the neural network. We can use the summary function to show a nice overview of the neural network model. summary(model) ## Model: &quot;sequential&quot; ## _______________________________________________________________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ======================================================================================================================================= ## dense_2 (Dense) (None, 256) 200960 ## ## dropout_1 (Dropout) (None, 256) 0 ## ## dense_1 (Dense) (None, 128) 32896 ## ## dropout (Dropout) (None, 128) 0 ## ## dense (Dense) (None, 10) 1290 ## ## ======================================================================================================================================= ## Total params: 235,146 ## Trainable params: 235,146 ## Non-trainable params: 0 ## _______________________________________________________________________________________________________________________________________ 8.4.5 Compiling and training the model We now have a model, the next step is to compile it. Durring compiling we also have to specify the loss function, optimizer and metrics for evaluations and validation. model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39; ) Since we want to classify the digits, we have chosen the loss to be categorical cross entropy. We use the adam-algorithm during optimization and we also specify that we want to return the accuaracy of the classification during training and validation. After the compilation, we can call the fit() function to train our model. The fit function take x_train and y_train as inputs. We also have to specify how many epoch the model should be trained on (how many times we will see the entire data set), the batch size (how many/large the splits of the training data set) and how large proportion of the data should be used as online validation data. Test data are set aside for evaluation the model when it has finished training. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2 ) It is possible to specify specific validation data if that is desirable. Batch size, epochs, size of the neural network, e.g. number of layers nodes, type of network (e.g. cnn, rnns) are so called hyper parameters. This is parameters that are not optimized in the network itself, but has to be specified manually. Theses hyper parameters are important and there are different strategies for optimize them. Hyper parameter search could be done with traditional grid search, but also more sophisticated approaches such as Bayesian hyper optimization. (#fig:Validation_plot)Loss and accuarracy after each training epoch. We have plotted the loss and accuaracy calculated after each training epoch. It can be observed that the loss after approx 10-15 epochs, is significantly lower than the validation loss. This is an indication that the model starts to overfit. 8.4.6 Evaluating the model To evaluate the trained model on the test data, we can use the pipe syntax together with the evaluate function model %&gt;% evaluate(x_test, y_test) ## loss accuracy ## 0.0749504 0.9825000 The trained model performes pretty well on the test data set. It has an accuaracy of approximatley 0.98, that is 98 \\(\\%\\) of the data in the test data set is classified correctly. Lets ### Predicting We can also predict the most probable class for each of the instances. For this we can use the predict function. (Litt mer her…) #model %&gt;% predict(x_test) %&gt;% k_argmax() prediction &lt;- model %&gt;% predict(x_test) #plot(prediction[1710,], ) dim(prediction) ## [1] 10000 10 #Lets find one of the most uncertain predictions to view how it looks like #We find the most probable predictions in the test data set largest &lt;- apply(prediction,1,max,na.rm=TRUE) #Then we choose the predictions which has the lowest confident less_confident &lt;- sort(largest,index.return = TRUE)$ix[1:36] #We find out what the model predicts these less_confident predictions are pred_class &lt;- prediction %&gt;% k_argmax() #and plot the digits par(mfcol=c(6,6)) par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) for (idx in less_confident) { im &lt;- mnist$test$x[idx,,] im &lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=&#39;n&#39;, main=paste(mnist$test$y[idx])) } Data camp We highly recommend the data camp course Introduction to TensorFlow in R chapters 1-3. References "],["lecture9.html", " 9 Feature selection/Explainable AI", " 9 Feature selection/Explainable AI "],["references.html", "References", " References "]]
