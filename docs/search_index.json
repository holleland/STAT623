[["index.html", "Data science with R: Applied Predictive Modelling Compendium for STAT623 Course overview Learning outcomes and objectives Lecture overview Litterature", " Data science with R: Applied Predictive Modelling Compendium for STAT623 Sondre Hølleland and Kristian Gundersen 2022-04-24 09:51:26 Course overview Learning outcomes and objectives The course presents various advanced methods within data science for predictive modelling and the use of R. Methods for regression, including non-linear regression and generalized additive models, and methods for classification, including trees, boosting and Support Vector Machines, will be examined. The course will focus on practical use in r, without going into details of the mathematical theory of the methods. On completion of the course the student should have the following learning outcomes: Knowledge Knows the basic ideas underpinning carious methods in data science/predictive modelling Skills Can implement various models within data science/predictive modelling in R Use data science methods on real data sets and perform predictions General competence Have an overview of how data science methods can be used to analyze larger data sets Lecture overview Lecture Subject Exercises Datacamp 1 Introduction and short recap of R and Data preprocessing Recap of R Introduction to Regression in R (ch 1-2) 2 Over-fitting and model tuning, selection and evaluation and multiple regression Multiple regression Supervised Learning in R: Regression (ch 1-2) 3 Non-linear regression GAMs Nonlinear modeling in R with GAMs (ch 1-2) 4 Classification methods Supervised Learning in R: Classification (ch 1-3) 5 Decision Trees and Bagged trees Machine Learning with Tree-Based Models in R (ch 1-3) 6 Random forrest and boosting xgboost Machine Learning with Tree-Based Models in R (ch 3-4) 7 Support vector machines Support Vector Machines in R (ch 1-2) 8 Neural Networks Introduction to TensorFlow in R (ch 1-3) 9 Feature selection/Explainable AI Supervised Learning in R: Classification (ch 3, video on automatic feature selection Litterature We will use many different sources for teaching you applied predictive modelling. Kuhn and Johnson (2016) and James et al. (2013) are the main references. References "],["lecture1.html", " 1 Introduction and short recap of R 1.1 Introduction 1.2 Recap of R 1.3 Data preprocessing 1.4 Case study", " 1 Introduction and short recap of R Goals: In this Lecture 1 we will give a introduction to the course and a short recap of using R. 1.1 Introduction This course is called applied predictive modelling. What do we actually mean by these three words? Well, applied is as opposed to theoretical - indicating that this will not be a very theoretical course. We will focus on usage of the different methods that you will learn throughout the course. Predictive modelling means that the overall goal of the modelling is to predict something or make a prediction. Gkisser (1993) defines predictive modelling as the process by which a model is created or chosen to try to best predict the probability of an outcome, while Kuhn and Johnson (2016) define it as the process of developing a mathematical tool or model that generates an accurate prediction and the give the following examples of types of questions one could be interested in predicting: How many copies will a book sell? Will this customer move their buisness to a different company? How much will my house sell for in the current market? Does a patient have a specific disease? Based on past choices, which movies will interest this viewer? Should I sell this stock? Which people should we match in our online dating service? Is an email spam? Will this patient respond to this therapy? Examples of stakeholders or users of predictive modelling can be insurance companies. They need to quantify individual risk of potential policy holders. If the risk is too high, they may not offer the potential customer insurance or they may use the quantified risk to set the insurance premium. Governments may use predictive models to detect fraud or identify terror suspects. When you go to a grocery store and sign up for some discounts, usually either by creating an account or registering your credit card, your purchase information is being collected and analyzed in an attempt to who you are and what you want. Predictive modelling can often a good thing. When for instance Netflix learns what kind of TV-shows you prefer to watch, they can come up with suggestions for other TV-shows that you are likely to also enjoy. This can often be recognized by statements like people who liked TV-series A, also liked TV-series B. For the streaming provider the goal is to keep you as an entertained, satisfied and paying customer, but it is also in your interest to find entertainment that you like. But you have probably also noticed that sometimes, these predictions are quite inaccurate and provide the wrong answer. For instance, you did not like the suggested series or you did not receive an important email because it was wrongly classified as spam by (predictive) spam filter. Kuhn and Johnson (2016) give four reasons for why predictive models fail: Inadequate pre-processing of the data. Inadequate model validation. Unjustified extrapolation. Over-fitting the model to existing data. They also mention the fact that modellers tend to explore realtively few models when searching for predictive realationships. This is usually because the modeler has a preference for a certain type of model (the one he/she has used before and know well). It can also be due to software availability. We will cover (at least) some of these aspects in this course. 1.1.1 Prediction or interpretation? In the examples listed above, the main goal is to predict something and there are likely data available to train a statistical model to do so in most of the cases. Note that we are not so interested in answering questions of why something happens or not. Our primary interest is to accurately predict the probability that something will, or will not, happen. In situations like these, we should not be worried with having interpretative models, and focus our attention on prediction accuracy. If your spam filter classifies an email as spam, you would not care why the filter did so, as long as you receive the emails you care about and not the ones you do not. An interpretative model can be a model for a certain stocks value that can support statements like the stock value prediction went up, because the company released information about a big contract. One can explain why the model behaves the way it does. The alternative is often called a black box, meaning that you put your data in on one side of the box and on the other the prediction pops out, without you knowing what happened inside the box. So, our primary interest is prediction accuracy. Cannot interpretability be our secondary target, so we can understand why it works? Kuhn and Johnson (2016) writes that the unfortunate reality is that as we push towards higher accuracy, models become more complex and their interpretability becomes more difficult. 1.1.2 Terminology In the predictive modelling terminology there are quite a lot of things that mean the same thing. We will list some terminology here with some explanations (see Kuhn and Johnson (2016, p6)): The terms sample, data point, observation, or instance refer to a single, independent unit of data, e.g. a customer, a patient, a transaction, an individual. Sample can also refer to a collection of data points (a subset). The training set consists of data used to develop the models while the test and validation sets are used solely for evaluating the performance of the final model. The predictors, idendependent variables, attributes, descriptors, or covariates are the data used as input for the prediction equation (e.g. what you feed into the black box). Outcome, dependent variable, target, class, or response refer to the outcome of the event or qunatity that is being predicted (e.g. what comes out of the black box). Continuous data have natural, numeric scales (e.g. blood pressure, price of an item, a persons body mass index, number of bathrooms, etc). Categorical, nominal, attribute or discrete data all mean the same thing. These are data types that take on specific values that have no scale (e.g. credit status (good or bad) or color (red, white, blue)) Model building, model training, and parameter estimation all refer to the process of using data to determine values of model equations. 1.2 Recap of R 1.2.1 Installing R and Rstudio If you have not already installed the newest version of R and Rstudio on your personal computer, we will now tell you how. This installation guide will be based on Windows, but iOS and Linux are very similar and we will indicate where to deviate. The video below shows the same steps as listed. First we install R: Go to https://cran.uib.no. Select Download R for Windows (or macOS/Linux) (this will be the newest R version). Go to your Downloads folder and run the .exe file. Use standard settings throughout the installation steps (click next-next-next-etc.). R is now installed. Now that R is installed, we can install Rstudio: Go to https://www.rstudio.com/products/rstudio/download/. Press Download under the Rstudio Desktop - Open source licenece - Free. Press Download Rstudio for Windows. I guess if you have macOS/Linux it will detect that automatically. If not, select it manually from the list below. Go to your Downloads folder and run the Rstudio.exe file. Use standard settings throughout the installation steps (click next-next-next-etc.). RStudio is now installed. Open RStudio and check that the print-out in the console matches the R version you just installed. At the time when this is written the newest version is 4.1.1 (2021-08-10) Kick Things. The printout at the top looks like this: R version 4.1.1 (2021-08-10)  Kick Things Copyright (C) 2021 The R Foundation for Statistical Computing Platform: i386-w64-mingw32/i386 (32-bit) If you had R and Rstudio already, but updated to the newest version, remember that you will have to also install all your packages again. 1.2.2 R community and packages Currently, there are over 18 000 packages available on the official CRAN server - open source and free. If you have a problem and need a smart function for solving that, you will most likely find that someone else have had the same problem, already solved it and made an R package that you can use. The large R community is also very active on the net forum stackexchange.com. If you have a programming issue and google it, you will typically end up in one of these forums where someone else has posted a question similar to yours and others from the community has provided a solution. I have been using R for over 10 years now and this strategy for solving programming issues has not failed me yet. Once you have found an R package you want to install, you can install it using the install.packages function. We will use the tidyverse package developed by Wickham et al. (2019) as an example. This is a bundling package built up of many packages, which we will use throughout the course. install.packages(&quot;tidyverse&quot;) To load a package into you R environment, making all its functionality available to you, you can use the library or require functions. The two are quite equal, but if you do not have the package installed require will cast a warning and continue, while library will cast an error and stop. library(tidyverse) As part of the output here, you can see all the packages that tidyverse attaches to your working environment. We will be using many of these. 1.2.3 Datacamp As a part of the course, you will be given access to Datacamp. We will suggest courses to take there in combination with the lectures provided. If you want a more thorough recap of using R, you can take the course Introduction to R or test yourself with the practice module Introduction to R. 1.3 Data preprocessing Data pre-processing techniques typically involve addition, deletion or transformation of training set data. Preparing the data in a good way can make or break the predictive ability of a model. How the predictors enter the model is important. By transforming the data before feeding them to the model one may reduce the impact of data skewness or outliers. This can signifcantly improve the models performance. The need for data pre-processing may depend on the method you are using. A tree-based model is notably insensitive to the characteristics of the predictor data, while a linear regression is not. How the predictors are encoded, called Feature engineering, can also have a huge impact on the model performance. There are many ways to encode the same information. Combining multiple predictors can sometimes be more effective than using the individual ones. For example, using the body mass index (BMI = weight/length\\({}^2\\)) instead of using length and weight as separate predictors. The modelers understanding of the problem can often help in choosing the most effective encoding. Different ways of encoding a date predictor can be The number of days since a reference date Using the month, year and day of the week as separate predictors The numeric day of the year (julian day) Whether the date was within the school year (as opposed to holiday) 1.4 Case study We will use the same example as Kuhn and Johnson (2016) from the R package install.packages(&quot;AppliedPredictiveModeling&quot;) 1.4.1 Data transformations for individual predictors library(AppliedPredictiveModeling) data(&quot;segmentationOriginal&quot;) segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;) 1.4.2 Centering and scaling The most straightforward and common transformation of data is to center and scale the predictor variables. Let \\(\\textbf{x}\\) be the predictor, \\(\\overline{x}\\) its average value and \\(sd(x)\\) its standard deviation. Centering means to substract the average predictor value from all the predictor values (\\(\\textbf{e} = \\textbf{x}-\\overline{x}\\)) and scaling means dividing all values by the empirical standard deviation of the predictor (\\(\\textbf{e}/sd(x)\\)). By centering the predictor, the predictor has zero mean and by scaling, you coerce the value to have a common standard deviation of one. A common term for a centered and scaled predictor is a standarized predictor, indicating that if you do this to all predictors they are all on a common scale. In the Figure 1.1 below, we have simulated 1000 normally distributed observations with mean 22 and standard deviation 4 and plotted a histogram of them (left panel) and a histogram of them standardized (right panel). As you can see, the distribution looks the same, but the scale on the x-axes has changed. library(ggplot2) set.seed(123) x &lt;- rnorm(1000, mean = 22, sd = 4) df &lt;- data.frame(x = c(x, (x-mean(x))/sd(x)), mean = rep(c(22, 0),each = 1000), type = rep(c(&quot;Original&quot;, &quot;Standardized&quot;), each = 1000)) ggplot(data= df,aes(x = x, fill = type)) + geom_histogram() + facet_wrap(~type, scales = &quot;free&quot;) + geom_vline(aes(xintercept = mean), col = 1, lty = 2) + scale_y_continuous(expand =c(0,0)) + theme(legend.title = element_blank(), legend.position = &quot;none&quot;)+ xlab(&quot;&quot;) Figure1.1: Simulated normally distributed variables on original- and standardized scale. 1.4.3 Tranformations to resolve skewness As noted above, standardizing preserves the distribution, but sometimes we need to remove distributional skewness. An un-skewed distribution is roughly symmetric around the mean. A right-skewed distribution has a larger probability of falling on the left side (i.e. small values) than on the right side (large values). We have illustrated left-, right- and un-skewed normal distributions in Figure 1.2. x &lt;- seq(-4,4,by = .1) alpha &lt;- c(-5,0,5) df &lt;- expand.grid(&quot;x&quot; = x,&quot;alpha&quot; = alpha) df &lt;- df %&gt;% mutate(case = ifelse(alpha == -5, &quot;Left skewed&quot;, ifelse(alpha ==5, &quot;Right skewed&quot;, &quot;Unskewed&quot;)), delta = alpha/sqrt(1+alpha^2), omega = 1/sqrt(1-2*delta^2/pi), dens = sn::dsn(x,xi = -omega*delta*sqrt(2/pi), alpha =alpha, omega = omega)) ggplot(df, aes(x = x,y = dens)) + geom_line() + facet_wrap( ~case, scales = &quot;fixed&quot;, nrow = 3, strip.position = &quot;right&quot;) + geom_vline(xintercept = 0, lty = 2) + xlab(&quot;&quot;) + ylab(&quot;Density&quot;) Figure1.2: Skewed normal distributions with mean 0 and standard deviation 1. A rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. One can calculate the skewness statistic and use that as a diagnostic. Is it close to zero, the distribution if roughly symmetric. Is it large, indicates right-skewness and negative values indicates left-skewness. The formula for the sample skewness statistic is given by \\[\\text{skewness} = \\frac{\\sum_{i=1}^n (x_i-\\overline{x})^3}{(n-1)v^{3/2}},\\quad \\text{where}\\quad v = (n-1)^{-1}\\sum_{i=1}^n(x_i-\\overline{x})^2,\\] \\(x\\) is the predictor variable, \\(n\\) the number of values and \\(\\overline{x}\\) is sample mean. Transforming the predictor with the log, square root or inverse may help remove the skew. Alternatively, a Box-Cox transformation can be used. The Box-Cox family of transformations are indexed by a parameter denoted \\(\\lambda\\) and defined by \\[x^\\star = \\begin{cases} \\frac{x^\\lambda-1}\\lambda, &amp; \\text{if }\\lambda\\neq 0\\\\ \\log(x), &amp; \\text{if }\\lambda = 0. \\end{cases}\\] In addition to the log transformation, the family can identify square transformation (\\(\\lambda = 2\\)), square root (\\(\\lambda = 0.5\\)), inverse (\\(\\lambda = -1\\)) and other in-between. By using maximum likelihood estimation on the training data, \\(\\lambda\\) can be estimated for each predictor that contain values greater than zero. References "],["lecture2.html", " 2 Over-fitting and model tuning, selection and evaluation and multiple regression 2.1 Overfitting 2.2 Training, validation and test split 2.3 Multiple regression", " 2 Over-fitting and model tuning, selection and evaluation and multiple regression In this lecture we will cover the terms overfitting, training-, validation- and test sets, model selection and -evaluation. These terms are part of the chargong of predictive modelling and is something we need to get familiar with before learning about specific models. Towards the end of this lecture, we will learn about multiple regression. 2.1 Overfitting Overfitting is the phenomenon when you build a model that fits (almost) perfectly to the training set, but when applied to new data the performance is low. For the modeller there is a balance between choosing a model that has a good fit to the training data, but also generalize well to new data. The goal for the model should be to discover the underlying signal and not fit to all the noise. We will illustrate overfitting by an example with simulated data and GAM models (topic for the lecture Lecture 3) set.seed(3) x &lt;- seq(0,2*pi,0.1) z &lt;- sin(x) y &lt;- z + rnorm(mean=0, sd=0.5*sd(z), n=length(x)) df &lt;- cbind.data.frame(x,y,z) p &lt;- ggplot(df, aes(x = x, y = y)) + geom_point() + geom_line(aes(y=z), lwd = .8, col = 2) p In the figure above, the true underlying signal is the red curve while the black dots are the observations. If we fit a model with a high level of flexibility, we can make it fit well to the observations. p + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k = 50, sp = 0)) We will come back to the details of the model setup when discussing GAM models in the next lecture, but the point here is that we have turned off the likelihood penalization by setting sp=0 in the smoother function s and set the order of the smoother to a high number (k = 50). If we set the order to k = 4 and let the GAM procedure set the penalization itself, we get a much smoother curve that lies closer to the true signal. p + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k = 4)) The first model explains 96.3% of the deviance, while the second explains 81.7%. In a normal situation, we would of course not know the true underlying signal, but overfitting can be suspected when you get such a wiggly prediction curve that seems to follow every little move in the observations. Judging by the fit to the points, the first GAM model is much closer to the observations, but we can check how well it would perform on a new set of data with the same structure. library(mgcv) # package for fitting gam models # Fit model with high level of complexity: fit1 &lt;- gam(y ~ s(x, k= 50, sp = 0), data = df) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x, k = 50, sp = 0) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02501 0.04060 -0.616 0.549 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 49 49 6.895 0.000262 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.823 Deviance explained = 96.3% ## GCV = 0.50314 Scale est. = 0.10382 n = 63 # Fit simpler model: fit2 &lt;- gam(y ~ s(x, k= 4), data = df) summary(fit2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x, k = 4) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02501 0.04238 -0.59 0.557 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 2.988 3 87.32 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.807 Deviance explained = 81.7% ## GCV = 0.12081 Scale est. = 0.11317 n = 63 # simulate new set of data: set.seed(4) newdata &lt;- data.frame(x, z, y = z+rnorm(mean=0, sd=0.5*sd(z), n=length(x))) # Make predictions from the two models: newdata$pred1 &lt;- predict(fit1, newdata=newdata) newdata$pred2 &lt;- predict(fit2, newdata=newdata) # Calculate sum of squared residuals: with(newdata, c(&quot;SS1&quot; = sum((y-pred1)^2), &quot;SS2&quot; = sum((y-pred2)^2))) ## SS1 SS2 ## 14.160736 7.734603 As you can see, the sum of squared residuals is almost twice as high for the complex model when applied to new observations, compared to the simpler model. In a preditive modelling situation, it is the prediction performance that is important and not as much the deviance explained on training data. Below is a figure with the two predicition curves on the new data. Since \\(x\\) is the only input to the models, the two lines are equal to the curves above, but the data is new. # Plot the new data: ggplot(data= newdata, aes(x=x,y=y)) + geom_point() + geom_line(aes(y=pred1, col = &quot;fit1&quot;))+ geom_line(aes(y=pred2, col = &quot;fit2&quot;)) 2.2 Training, validation and test split In order to evaluate a predictive models performance, we should test its predictive abilities on data the model has never seen before (during model fitting). Usually, you only have one dataset to start with, so it is common to split the original data into three subsets; a training set, a validation set and a test set. The training set is the data used to train the model, meaning estimating its parameters. If your original data is split, this will typically be the biggest portion of the data. One should measure the model performance on the training data, but this should not be used solely for model selection. If your model has a very good fit to the training data, this can often lead to overfitting issues when applying the model to new data. This is one of the reasons for using a validation set. The validation set is used for tackling overfitting and doing model selection. This is a smaller portion of the data not seen during training, which is key. We can therefore measure the models performance on these unseen data and if the model performs similarly as on the traning data, this means it generalizes well to new situations and overfitting is likely not a big issue. If the performance is high in training and low for validation, this is a sign of overfitting. If you are in a scenario where you have many different candidate models, either from different model families or the same model but with different setups, you can use the validation set to select the best model according to the performance criteria you have chosen. Once you have found a model that does not overfit to training data and performs well on the validation set, you can apply it to the test set. This should only be used to measure the models performance. In some cases, one sees modellers only splitting the data in two, a training and testing set. If you use the validation set to measure the models performance, this will give a biased estimate of model performance, since you have selected the model based on the validation set. Basically, you will not know whether your model performs better because of changes you made or because it just happened to fit the validation set better. Therefore, we need a separate test set. The performance on the test set should ideally be similar to the performance on the validation set. If it is significantly lower, this can indicate overfitting to the validation set. Figure2.1: Summary of the three-way data split. We will illustrate how this three-way split can be used on an example, after we have learned about multiple regression. 2.3 Multiple regression Multiple regression is an extension of simple linear regression, where more than one covariate is used to predict the value of the dependent variable (the Y). The form of the predictor is a linear combination of the set of explanatory variables, i.e. \\[\\widehat Y_i = \\beta_0 + \\beta_1X_{i1} + \\ldots + \\beta_p X_{ip},\\quad i=1,\\ldots,n,\\] where \\(\\widehat Y_i\\) is the predictor of observation \\(i\\), \\(\\beta_0, \\ldots, \\beta_p\\) are the parameters and \\(X_{\\cdot 1}, \\ldots, X_{\\cdot p}\\) are the vectors of explanatory variables. It is quite common to write the equation above on vector form. Define the vectors \\(\\widehat{\\mathbf{Y}}=(\\widehat Y_1, \\ldots, \\widehat Y_n)&#39;\\) and \\(\\boldsymbol{\\beta} = (\\beta_0, \\ldots, \\beta_p)&#39;\\), and the design matrix \\(\\mathbb X = (\\mathbf{1}, \\mathbf X_{\\cdot 1},\\ldots, \\mathbf X_{\\cdot p})\\). Then the equation above can be written as \\[\\widehat{\\mathbf{Y}} = \\mathbb X\\,\\boldsymbol\\beta.\\] The assumptions are that the residuals, \\(Z_i = Y_i - \\widehat Y_i\\), \\(i=1,\\ldots, n\\) are independent and normally distributed. The explanatory variables should not be correlated. Parameters are estimated using maximum likelihood estimation. Note that we are using capital letters here. This is a statistical convention when we are talking about variables. Once we introduce observations, we switch to small letters for the same quantities. To learn more about multiple regression, take the datacamp course Multiple and Logistic Regression in R. For now you can focus on the multiple regression part. We will simply go on to illustrate usage by an example. 2.3.1 Example In this example, we consider a dataset containing the impact of three advertising medias (youtube, facebook and newspaper) on sales for different companies. The advertising budgets and sales are in thousands of dollars and the advertising experiment has been repeated 200 times. We will use multiple regression to model the relationship between sales and the advertising budgets from the different medias. In the video below we walk you through the example, but you can also read it below. We start by looking at the data: # load data: data(&quot;marketing&quot;, package = &quot;datarium&quot;) head(marketing) ## youtube facebook newspaper sales ## 1 276.12 45.36 83.04 26.52 ## 2 53.40 47.16 54.12 12.48 ## 3 20.64 55.08 83.16 11.16 ## 4 181.80 49.56 70.20 22.20 ## 5 216.96 12.96 70.08 15.48 ## 6 10.44 58.68 90.00 8.64 plot(marketing) From looking at the plot above, it does not seem to be a very strong correlation between the three covariates: youtube, facebook and newspaper. This is based on that the first 3x3 scatters plots seem to be randomly distributed without any clear patterns. Looking at the last row of panels we see the marginal relationships between the covariates and the response; sales. Here it looks like the marginal relationship between youtube and facebook variables is close to linear, while for newspaper it does not look like a linear relationship is well suited. We will therefore use facebook and youtube to predict sales. We will also check if including newspaper can improve the fit. We create a train and a test set, by doing a 80-20 random split (80% for training set - 20% test set): set.seed(123) train.ind &lt;- sample(1:nrow(marketing), nrow(marketing)*.8, replace = FALSE) trainset &lt;- marketing[train.ind, ] testset &lt;- marketing[-train.ind, ] We start by fitting the following model: \\[\\mathrm{sales} = \\beta_0 + \\beta_1\\cdot \\mathrm{youtube} + \\beta_2\\cdot \\mathrm{facebook}+\\beta_3\\cdot \\mathrm{youtube}\\cdot\\mathrm{facebook}\\] This model can be set up by different formula arguments in R. The different model calls below are equivalent. mod1 &lt;- lm(sales ~ youtube + facebook + youtube:facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ youtube + facebook + youtube:facebook, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 mod1 &lt;- lm(sales ~ 1 + youtube + facebook + youtube:facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ 1 + youtube + facebook + youtube:facebook, ## data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 mod1 &lt;- lm(sales ~ youtube * facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ youtube * facebook, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 As you can see from the outputs, all models are equivalent. If you want to learn more about setting the formula argument, check out the helpsite for the formula function: ?stats::formula We will choose the model that has the highest predictive ability. We will therefore suggest several models and choose the one that has the lowest value of Akaikes information criteria (AIC). We will also evaluate the different models on the test set. mod2 &lt;- lm(sales ~ youtube * facebook * newspaper, data = trainset) mod3 &lt;- lm(sales ~ youtube * facebook + newspaper, data = trainset) mod4 &lt;- lm(sales ~ youtube, data = trainset) mod5 &lt;- lm(sales ~ facebook, data = trainset) mod6 &lt;- lm(sales ~ newspaper, data = trainset) AIC(mod1, mod2, mod3, mod4, mod5, mod6) ## df AIC ## mod1 5 511.4668 ## mod2 9 517.5961 ## mod3 6 512.9814 ## mod4 3 894.2953 ## mod5 3 986.9928 ## mod6 3 1043.6815 We choose the model with lowest AIC value. As you can see from the output above, this is the model we named mod1. Adding newspaper as covariate will provide the model with more information, but the cost of adding more parameters to be estimated is deemed higher than the benefit of including this information in the model according to AIC. We can also use the predictive abilities of the models on the test set to choose model. We will restrict ourselves to the top three model based on AIC and use root mean square error (RMSE) to assess the quality of the predictions. First we evaluate the in-sample prediction. That is, we calculate predictions on the training set and summarize by RMSE. trainset %&gt;% bind_cols( mod1 = predict(mod1, newdata = trainset), mod2 = predict(mod2, newdata = trainset), mod3 = predict(mod3, newdata = trainset) ) %&gt;% pivot_longer(cols = 5:7, names_to = &quot;models&quot;, values_to = &quot;pred&quot;) %&gt;% group_by(models) %&gt;% summarize(RMSE = sqrt(mean((sales-pred)^2))) ## # A tibble: 3 x 2 ## models RMSE ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 1.16 ## 2 mod2 1.15 ## 3 mod3 1.16 Perhaps not very surpising, the most complex model has the lowest in-sample, but the differences seem small. Lets evaluate the models on the test set as well. predictions = testset %&gt;% bind_cols( mod1 = predict(mod1, newdata = testset), mod2 = predict(mod2, newdata = testset), mod3 = predict(mod3, newdata = testset) ) %&gt;% pivot_longer(cols = 5:7, names_to = &quot;models&quot;, values_to = &quot;pred&quot;) head(predictions) ## # A tibble: 6 x 6 ## youtube facebook newspaper sales models pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 240. 3.12 25.4 12.7 mod1 13.4 ## 2 240. 3.12 25.4 12.7 mod2 13.4 ## 3 240. 3.12 25.4 12.7 mod3 13.4 ## 4 245. 39.5 55.2 22.8 mod1 22.7 ## 5 245. 39.5 55.2 22.8 mod2 22.7 ## 6 245. 39.5 55.2 22.8 mod3 22.7 predictions %&gt;% group_by(models) %&gt;% summarize(RMSE = sqrt(mean((sales-pred)^2))) ## # A tibble: 3 x 2 ## models RMSE ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 0.957 ## 2 mod2 0.916 ## 3 mod3 0.967 We see that the RMSE is a bit lower for the test set. This indicates that we are not overfitting our models at least, since the models perform better on the test set. Solely based on the prediction on the test set, we would choose mod2, which is the full model, including youtube, facebook and newspaper with all interaction terms included. Depending on what one will use the model for, one may choose mod1 or mod2. We can look at the summary output of mod2. summary(mod2) ## ## Call: ## lm(formula = sales ~ youtube * facebook * newspaper, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0556 -0.4672 0.2466 0.7246 1.7399 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.933e+00 6.277e-01 12.640 &lt; 2e-16 *** ## youtube 1.937e-02 2.996e-03 6.466 1.3e-09 *** ## facebook 1.860e-02 1.872e-02 0.994 0.322 ## newspaper 8.807e-03 1.945e-02 0.453 0.651 ## youtube:facebook 9.622e-04 9.108e-05 10.564 &lt; 2e-16 *** ## youtube:newspaper -2.513e-05 8.613e-05 -0.292 0.771 ## facebook:newspaper 2.382e-05 4.514e-04 0.053 0.958 ## youtube:facebook:newspaper -4.273e-07 2.092e-06 -0.204 0.838 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.183 on 152 degrees of freedom ## Multiple R-squared: 0.9659, Adjusted R-squared: 0.9644 ## F-statistic: 615.9 on 7 and 152 DF, p-value: &lt; 2.2e-16 Based on classical statistics, you would fix the non-significant estimates to zero (p-values below 5%) and choose a more parsimoneous model, like mod1. This example shows the difference between modelling for prediction (choose mod2) and modelling for interpretation (choose mod1). We can also look at the observations vs predictions plots for the three models we compared in terms of RMSE. It seems to be very small differences between the predictions from the different models. ggplot(predictions, aes(x = pred, y = sales)) + geom_point() + geom_abline(intercept = 0, slope = 1, col = 2, lty = 2) + facet_wrap(~models, ncol= 3) + xlab(&quot;Predicted&quot;) + ylab(&quot;Observed&quot;) "],["lecture3.html", " 3 Non-linear regression 3.1 GAM example", " 3 Non-linear regression As we saw in the previous lecture in the example illustrating the overfitting concept, we do not always have a linear relationship between the response and the covariates. In that example, the relationship was a periodic sine function. Other examples of non-linear relations can be polynomial-, exponential- and logistical functions. There are many non-linear regression models designed for solving different problems, but we will mainly focus on the generalized additive models (GAMs). These are quite closely related to generalized linear models (GLMs). There are a few options for R packages for GAMs, but we will focus on the mgcv package (Wood 2017). (#fig:examples of nonlinear relations)Examples of relationsships between x and y. In the figure above, we illustrate different types of relationships between x and y. The green one is a linear one, while the others are non-linear. The red curve is even an additive combination of some of the other types of curves, i.e.  \\[ y(x) = -0.3 x + 0.5 \\sin(x)+ 0.5 \\log(x+1) - 0.02 x^2 + \\exp((x+1)/10).\\] This is actually not so different from the principal idea behind GAMs. A GAM uses a set of basis functions, often called smoothers, to map the dependent variables and use the transformed variables as covariates in an additive manner. We will explain this further. For a simple linear model, we have that the expected value of the \\(i\\)th variable is \\(\\mathrm{E} Y_i = \\mu_i = \\beta_0+\\beta_1 X_i\\). For a generalized linear model, the relationship is mapped using a link function \\(g\\), such that \\(g(\\mu_i) = \\beta_0+\\beta_1X_i\\). For a generalized additive model we take it one step further, by also mapping the dependent variable: \\[g(\\mu_i) = \\beta_0 + \\sum_{j=0}^k \\beta_j f_j(X_i),\\] where \\(\\{f_j:\\, j = 1,\\ldots, k\\}\\) is a set of basis functions. The standard in the mgcv package is to use thin plate splines. 3.1 GAM example In the video below, Sondre goes through this sections example in a bit more detail and with more in-depth explanations of the code. You can also read the example below the video. We will simulate a dataset where the true relationship between X and Y is given by (for observation \\(i\\)), \\[Y_i = 0.5X_i + \\sin(X_i)+Z_i, \\quad \\text{where } Z_i\\sim N(0,0.29^2).\\] We start by generating the data in a tibble data frame: set.seed(123) # seed for reproducibility dat &lt;- tibble( x = seq(0, pi * 2, 0.1), sin_x = .5*x+sin(x), # expected value y = sin_x + rnorm(n = length(x), mean = 0, sd = sd(sin_x / 2))) # add noise head(dat) # print first 6 observations ## # A tibble: 6 x 3 ## x sin_x y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 -0.162 ## 2 0.1 0.150 0.0833 ## 3 0.2 0.299 0.749 ## 4 0.3 0.446 0.466 ## 5 0.4 0.589 0.627 ## 6 0.5 0.729 1.22 We can plot the true underlying signal (black line) and the observations (black dots) adding a linear regression line (blue) ontop. Clearly a linear model would fit poorly to these data. ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + geom_line(aes(y = sin_x)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Instead, we will use a GAM model. As you will see below the syntax is very similar to the glm or even lm function in base R. library(mgcv) # gam package mod &lt;- gam(y ~ s(x), data = dat, method = &quot;REML&quot;) The mgcv package uses the syntax y ~ s(x) to specify the model. This means that y is modelled as a smooth function of x. If nothing else is specified, the procedure will use a thin plate spline and determine the number of basis functions to use itself. You can also fix this by setting the \\(k\\) argument of the \\(s()\\) function. When problems with overfitting occur, you can tackle this by setting a lower value for \\(k\\) or setting the smoothing parameter. This argument is \\(sp\\) in the \\(s()\\) function. We will not go into details about this here, but in general it is recommended to use method = REML. This means the model is using restricted maxmimum likelihood for the estimation and use this algorithm to set of the smoothing parameter for you. Lets look at the model summary. summary(mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.56585 0.03236 48.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 6.367 7.517 37.88 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.821 Deviance explained = 83.9% ## -REML = 15.726 Scale est. = 0.065972 n = 63 Here you can see we have a significant intercept term and a approximate significance of the smooth term. The procedure uses an approximation here because the smooth term not really just one covariate. It is important to check that the estimated degree of freedom (edf) is not very close to the reference degree of freedom (Ref.df). This indicates that model has not been given enough flexibility and you could consider setting a higher \\(k\\) value for the smoother. Lets have a closer look at the smoothing terms being used in this model. In the code below we extract the model matrix and plot the basis function. To have a slightly deeper understanding of what the GAM model does is that it creates a weighted sum of these basis functions to, as optimally as possible, fit the underlying curve of the data. # Extract model matrix: MM &lt;- model.matrix(mod) MM &lt;- as_tibble(MM) MM$x &lt;- dat$x MM &lt;- pivot_longer(MM, -x) # Plot basis functions ggplot(MM, aes(x = x, y = value, group = name, col = name)) + geom_line() + theme_bw(12)+ theme(legend.position = &quot;none&quot;) + labs(y = &quot;Basis functions&quot;) If we multiply these basis functions with the weights for the fitted model, we get the additive terms that constitutes the full model prediction. # Extract coefficient from the model and merge model matrix: coefs &lt;- tibble(coef = coef(mod)) coefs$name &lt;- names(coef(mod)) MM &lt;- left_join(MM,coefs, by = &quot;name&quot;) # Plot model weigthed basis functions ggplot(MM, aes(x = x, y = value*coef, group = name, col = name)) + geom_line() + theme_bw(12)+ theme(legend.position = &quot;none&quot;) + labs(y = &quot;Model weigthed basis functions&quot;) Here you can see how the different curves contribute the full signal. Lets have a look at the final prediction and plot that along with a 95% confidence interval, the observations and the true signal. pred.mod &lt;- predict(mod, se.fit = TRUE) # predict values with standard error dat &lt;- dat %&gt;% mutate(pred = pred.mod$fit, lwr = pred - 1.96 * pred.mod$se.fit, upr = pred + 1.96 * pred.mod$se.fit) ggplot(data = dat, aes(x = x)) + geom_point(aes(y=y)) + geom_line(aes(y = pred), col = &quot;magenta&quot;)+ geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .3, fill = &quot;magenta&quot;) + theme_bw(12) + labs( x = &quot;x&quot;, y = &quot;y&quot; ) Normally, we would do the train-test split, but in this example we know the truth and the point of the exercise is to reproduce the underlying signal. As a last element of this example, we will see how well the model extrapolates as we try to predict the signal for values the model has not seen. For the training we used values of \\(x\\) between 0 and \\(2\\pi\\). Lets see how it performs for values of x between \\(2\\pi\\) and \\(3\\pi\\). # Setting up new data: dat.ext &lt;- tibble( x = seq(2*pi, 3*pi, 0.1), sin_x = 0.5*x+sin(x) # underlying signal ) # Predicting pred.ext &lt;- predict(mod, newdata = dat.ext, se.fit = TRUE, type =) # adding predictions to data frame: dat.ext &lt;- dat.ext %&gt;% mutate(pred = pred.ext$fit, lwr = pred - 1.96 * pred.ext$se.fit, upr = pred + 1.96 * pred.ext$se.fit) # Plotting results: ggplot(dat.ext, aes(x = x))+ geom_line(aes(y = pred), col = &quot;magenta&quot;)+ geom_ribbon(aes(ymin = lwr, ymax = upr), fill = &quot;magenta&quot;, alpha = 0.3)+ geom_line(aes(y = sin_x), col = &quot;red&quot;)+ theme_bw(12) First of all, note the width of the prediction intervals. As the model has not been trained on these values of \\(x\\), it is expected that these are quite wide. It is comforting that the underlying signal (in red) is inside the intervals, but as you can see the shape of the curve is not very near the true signal. Hence, such a prediction will not be very informative. Finally, lets make a plot, combining the prediction with the training data. Note that we add the y=NA since this column is not included in the dat.ext data frame. We also add the type column to separate the two data sources and split the coloring based on this column. dat.combined &lt;- rbind( dat %&gt;% mutate(type = &quot;training&quot;), # adding colum type to split dat.ext %&gt;% mutate(y=NA, type = &quot;extrapolation&quot;)# the two data sources by ) ggplot(dat.combined, aes(x=x, col = type, fill = type))+ geom_point(aes(y=y), color = &quot;black&quot;) + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .4)+ geom_line(aes(y=pred))+ geom_line(aes(y=sin_x), col = &quot;yellow&quot;, lwd = 1.2, lty =2)+ theme_bw(12) ## Warning: Removed 32 rows containing missing values (geom_point). The warning about the 32 missing values are because we added y=NA to the dat.ext data frame. Data camp There is a very informative and useful GAM module on data camp that we can highly recommend called Nonlinear modeling in R with GAMs - especially chapter 1 and 2. References "],["lecture4.html", " 4 Classification methods 4.1 k-nearest neighbor 4.2 Logistic regression 4.3 Naive bayes 4.4 Wrap-up", " 4 Classification methods Sometimes we are not interested in predicting a continuous output, but rather a factor or a class. We use classes or categories for many things and often we want a model to make a prediction into discrete categories. Is this an image of a cat or a dog? Given information of each passenger, is it likely that a certain individual would survive the shipwrecking of Titanic? Based on the score of certain hand-ins in a predictive modelling course, what final grade (A-F) is the student likely to get on the final exam? In this chapter we will learn about the classification methods k-nearest neighbor (knn), naive bayes and logistic regression. We will briefly consider all three methods here, and refer to the data camp course for further details (see data camp section below). We will also use the same case as an example for all three methods, and compare them all together at the end. In the video at the end, we walk through all the example code. 4.1 k-nearest neighbor The method k-nearest neighbor (knn) is a very simple classificiation method, where for predicting a class of observations in the test set one find the \\(k\\) observations in the training set that has covariates nearest to the covariates of the test case in terms of Euclidean distance. There are many different implementations of knn, for instance in the class package (see ?class::knn). But since we will be using the package bundle tidymodels in the next lecture we will also use it here. Tidymodels is, similar to the tidyverse, a combination of many packages using a kind of pipe notation to build models. It is very well integrated with tidyverse and provides a general framework for many different models. We will use knn as an example of how we set up a model in the tidymodels setup. Example As an example for classification we will use the famous Iris dataset of Edgar Anderson (see ?iris). We start by loaded the packages and the data containing inforation of sepal- and petal- lengths and widths of three different iris flower species (Iris setosa, Iris versicolor and Iris virginica). The goal will be to make a model that can tell us which Iris flower species we are dealing with, based on the given lengths and widths. library(tidyverse) library(tidymodels) df &lt;- as_tibble(iris) head(df) ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa We make our train-test split using the tidymodels::initial_split function to create the split and respecitvely the training and testing functions to extract the train and test sets. Note that we use strata = Specices. This means we are doing stratified subsampling so that we have the (roughly) the same proportion of the different species in the test and train sets. set.seed(999) split &lt;- initial_split(df, strata = Species) df_train &lt;- training(split) df_test &lt;- testing(split) To fit a model in tidymodels we must first specify which type of model (nearest neighbor) we want to create and set an engine and a mode. The engine is the package used to fit the model and the mode is usually classification or regression, depending on what type of y variable you are considering. knn_spec &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% # requires &quot;kknn&quot; package installed set_mode(&quot;classification&quot;) Then we are ready to fit the model using the fit function, where we specify a formula and which data to use. We will here use all covariates in the train set to predict Species. knn_fit &lt;- knn_spec %&gt;% fit(Species ~., data = df_train) knn_fit ## parsnip model object ## ## ## Call: ## kknn::train.kknn(formula = Species ~ ., data = data, ks = min_rows(5, data, 5)) ## ## Type of response variable: nominal ## Minimal misclassification: 0.05405405 ## Best kernel: optimal ## Best k: 5 As you can see from the output, the best number of neighbors to be used was 5. The knn model will thus look for the 5 flowers that has the nearest covariates and then predict based on the majority vote of the five. For example if three of them are Iris Virginica and two are Iris versicolor, the prediction will be Iris Virginica. Let us predict on the test set and chech the accuracy and \\(\\kappa\\) using the metrics function. knn_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) metrics(truth = Species, estimate = .pred_class) # calculate metrics ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 The accurcay metric is simply the number of correct classifcation divided by the total number of predictions made. We get (roughly) 95% correct, which seems very acceptable. The Cohens kappa metric is useful when the data is imbalanced, e.g. you have many more Iris versicolor than Iris setosa in the data, such that predicting all flowers as Iris versicolor would also give a high accuracy. We can also check the confusion matrix. This is a matrix with frequencies of the true classes as columns and the predicted classes as rows. If all cells except the diagonal is zero, it means the classifier has 100% correct classification. Otherwise, we can see where the mistakes happen. To make the confusion matrix, we only need to change the last line of the previous code snippet: knn_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) conf_mat(truth = Species, estimate = .pred_class) # calculate metrics ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 4.2 Logistic regression Logistic regression is a classical method for estimating the conditional probability of certain discrete outcomes given the value of covariates. In the Iris example, we can get estimates of the probability of the flower species being setosa, vercicolor or viriginca given the length and width of sepal and petal. As a first step, we will only assume we have two outcomes, either the flower is a setosa or it is something else. THe response variable is then 1 for setosa and 0 for other. For logistic regression the prediction is thus a value between 0 and 1, and this is achieved by mapping the linear predictors using the inverse logit link function, defined by \\[\\rm{logit}^{-1}(x) = \\frac{e^x}{1+e^x}, \\quad x\\in\\mathbb R.\\] We can also define the logit function directly, mapping probabilites \\(p\\in (0,1)\\) to \\(\\mathbb R\\), by \\[\\rm{logit}(p) = \\ln \\bigg(\\frac{p}{1-p}\\bigg).\\] The right hand side of this definition is often referred to as the log-odds and p would in our case be the probability of a certain flower being an iris setosa. Thus, if \\(x\\) is the vector of covariates for a certain flower and \\(\\beta\\) the parameter vector, we will model the probability of an Iris flower belonging to the species setosa given the set of lengths and widths, by \\[p=P(Y=1|x) = \\rm{logit}^{-1}(x^\\prime \\beta)=\\frac{\\exp(x^\\prime \\beta)}{1+\\exp(x^\\prime \\beta)}.\\] We can also express it in terms of the log-odds \\[\\log \\frac p{1-p} = x^\\prime \\beta = \\beta_0 + \\beta_1x_1 + \\cdots + x_p\\beta_p,\\] which you may note is linear in the parameters. Since the outcome here is binary, we can assume a Binomial distribution and estimate the parameters using maxmimum likelihood estimators. In a prediction setting, a logistic regression model will output a probabilty of flower belonging to class setosa, i.e. a number between zero and one. To translate this into a classification we need to set a threshold, e.g. 50%. Is it more than a 50% chance that the flower is setosa, we will predict that it is. We have tried to explain logistic regression for a binary classification setting, but it can also be used in multinomial settings. We will not go into details about this now, but move over to the practicle implementation in R. If you want to learn more about the theory of logistic regression, see Dobson and Barnett (2018). Example In the practicle example, we will use all three categories of iris flowers. We will use the same train and test sets as we used for knn. We will again, stick to the tidymodels set-up. There are simpler ways to set up a logistic regression using basic functions in R (see ?stats::glm). lr_spec &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% # requires &quot;kknn&quot; package installed set_mode(&quot;classification&quot;) lr_spec ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm Let us fit the model: lr_fit &lt;- lr_spec %&gt;% fit(Species ~., data = df_train) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred As you can see from the output, the algorithm did not converge and the fitted probabilities are at the extremeties 0 and 1, which is not good. We could try to centralize (subtract the mean value) and standardize (divide by the standard deviation) the covariates or to take away some of them to see if we can a converging algorithm. lr_fit &lt;- lr_spec %&gt;% fit(Species ~ Sepal.Length, data = df_train) Using only Sepal.Length seems to converge at least. Let us look at the performance on the test set. lr_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.564 ## 2 kap multiclass 0.346 As expected the accuracy is poor. We can also look at the confusion matrix to see why: lr_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = Species, estimate = .pred_class) ## Truth ## Prediction setosa versicolor virginica ## setosa 12 3 1 ## versicolor 1 10 12 ## virginica 0 0 0 The logistic regression model we built cannot separate Iris Virginica from Iris Versicolor. 4.3 Naive bayes Naive Bayes is the third and last classification method we will consider in this lecture. It is a probabilistic classifier based on applying Bayes theorem with independence assumptions between the features (making it naive). They are a type of Bayesian network, but in combination with kernel density estimation, you will not notice the Bayesian framework they are usually wrapped in. In Bayesian methods, you have to specify priors for your parameters, but the R packages we will be using uses Kernel densities instead and you only need to tune them with smoothness parameters. In its essence, a naive bayes classifier finds the probability that a certain flower belongs to any of the classes and select the one with highest probability. We will derive the objective function using Bayes theorem below and use the methods on our Iris data with tidymodels and the discrim package. As the name describes, Naive bayes classifiers uses Bayes theorem to assign a conditional probability given the covariates \\(X_1, \\ldots, X_p\\) that the instance in question belongs to category \\(C_k\\), i.e. \\[P(C_k|X_1,\\ldots, X_p),\\quad k = 1,\\ldots, K,\\] for each of K outcomes or classes \\(C_k\\). According to Bayes theorem, we can write this probability as \\[P(C_k|X_1\\ldots, X_p) = \\frac{P(C_k)P(X_1,\\ldots, X_p|C_k)}{P(X_1,\\ldots, X_p)}.\\] As is often the case for Bayesian methods, the denominator is not of interest here, since it does not depend on \\(C_k\\) - it is only a normalizing constant. Now, the naive property of naive bayes is that we assume all covariates to be conditionally independent of each other given \\(C_k\\). Meaning, \\(P(X_i|X_1,\\ldots, X_{i-1}, X_{i+1}, \\ldots, X_p, C_k) = P(X_i|C_k)\\) for all i and any \\(k\\). The numerator above is the joint probability of \\(C_k\\) and \\(X_1,\\ldots, X_p\\). We can write it as \\[\\begin{align} P(C_k)P(X_1,\\ldots, X_p) &amp;= P(X_1, \\ldots, X_p, C_k) \\\\ &amp;= P(X_1|X_2, \\ldots, X_p, C_k)P(X_2, \\ldots, X_p, C_k)\\\\ &amp;= P(X_1|X_2, \\ldots, X_p, C_k)P(X_2|X_3, \\ldots, X_p, C_k)P(X_3, \\ldots, X_p, C_k)\\\\ &amp;=\\ldots\\\\ &amp;= P(X_1|X_2, \\ldots, X_p, C_k)P(X_2|X_3, \\ldots, X_p, C_k)\\cdots \\\\&amp;\\hspace{50pt}\\cdots P(X_{p-1}|X_p, C_k) P(X_p| C_k)P(C_k) \\end{align}\\] Using the Naive conditional independence property, we get \\[\\begin{align} P(C_k)P(X_1,\\ldots, X_p) &amp;= P(C_k)\\prod_{i=1}^p P(X_i|C_k) \\end{align}\\] Thus, we can write that the conditional probability of \\(C_k\\) given the covariates (commonly known as the posterior distribution in Bayesian theory) as \\[P(C_k|X_1,\\ldots, X_p) \\propto P(C_k)\\prod_{i=1}^p P(X_i|C_k),\\] where \\(\\propto\\) mean proportional to, because we have neglected the denominator. We can then construct a classifier based on this posterior distribution, selecting the category or class \\(k\\) that has the highest posterior probability. Formallly, we can define the classifier \\(\\widehat y\\) as \\[\\widehat y = \\mathrm{argmax}_k\\, P(C_k)\\prod_{i=1}^p P(X_i|C_k)\\] There are different ways of selecting priors, i.e. the probability functions \\(P(C_k)\\) and \\(P(X_i|C_k)\\). For the \\(P(C_k)\\) it is common to either use the proportion of each class in the training set or set all probabilities equal \\(1/K\\). For the feature distributions one can select Mutinomial or Bernoulli distributions for discrete covariates or Gaussian for continous, for instance. The package we will be using is using Kernel density estimation for the priors, which is a non-parametric procedure. Example Turning back to the flowers one last time, we will be using the package discrim with tidymodels. For hyper parameters there is a smoothness term one can set in the set_engine that determines how smooth the kernel should be. We will only use standard values here and see how it goes. The procedure is more or less the same as for the other two approaches we have considered - as you will see in the code below. # Load naive bayes pacakge: library(discrim) # Set up model specification: nb_spec &lt;- naive_Bayes() %&gt;% set_engine(&quot;naivebayes&quot;) %&gt;% set_mode(&quot;classification&quot;) # Fit the model: nb_fit &lt;- nb_spec %&gt;% fit(Species ~ ., data = df_train) # Evaluate performance on the test set: nb_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 # Confusion matrix: nb_fit %&gt;% predict(new_data = df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = Species, estimate = .pred_class) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 4.4 Wrap-up We have now fitted three different classfiers to the iris data set. In this setting and without doing any in-depth fine tuning of the models, we found that the logistic regression did not perform very well, while k-nearest neighbour and naive bayes seemed to do a good job of splitting the flowers into the correct categories. In fact, on this particular example and without model tuning, KNN and naive bayes performed exacly equal. Lets do a quick check that they are 100% in agreement by making a confusion matrix between the predictions from KNN and naive bayes. # KNN prediction on test set knn_pred &lt;- knn_fit %&gt;% predict(new_data = df_test) %&gt;% rename(&quot;knn&quot;=&quot;.pred_class&quot;) # Renaming column for predictions # Logistic regression prediction on test set lr_pred &lt;- lr_fit %&gt;% predict(new_data = df_test) %&gt;% rename(&quot;lr&quot;=&quot;.pred_class&quot;) # Naive Bayes prediction on test set nb_pred &lt;- nb_fit %&gt;% predict(new_data = df_test) %&gt;% rename(&quot;nb&quot;=&quot;.pred_class&quot;) predictions &lt;- bind_cols(knn_pred, nb_pred, lr_pred) conf_mat(data = predictions, truth = knn, estimate = nb) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 15 0 ## virginica 0 0 11 Let us also compare one of them to the logistic regression predictions conf_mat(data = predictions, truth = knn, estimate = lr) ## Truth ## Prediction setosa versicolor virginica ## setosa 12 4 0 ## versicolor 1 11 11 ## virginica 0 0 0 For this partitular problem, logsitic regression was not the method to choose, but remember that it may be different in other settings. Data camp We highly recommend the data camp course Supervised Learning in R: Classification - chapters 1-3. The subject of chapter 4 is covered separately in the next lecture 5. 4.4.1 Sources rpubs References "],["lecture5.html", " 5 Decision Trees and Bagged Trees 5.1 Decision trees 5.2 Bagged trees", " 5 Decision Trees and Bagged Trees In this lecture we will learn about decision trees and bagged trees for classification or regression models. As in the previous lecture, we will use the tidymodels framework (Kuhn and Wickham 2020). If you have not installed tidymodels, this can be done by install.packages(&quot;tidymodels&quot;) We start with decision trees. 5.1 Decision trees A decision tree is a classifier that can be represented as a flow chart that looks like a tree. In each prediction, we start at the root asking a question deciding to which branch we should go for the prediction. As an example, say we want to predict whether or not a person likes computer games and make a decision tree for solving that problem. We have illustrated such a tree below. First you must ask: Is the person younger than 15 years old? If yes, predict that the person likes computer games. If the answer is no, is the person a boy? If yes, predict YES, he likes computer games, or if no, then NO, she does not like computer games. (#fig:decisiontree_computergames)A simple decision tree In the illustration we have tried to make the decision tree grow from the ground-up, but it is most common to print them the other way around. The pros of decision trees are: Easy to explain and intuitive to understand Possible to capture non-linear relationships Require no standardization or normalization of numeric features No need for dummy variables for categoric features Robust to outliers Fast for large datasets Can be used for regression and classification The cons are: Hard to interpret if large, deep, or ensembled High variance, complex trees are prone to overfitting To set up a tidymodels decision tree, we start by loading the package library(tidymodels) To make a decision tree, there are three elements we need. First a decision tree object, then the engine and finally the mode. tree_spec &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) Here decision_tree() makes the decision tree object. There are different packages we can use for decision trees, but we select a package called rpart, which is a package for recursive partitioning. This is called the engine in the tidymodels framework. Finally, we must decide if we are to do classification or regression, and this is the mode. Here we use classification. The tree_spec object is just a skeleton, we need data to make it useful. For example purposes, let use the same data as in lecture 4 (the iris data). library(tidyverse) df &lt;- as_tibble(iris) set.seed(999) split &lt;- initial_split(df, strata = Species) df_train &lt;- training(split) df_test &lt;- testing(split) Then we are ready to fit a decition tree to our training data. You will notice the procedure is very similar to the procedure we used in lecture 4. This is the largest benefit of using tidymodels. tree_fit &lt;- tree_spec %&gt;% fit(Species ~., data = df_train) tree_fit ## parsnip model object ## ## n= 111 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 111 74 setosa (0.33333333 0.33333333 0.33333333) ## 2) Petal.Length&lt; 2.6 37 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.6 74 37 versicolor (0.00000000 0.50000000 0.50000000) ## 6) Petal.Width&lt; 1.75 39 3 versicolor (0.00000000 0.92307692 0.07692308) * ## 7) Petal.Width&gt;=1.75 35 1 virginica (0.00000000 0.02857143 0.97142857) * Using the rpart.plot package, we can visualize the decision tree we have fitted by library(rpart.plot) rpart.plot(tree_fit$fit,roundint=FALSE) So, to make a prediction, the decision tree asks first: Is the petal length below 2.6? If yes, predict Setosa. If no: Is the petal width below 1.75 (rounded off to 1.8 in the illustration)? If the answer is yes, it is a vercicolor and a virginica if no. We can also make predictions on the test set and calculate the accuracy tree_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) metrics(truth = Species, estimate = .pred_class) # calculate metrics ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 and the confusion matrix tree_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) conf_mat(truth = Species, estimate = .pred_class) # confusion matrix ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 If you remember from the last lecture, this is the same result as we got using knn and naive bayes. 5.2 Bagged trees Bagged threes is an ensemble method. This means, instead of fitting just one decision tree, we fit many, and aggregate the predictions from all of them to improve our final prediction. Bagging is short for bootstrap aggregation. Bootstrap is a method for resampling that we use on the training set to get many training sets with the same properties as the original one. We sample random rows of the training set with replacement to make these bootstrapped datasets and fit a decision tree to each of these. (#fig:bagged_ensemble)A bagged tree This will give us many predictions that we then aggregate to arrive at our bagged prediction. If we are in a regression setting, the aggregated prediction is simply the mean, while in a classification setting the majority vote becomes the final prediction. Bagging can reduce the variance of our prediction significantly. (#fig:bagged_ensemble_pred)A bagged tree prediction Let us fit a bagged tree to the iris data. Here we need to install an additional package called baguette. install.packages(&quot;baguette&quot;) This package contains the function bag_tree that we will use in our model spec. library(baguette) bag_spec &lt;- bag_tree() %&gt;% set_engine(&quot;rpart&quot;, times = 100) %&gt;% set_mode(&quot;classification&quot;) As you have learned by now, the structure is the same, but notice in the engine specification we have set times = 100. This is how we specify how many trees we want to include in our bag. set.seed(123) bag_fit &lt;- bag_spec %&gt;% fit(Species ~., data = df_train) bag_fit ## parsnip model object ## ## Bagged CART (classification with 100 members) ## ## Variable importance scores include: ## ## # A tibble: 4 x 4 ## term value std.error used ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Petal.Length 66.8 0.320 100 ## 2 Petal.Width 66.7 0.277 100 ## 3 Sepal.Length 48.2 0.489 100 ## 4 Sepal.Width 27.9 0.487 100 In the output, we see the variable importance scores. As you can see the Petal.Length is the most important covariate. We can also make predictions on the test set and calculate the accuracy bag_fit %&gt;% predict(df_test) %&gt;% bind_cols(df_test) %&gt;% metrics(truth = Species, estimate = .pred_class) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 and the confusion matrix bag_fit %&gt;% predict(df_test) %&gt;% bind_cols(df_test) %&gt;% conf_mat(truth = Species, estimate = .pred_class) ## Truth ## Prediction setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 2 ## virginica 0 0 11 It seems this is the best we can do with the data that we have - same result as for knn, naive bayes and a simple decision tree. On data camp you will learn more about decision trees and bagged trees - also how to use them in a regression setting. Data camp We highly recommend the data camp course Machine Learning with Tree-Based Models in R chapters 1-3. References "],["lecture6.html", " 6 Random forrest and boosting", " 6 Random forrest and boosting "],["lecture7.html", " 7 Artificial Neural networks 7.1 MLP 7.2 Objective functions and training 7.3 Validation of trained models 7.4 Implementation of MLP in Keras", " 7 Artificial Neural networks This section describes the basics theory and classical optimization of an artificial neural network. This includes defining a 2-layer feed forward neural network, cost and objective functions, regularization and the manner in which a network can be trained to produce desired output. The section will be limited to so-called supervised learning, i.e. where know the true state of an output. The main references for this chapter are the textbook Deep Learning by @{goodfellow2016deep}. 7.1 MLP Let \\(\\mathbf{X}^{(i)}\\) be an instance of data which is input to a deep learning model, with associated target values \\(\\mathbf{y}^{(i)}\\). A target value can be a class or as we will use later the data \\(\\mathbf{X}^{(i)}\\) itself. All the instances of \\(\\mathbf{X}^{(i)},\\) \\(i=1, \\ldots N,\\) constitute the data set \\(\\mathbf{X}^{(i)}\\), and all the target values \\(\\mathbf{y}^{(i)},\\) \\(i=1, \\ldots N,\\) comprise the data set \\(\\mathbf{Y}.\\) \\ A feed forward neural network is a hierarchical model that consists of nodes or computational units divided into subsequent layers. For each node, a non-linear activation function is applied. The nodes between each layer are connected, so that the input to a node is totally dependent on the output from the nodes of the previous layer. The model is called a if there are multiple hidden layers; see . The simplest deep learning model has at least one hidden layer: an input layer and an output layer. This hierarchical structure makes it possible to formulate the deep learning model as a linear system of equations. \\ Illustration of a deep neural network with three hidden layers The model input to a neural network is here defined as vector \\(\\mathbf{x}^{(i)}\\) with \\(Q\\) elements. The input is transformed linearly by \\(\\mathbf{W}_1\\) and \\(\\mathbf{b}\\) such that \\(f(\\mathbf{x}^{(i)}) = \\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b}\\). \\(\\mathbf{W}_1\\) transforms the input to a vector of \\(K\\) elements and is often called the weight matrix, while the translation \\(\\mathbf{b}\\) is referred to as the bias. The bias can be interpreted as a threshold for when the neuron activates. \\ A nonlinear activation function is applied element wise to the transformed data. The activation function is typically given as a rectified linear unit (ReLU) @{nair2010rectified} (legg inn Relu equation!!) or \\(\\tanh\\) function. This activation introduces non-linearity to the other linear operations. The superposition of the linear and nonlinear transformation is in combination with the activation function and is what we refer to as a hidden layer. \\ Applying another linear transformation \\(\\mathbf{W}_2\\) to the hidden layer results in this case to the model output or output layer. The size of the output layer is a row vector with \\(D\\) elements. Generally, many transformations and activations can be applied consecutively which will result in a more complex hierarchical model. A generalization to a network with several hidden layers is straightforward; to make this clear we here limit the notation to a single hidden layer. We note that \\(\\mathbf{x}^{(i)}\\) is vector of size \\(Q\\), \\(\\mathbf{W}_1\\) is a \\(K \\times Q\\) matrix that transforms the input to \\(K\\) elements, \\(\\mathbf{W}_2\\) is a \\(D \\times K\\) matrix, transforming the vector into \\(D\\) elements and \\(\\mathbf{b}\\) consists of \\(K\\) elements. We write this as linear system of equations transformed with the activation function \\(\\sigma\\) \\[\\widehat{\\mathbf{y}}^{(i)} = \\mathbf{W}_2 (\\sigma(\\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b})) := \\mathbf{f}^{\\omega}, \\qquad \\omega = \\{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}\\}\\] Depending on how the output layer is defined, we can use the network for classification or regression tasks. For classification purposes, the number of nodes in the output layer equals the number of classes, and typically transformed with a softmax function @{goodfellow2016@softmax}. The softmax function is a generalization of the logistic map that normalizes the output relative to the different classes (Legg inn softmax equation !!). In regression problems we want to estimate relations between variables; we want to predict a continuous output based on some input (variables). To use a linear activation function on the output layer will serve this purpose. It has been shown that ANN is a universal approximation @{hornik1989multilayer}; thus, our goal is to find the weights of the given network to best approximate the map from the input to the output. This means that we want to estimate the weights of the ANN \\(\\mathbf{\\omega}\\), given the input data \\(\\mathbf{x}^{(i)}\\), the target \\(\\mathbf{y}^{(i)}\\) such that the predictions \\(\\widehat{\\mathbf{y}}^{(i)}\\) is minimized towards the true target values \\(\\mathbf{y}^{(i)}\\). This is a typical optimization problem, which can be minimized with an objective function and optimization procedure. 7.2 Objective functions and training An objective function for use in deep learning typically contains two terms: cost function and regularization. The cost function takes the predicted and the true values as input. Depending on the task and what one wants to minimize, the cost function maximizes a likelihood. In classification problems this is can be the negative cross entropy \\[\\mathcal{C}_1^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) = - \\frac{1}{N}\\sum\\limits_{j=1}^{N}\\mathbf{y}^{(i)}_j\\log(\\widehat{\\mathbf{y}}^{(i)}_j) = -\\log p(\\mathbf{Y}|\\mathbf{f}^{\\mathbf{\\omega}}(\\mathbf{X})),\\] and in regression problems the Mean Squared Error (MSE) \\[\\mathcal{C}_2^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(\\mathbf{y}^{(i)} - \\widehat{\\mathbf{y}}^{(i)})^2 = - \\log p(\\mathbf{Y}|\\mathbf{f}^\\mathbf{\\omega}(\\mathbf{X})),\\] Minimization of the negative cross entropy and the MSE is well known to be equivalent to minimize the negative log likelihood of the parameter estimation @{tishby1989consistent} for neural networks. Depending on the task, minimizing or with respect to the parameters \\(\\omega = \\{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}\\}\\) maximizes the likelihood of these parameters. The choice of the cost function is not restricted to those given above, and depend on the data, the model structure and what one wants to predict with the model.\\ One of the key problems in deep learning is a phenomenon called over-fitting (some blue Ref to Section ). Over-fitting occurs if the optimized model performs poorly on new unseen data, i.e. it does not generalize well. To address this problem, regularization is added to the cost function. \\ Regularization is a general technique, where the goal is to make an ill posed problem well-posed (Ref. her kanskje?). Over-fitting is basically one example of an ill-posed problem. For optimization problems, you could add a penalizing functional: L2 or L1 norm for the parameters; or use dropout (Ref. XX). \\ Regularization in neural networks work by penalizing the cost function, e.g. forcing the weights to become small. The idea behind a specific regularization term could be to minimize the weights of the ANN to generate a simpler model that helps against over-fitting. \\(L2\\) regularization multiplied with some penalizing factor \\(\\lambda_i\\) is one of the most common regularization techniques. The cost function with regularization is called the objective function. Adding \\(L2\\) regularization to equation or result in the objective function \\[\\mathcal{L}(\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}) = \\mathcal{C}^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) + \\lambda_1||\\mathbf{W}_1||^2 + \\lambda_2||\\mathbf{W}_2||^2 + \\lambda_3||\\mathbf{b}||^2\\] Another common way of regularizing the cost function is through dropout, which is a stochastic regularization technique. In the example later we will utelize dropout as regularization. \\ Minimizing the objective in with respect to the weights \\(\\mathbf{\\omega}\\) with an objective function and a gradient descent optimization method has proven to give good results in a wide range of applications. \\ The gradient descent method @{curry1944method} updates the parameter \\(\\mathbf{\\omega}\\) using the entire data set \\[\\mathbf{\\omega}_t = \\mathbf{\\omega}_{t-1} - \\eta \\nabla \\mathcal{L}(\\mathbf{\\omega}_{t-1}). \\] Here \\(\\omega_t\\) represents the current configuration of the weights, while \\(\\omega_{t-1}\\) represents the previous one. The parameter \\(\\eta\\) is referred to as the learning rate, i.e. how large the step in the negative gradient direction the update of the weights should be. Too small steps can lead to poor convergence, while to large steps can lead to overshooting, i.e. missing local/global minimums. Usually it is too expensive to calculate the gradient over the entire dataset. This is solved by a technique called stochastic gradient descent @{robbins1951stochastic}. Stochastic gradient descent performs a parameter update for each training example. A natural extension and a more cost-efficient approach is the mini-batch gradient descent approach. In mini-batch optimization, the gradient is approximated by calculating the mean of the gradients on sub-sets or batches of the entire data set, \\[\\mathbf{\\omega}_t = \\mathbf{\\omega}_{t-1} - \\frac{\\eta}{n}\\sum\\limits_{i=1}^{n}\\nabla \\mathcal{L}_i(\\mathbf{\\omega}_{t-1}).\\] The mini-batch gradient descent iterative process can be implemented in the neural network with the back-propagation algorithm @{rumelhart1988learning}. In back-propagation, the weights are updated through a forward and backward pass. In the forward pass, we predict with the current weight configuration and compare towards the target values. In the backward pass, we use the chain rule successively from the output to the input to calculate the gradient of \\(\\omega\\). Based on the gradient direction and the learning rate, the configuration of the weights is updated. To find the correct learning rate is difficult, hence, several methods has been developed to adjust the learning rate adaptively. One of the disadvantages of the vanilla gradient descent approach to the ANN optimization problem, is that it has a fixed leaning rate \\(\\eta.\\) In line with the development of ANN, methods dedicated to deep learning and adaptive adjustment of the learning rate have been developed. Besides SGD with momentum @{sutskever2013importance}, the two most used optimization methods for ANNs are ADAM @{kingma2014adam} and Root Mean Square Propagation (RMSProp) @{tieleman2012lecture}. RMSProp adaptively adjusts the learning rate of the gradients based on a running average for each of the individual parameters. The ADAM-algorithm individually adjusts the weights in terms of both the running average, but also with respect to the running variance. \\ The use of back-propagation together with a stochastic gradient descent method, increase in available data and hardware have been the successes of deep learning during the past decade. 7.3 Validation of trained models To validate and ensure that the predictions of the deep learning model also performs well on new unseen instances, the data is split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. \\ After each epoch, we predict with this temporally model on the validation data set. Usually we put criteria on the performance on the validation data set for when to stop the optimization. We can use a so called early stopping regime, where the model stops training if it does not see improvement on the validation score after a certain number of epochs without improvement. \\ The purpose of the test data set is to validate on new unseen data that has not been part of the training or the continuous validation of the model. Lets look at an example of how to implement a classifier on the mnist data in keras   7.4 Implementation of MLP in Keras In this section we will show you how to implement a MLP with dropout as regularization with the low level API for tensorflow called Keras. A lot of the development within deep learning is concentrated around the python programming language. Keras is also a tool that is originally developed in keras, but also available in R. Lets start by installing tensorflow: 7.4.1 Installing Keras and Tensorflow First install tensorflow with the following commands install.packages(&#39;tensorflow&#39;) library(&#39;tensorflow&#39;) install_tensorflow() Then install keras with the following commands install.package(&#39;keras&#39;) library(keras) install_keras() 7.4.2 Importing MNIST We will use the famous MNIST data set as an example in this section (Ref. XX). The MNIST dataset contains 60000 handwritten digits for training the model, and 10000 for testing. The digits has a shape of 28x28 pixels in grey scale, that is, they only have one channel, describing the black/white intensity of the pixel value. Each picture/instance is associated with a label, that is a number from 0-9. The keras package contains a function for downloading the MNIST dataset from the source (Ref. XX) library(keras) mnist &lt;- dataset_mnist() To ensure that the predictions of the neural network model also performs well on new unseen instances, the data is often split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize/train the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. We will come back to validation and testing of the neural network later in this section. The MNIST dataset are split into a test and train data set from source # x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y We can check out some of the training data #checking the dimension of the train/test data library(ggplot2) # visualize the digits par(mfcol=c(5,5)) par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) for (idx in 1:25) { im &lt;- x_train[idx,,] im &lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=&#39;n&#39;, main=paste(y_train[idx])) } #checking the dimension of the train/test data dim(y_train) head(y_train) Here we see that the training dataset has a shape of \\(60000 \\times 28 \\times28\\). This means that there are \\(60000\\) instances, or pictures of handwritten digits, where each of the pictures has \\(28 \\times 28\\) dimension in the horizontal and vertical direction. The dimension of the y_train variable represents the class or label of the x_train variable. 7.4.3 Preprocessing Pre-processing is the step where we transform, scale, removes, imputes etc. the data before we feed it to the deep learning model. Removing erroneous data, or impute NA values may be of crucial importance to get the algorithm run at all. Transforming or scaling the data helps improve the so called conditioning problem. Conditioning of a problem say something about the sensitivity of the solution to changes in the problem data. To create an easier problem to optimize, we pre-process the MNIST-data before we are feeding it to the neural network model. Here we reshape and scale the data. # reshape x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784)) x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784)) # rescale x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 We also need to convert the y_train and y_test to categorical variables. Instead of having a vector with a number from 0-9, we convert it to a binary representation. That is, each label is represented by a vector of length 10 with one element is 1 and the rest 0. The element that is 1 represent the label of the digit. y_train &lt;- to_categorical(y_train, num_classes = NULL) y_test &lt;- to_categorical(y_test, num_classes = NULL) head(y_train) 7.4.4 MLP model In this example we want to create a so called dense neural network, or MLP. That is, each node are influenced from all of the nodes in the previous layer. This is contrast to e.g. convolutional neural networks that uses convolutions to reduce the connectivity between layers, thus reducing the connections. We construct a dense neural network. After reshaping the input, the network has a shape of \\(28 \\times 28 = 784\\) per instance. This is our input shape. This shape has to be specified in the model, as shown below. The first layer has \\(256\\) nodes, and we use the rectified linear unit as activations function. The next layer is a so called dropout layer. This layer drops randomly a proportion of the nodes out during a forward and backward pass. This will help avoiding overfitting, i.e. it is a form of regularization of the network. The next hidden layer has \\(128\\) nodes, and also ReLU activation function. The last layer of the network is the output. The output in this case, is to classify the label of the digit. There are 10 possible classes, 0-9 and we thus use a softmax activation function, with 10 units. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(784)) %&gt;% layer_dropout(rate = 0.4) %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;) First we construct a sequential model object. Then we can use the pipe syntax to specify the layers of the neural network. We can use the summary function to show a nice overview of the neural network model. summary(model) 7.4.5 Compiling and training the model We now have a model, the next step is to compile it. Durring compiling we also have to specify the loss function, optimizer and metrics for evaluations and validation. model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39; ) Since we want to classify the digits, we have chosen the loss to be categorical cross entropy. We use the adam-algorithm during optimization and we also specify that we want to return the accuaracy of the classification during training and validation. After the compilation, we can call the fit() function to train our model. The fit function take x_train and y_train as inputs. We also have to specify how many epoch the model should be trained on (how many times we will see the entire data set), the batch size (how many/large the splits of the training data set) and how large proportion of the data should be used as online validation data. Test data are set aside for evaluation the model when it has finished training. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2 ) It is possible to specify specific validation data if that is desirable. Batch size, epochs, size of the neural network, e.g. number of layers nodes, type of network (e.g. cnn, rnns) are so called hyper parameters. This is parameters that are not optimized in the network itself, but has to be specified manually. Theses hyper parameters are important and there are different strategies for optimize them. Hyper parameter search could be done with traditional grid search, but also more sophisticated approaches such as Bayesian hyper optimization (Ref. XX). We have plotted the loss and accuaracy calculated after each training epoch. It can be observed that the loss after approx 10-15 epochs, is significantly lower than the validation loss. This is an indication that the model starts to overfit. 7.4.6 Evaluating the model To evaluate the trained model on the test data, we can use the pipe syntax together with the evaluate function model %&gt;% evaluate(x_test, y_test) The trained model performes pretty well on the test data set. It has an accuaracy of approximatley 0.98, that is 98 \\(\\%\\) of the data in the test data set is classified correctly. Lets ### Predicting We can also predict the most probable class for each of the instances. For this we can use the predict function. (Litt mer her) #model %&gt;% predict(x_test) %&gt;% k_argmax() prediction &lt;- model %&gt;% predict(x_test) #plot(prediction[1710,], ) dim(prediction) #Lets find one of the most uncertain predictions to view how it looks like #We find the most probable predictions in the test data set largest &lt;- apply(prediction,1,max,na.rm=TRUE) #Then we choose the predictions which has the lowest confident less_confident &lt;- sort(largest,index.return = TRUE)$ix[1:36] #We find out what the model predicts these less_confident predictions are pred_class &lt;- prediction %&gt;% k_argmax() #and plot the digits par(mfcol=c(6,6)) par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) for (idx in less_confident) { im &lt;- mnist$test$x[idx,,] im &lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=&#39;n&#39;, main=paste(mnist$test$y[idx])) } "],["lecture8.html", " 8 Support vector machines", " 8 Support vector machines We have finished a nice book. "],["lecture9.html", " 9 Feature selection/Explainable AI", " 9 Feature selection/Explainable AI "],["references.html", "References", " References "]]
