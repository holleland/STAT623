[["index.html", "Data science with R: Applied Predictive Modelling Compendium for STAT623 Course overview Learning outcomes Lecture overview Litterature", " Data science with R: Applied Predictive Modelling Compendium for STAT623 Sondre Hølleland 2022-03-18 Course overview Learning outcomes After completing this course, students should Be able to make their own models for different situations using methods such as: linear regression, multiple regression, generalized additive models, splines, support vector machines, decision trees, etc. Become native speakers of the R language. . . Lecture overview Lecture Subject Exercises Datacamp 1 Introduction and short recap of R and Data preprocessing Recap of R Introduction to Regression in R (ch 1-2) 2 Over-fitting and model tuning, selection and evaluation and multiple regression Multiple regression Supervised Learning in R: Regression (ch 1-2) 3 Non-linear regression GAMs Nonlinear modeling in R with GAMs (ch 1-2) 4 Classification methods Supervised Learning in R: Classification (ch 1-3) 5 Trees xgboost Machine Learning with Tree-Based Models in R (ch 1-4) 6 Support vector machines Support Vector Machines in R (ch 1-2) 7 Neural Networks Introduction to TensorFlow in R (ch 1-3) 8 Unsupervised learning Unsupervised Learning in R (ch 1-4) 9 Feature selection/Explainable AI Supervised Learning in R: Classification (ch 3, video on automatic feature selection Litterature We will use many different sources for teaching you applied predictive modelling. Kuhn and Johnson (2016) and James et al. (2013) are the main references. References "],["lecture1.html", " 1 Introduction and short recap of R 1.1 Introduction 1.2 Recap of R 1.3 Data preprocessing 1.4 Case study", " 1 Introduction and short recap of R Goals: In this Lecture 1 we will give a introduction to the course and a short recap of using R. 1.1 Introduction This course is called applied predictive modelling. What do we actually mean by these three words? Well, applied is as opposed to theoretical - indicating that this will not be a very theoretical course. We will focus on usage of the different methods that you will learn throughout the course. Predictive modelling means that the overall goal of the modelling is to predict something or make a prediction. Gkisser (1993) defines predictive modelling as the process by which a model is created or chosen to try to best predict the probability of an outcome, while Kuhn and Johnson (2016) define it as the process of developing a mathematical tool or model that generates an accurate prediction and the give the following examples of types of questions one could be interested in predicting: How many copies will a book sell? Will this customer move their buisness to a different company? How much will my house sell for in the current market? Does a patient have a specific disease? Based on past choices, which movies will interest this viewer? Should I sell this stock? Which people should we match in our online dating service? Is an email spam? Will this patient respond to this therapy? Examples of stakeholders or users of predictive modelling can be insurance companies. They need to quantify individual risk of potential policy holders. If the risk is too high, they may not offer the potential customer insurance or they may use the quantified risk to set the insurance premium. Governments may use predictive models to detect fraud or identify terror suspects. When you go to a grocery store and sign up for some discounts, usually either by creating an account or registering your credit card, your purchase information is being collected and analyzed in an attempt to who you are and what you want. Predictive modelling can often a good thing. When for instance Netflix learns what kind of TV-shows you prefer to watch, they can come up with suggestions for other TV-shows that you are likely to also enjoy. This can often be recognized by statements like people who liked TV-series A, also liked TV-series B. For the streaming provider the goal is to keep you as an entertained, satisfied and paying customer, but it is also in your interest to find entertainment that you like. But you have probably also noticed that sometimes, these predictions are quite inaccurate and provide the wrong answer. For instance, you did not like the suggested series or you did not receive an important email because it was wrongly classified as spam by (predictive) spam filter. Kuhn and Johnson (2016) give four reasons for why predictive models fail: Inadequate pre-processing of the data. Inadequate model validation. Unjustified extrapolation. Over-fitting the model to existing data. They also mention the fact that modellers tend to explore realtively few models when searching for predictive realationships. This is usually because the modeler has a preference for a certain type of model (the one he/she has used before and know well). It can also be due to software availability. We will cover (at least) some of these aspects in this course. 1.1.1 Prediction or interpretation? In the examples listed above, the main goal is to predict something and there are likely data available to train a statistical model to do so in most of the cases. Note that we are not so interested in answering questions of why something happens or not. Our primary interest is to accurately predict the probability that something will, or will not, happen. In situations like these, we should not be worried with having interpretative models, and focus our attention on prediction accuracy. If your spam filter classifies an email as spam, you would not care why the filter did so, as long as you receive the emails you care about and not the ones you do not. An interpretative model can be a model for a certain stocks value that can support statements like the stock value prediction went up, because the company released information about a big contract. One can explain why the model behaves the way it does. The alternative is often called a black box, meaning that you put your data in on one side of the box and on the other the prediction pops out, without you knowing what happened inside the box. So, our primary interest is prediction accuracy. Cannot interpretability be our secondary target, so we can understand why it works? Kuhn and Johnson (2016) writes that the unfortunate reality is that as we push towards higher accuracy, models become more complex and their interpretability becomes more difficult. 1.1.2 Terminology In the predictive modelling terminology there are quite a lot of things that mean the same thing. We will list some terminology here with some explanations (see Kuhn and Johnson (2016, p6)): The terms sample, data point, observation, or instance refer to a single, independent unit of data, e.g. a customer, a patient, a transaction, an individual. Sample can also refer to a collection of data points (a subset). The training set consists of data used to develop the models while the test and validation sets are used solely for evaluating the performance of the final model. The predictors, idendependent variables, attributes, descriptors, or covariates are the data used as input for the prediction equation (e.g. what you feed into the black box). Outcome, dependent variable, target, class, or response refer to the outcome of the event or qunatity that is being predicted (e.g. what comes out of the black box). Continuous data have natural, numeric scales (e.g. blood pressure, price of an item, a persons body mass index, number of bathrooms, etc). Categorical, nominal, attribute or discrete data all mean the same thing. These are data types that take on specific values that have no scale (e.g. credit status (good or bad) or color (red, white, blue)) Model building, model training, and parameter estimation all refer to the process of using data to determine values of model equations. 1.1.3 Overview HERE WE WILL WRITE ABOUT ALL THE NICE THINGS YOU WILL LEARN IN THIS COURSE 1.2 Recap of R 1.2.1 Installing R and Rstudio If you have not already installed the newest version of R and Rstudio on your personal computer, we will now tell you how. This installation guide will be based on Windows, but iOS and Linux are very similar and we will indicate where to deviate. The video below shows the same steps as listed. First we install R: Go to https://cran.uib.no. Select Download R for Windows (or macOS/Linux) (this will be the newest R version). Go to your Downloads folder and run the .exe file. Use standard settings throughout the installation steps (click next-next-next-etc.). R is now installed. Now that R is installed, we can install Rstudio: Go to https://www.rstudio.com/products/rstudio/download/. Press Download under the Rstudio Desktop - Open source licenece - Free. Press Download Rstudio for Windows. I guess if you have macOS/Linux it will detect that automatically. If not, select it manually from the list below. Go to your Downloads folder and run the Rstudio.exe file. Use standard settings throughout the installation steps (click next-next-next-etc.). RStudio is now installed. Open RStudio and check that the print-out in the console matches the R version you just installed. At the time when this is written the newest version is 4.1.1 (2021-08-10) Kick Things. The printout at the top looks like this: R version 4.1.1 (2021-08-10)  Kick Things Copyright (C) 2021 The R Foundation for Statistical Computing Platform: i386-w64-mingw32/i386 (32-bit) If you had R and Rstudio already, but updated to the newest version, remember that you will have to also install all your packages again. 1.2.2 R community and packages Currently, there are over 18 000 packages available on the official CRAN server - open source and free. If you have a problem and need a smart function for solving that, you will most likely find that someone else have had the same problem, already solved it and made an R package that you can use. The large R community is also very active on the net forum stackexchange.com. If you have a programming issue and google it, you will typically end up in one of these forums where someone else has posted a question similar to yours and others from the community has provided a solution. I have been using R for over 10 years now and this strategy for solving programming issues has not failed me yet. Once you have found an R package you want to install, you can install it using the install.packages function. We will use the tidyverse package developed by Wickham et al. (2019) as an example. This is a bundling package built up of many packages, which we will use throughout the course. install.packages(&quot;tidyverse&quot;) To load a package into you R environment, making all its functionality available to you, you can use the library or require functions. The two are quite equal, but if you do not have the package installed require will cast a warning and continue, while library will cast an error and stop. library(tidyverse) ## -- Attaching packages ------------------------------------------------------------------------------------------------------------- tidyverse 1.3.1 -- ## v tibble 3.1.6 v dplyr 1.0.8 ## v tidyr 1.2.0 v stringr 1.4.0 ## v readr 2.1.2 v forcats 0.5.1 ## v purrr 0.3.4 ## -- Conflicts ---------------------------------------------------------------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() As part of the output here, you can see all the packages that tidyverse attaches to your working environment. We will be using many of these. 1.2.3 Datacamp As a part of the course, you will be given access to Datacamp. We will suggest courses to take there in combination with the lectures provided. If you want a more thorough recap of using R, you can take the course Introduction to R or test yourself with the practice module Introduction to R. 1.3 Data preprocessing Data pre-processing techniques typically involve addition, deletion or transformation of training set data. Preparing the data in a good way can make or break the predictive ability of a model. How the predictors enter the model is important. By transforming the data before feeding them to the model one may reduce the impact of data skewness or outliers. This can signifcantly improve the models performance. The need for data pre-processing may depend on the method you are using. A tree-based model is notably insensitive to the characteristics of the predictor data, while a linear regression is not. How the predictors are encoded, called Feature engineering, can also have a huge impact on the model performance. There are many ways to encode the same information. Combining multiple predictors can sometimes be more effective than using the individual ones. For example, using the body mass index (BMI = weight/length\\({}^2\\)) instead of using length and weight as separate predictors. The modelers understanding of the problem can often help in choosing the most effective encoding. Different ways of encoding a date predictor can be The number of days since a reference date Using the month, year and day of the week as separate predictors The numeric day of the year (julian day) Whether the date was within the school year (as opposed to holiday) 1.4 Case study We will use the same example as Kuhn and Johnson (2016) from the R package install.packages(&quot;AppliedPredictiveModeling&quot;) 1.4.1 Data transformations for individual predictors library(AppliedPredictiveModeling) data(&quot;segmentationOriginal&quot;) segData &lt;- subset(segmentationOriginal, Case == &quot;Train&quot;) 1.4.2 Centering and scaling The most straightforward and common transformation of data is to center and scale the predictor variables. Let \\(\\textbf{x}\\) be the predictor, \\(\\overline{x}\\) its average value and \\(sd(x)\\) its standard deviation. Centering means to substract the average predictor value from all the predictor values (\\(\\textbf{e} = \\textbf{x}-\\overline{x}\\)) and scaling means dividing all values by the empirical standard deviation of the predictor (\\(\\textbf{e}/sd(x)\\)). By centering the predictor, the predictor has zero mean and by scaling, you coerce the value to have a common standard deviation of one. A common term for a centered and scaled predictor is a standarized predictor, indicating that if you do this to all predictors they are all on a common scale. In the Figure 1.1 below, we have simulated 1000 normally distributed observations with mean 22 and standard deviation 4 and plotted a histogram of them (left panel) and a histogram of them standardized (right panel). As you can see, the distribution looks the same, but the scale on the x-axes has changed. library(ggplot2) set.seed(123) x &lt;- rnorm(1000, mean = 22, sd = 4) df &lt;- data.frame(x = c(x, (x-mean(x))/sd(x)), mean = rep(c(22, 0),each = 1000), type = rep(c(&quot;Original&quot;, &quot;Standardized&quot;), each = 1000)) ggplot(data= df,aes(x = x, fill = type)) + geom_histogram() + facet_wrap(~type, scales = &quot;free&quot;) + geom_vline(aes(xintercept = mean), col = 1, lty = 2) + scale_y_continuous(expand =c(0,0)) + theme(legend.title = element_blank(), legend.position = &quot;none&quot;)+ xlab(&quot;&quot;) Figure1.1: Simulated normally distributed variables on original- and standardized scale. 1.4.3 Tranformations to resolve skewness As noted above, standardizing preserves the distribution, but sometimes we need to remove distributional skewness. An un-skewed distribution is roughly symmetric around the mean. A right-skewed distribution has a larger probability of falling on the left side (i.e. small values) than on the right side (large values). We have illustrated left-, right- and un-skewed normal distributions in Figure 1.2. x &lt;- seq(-4,4,by = .1) alpha &lt;- c(-5,0,5) df &lt;- expand.grid(&quot;x&quot; = x,&quot;alpha&quot; = alpha) df &lt;- df %&gt;% mutate(case = ifelse(alpha == -5, &quot;Left skewed&quot;, ifelse(alpha ==5, &quot;Right skewed&quot;, &quot;Unskewed&quot;)), delta = alpha/sqrt(1+alpha^2), omega = 1/sqrt(1-2*delta^2/pi), dens = sn::dsn(x,xi = -omega*delta*sqrt(2/pi), alpha =alpha, omega = omega)) ggplot(df, aes(x = x,y = dens)) + geom_line() + facet_wrap( ~case, scales = &quot;fixed&quot;, nrow = 3, strip.position = &quot;right&quot;) + geom_vline(xintercept = 0, lty = 2) + xlab(&quot;&quot;) + ylab(&quot;Density&quot;) Figure1.2: Skewed normal distributions with mean 0 and standard deviation 1. A rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. One can calculate the skewness statistic and use that as a diagnostic. Is it close to zero, the distribution if roughly symmetric. Is it large, indicates right-skewness and negative values indicates left-skewness. The formula for the sample skewness statistic is given by \\[\\text{skewness} = \\frac{\\sum_{i=1}^n (x_i-\\overline{x})^3}{(n-1)v^{3/2}},\\quad \\text{where}\\quad v = (n-1)^{-1}\\sum_{i=1}^n(x_i-\\overline{x})^2,\\] \\(x\\) is the predictor variable, \\(n\\) the number of values and \\(\\overline{x}\\) is sample mean. Transforming the predictor with the log, square root or inverse may help remove the skew. Alternatively, a Box-Cox transformation can be used. The Box-Cox family of transformations are indexed by a parameter denoted \\(\\lambda\\) and defined by \\[x^\\star = \\begin{cases} \\frac{x^\\lambda-1}\\lambda, &amp; \\text{if }\\lambda\\neq 0\\\\ \\log(x), &amp; \\text{if }\\lambda = 0. \\end{cases}\\] In addition to the log transformation, the family can identify square transformation (\\(\\lambda = 2\\)), square root (\\(\\lambda = 0.5\\)), inverse (\\(\\lambda = -1\\)) and other in-between. By using maximum likelihood estimation on the training data, \\(\\lambda\\) can be estimated for each predictor that contain values greater than zero. References "],["lecture2.html", " 2 Over-fitting and model tuning, selection and evaluation and multiple regression 2.1 Overfitting 2.2 Training, validation and test split 2.3 Multiple regression", " 2 Over-fitting and model tuning, selection and evaluation and multiple regression In this lecture we will cover the terms overfitting, training-, validation- and test sets, model selection and -evaluation. These terms are part of the chargong of predictive modelling and is something we need to get familiar with before learning about specific models. Towards the end of this lecture, we will learn about multiple regression. 2.1 Overfitting Overfitting is the phenomenon when you build a model that fits (almost) perfectly to the training set, but when applied to new data the performance is low. For the modeller there is a balance between choosing a model that has a good fit to the training data, but also generalize well to new data. The goal for the model should be to discover the underlying signal and not fit to all the noise. We will illustrate overfitting by an example with simulated data and GAM models (topic for the lecture Lecture 3) set.seed(3) x &lt;- seq(0,2*pi,0.1) z &lt;- sin(x) y &lt;- z + rnorm(mean=0, sd=0.5*sd(z), n=length(x)) df &lt;- cbind.data.frame(x,y,z) p &lt;- ggplot(df, aes(x = x, y = y)) + geom_point() + geom_line(aes(y=z), lwd = .8, col = 2) p In the figure above, the true underlying signal is the red curve while the black dots are the observations. If we fit a model with a high level of flexibility, we can make it fit well to the observations. p + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k = 50, sp = 0)) We will come back to the details of the model setup when discussing GAM models in the next lecture, but the point here is that we have turned off the likelihood penalization by setting sp=0 in the smoother function s and set the order of the smoother to a high number (k = 50). If we set the order to k = 4 and let the GAM procedure set the penalization itself, we get a much smoother curve that lies closer to the true signal. p + geom_smooth(method = &quot;gam&quot;, formula = y ~ s(x,k = 4)) The first model explains 96.3% of the deviance, while the second explains 81.7%. In a normal situation, we would of course not know the true underlying signal, but overfitting can be suspected when you get such a wiggly prediction curve that seems to follow every little move in the observations. Judging by the fit to the points, the first GAM model is much closer to the observations, but we can check how well it would perform on a new set of data with the same structure. library(mgcv) # package for fitting gam models ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## This is mgcv 1.8-38. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. # Fit model with high level of complexity: fit1 &lt;- gam(y ~ s(x, k= 50, sp = 0), data = df) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x, k = 50, sp = 0) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02501 0.04060 -0.616 0.549 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 49 49 6.895 0.000262 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.823 Deviance explained = 96.3% ## GCV = 0.50314 Scale est. = 0.10382 n = 63 # Fit simpler model: fit2 &lt;- gam(y ~ s(x, k= 4), data = df) summary(fit2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x, k = 4) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.02501 0.04238 -0.59 0.557 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 2.988 3 87.32 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.807 Deviance explained = 81.7% ## GCV = 0.12081 Scale est. = 0.11317 n = 63 # simulate new set of data: set.seed(4) newdata &lt;- data.frame(x, z, y = z+rnorm(mean=0, sd=0.5*sd(z), n=length(x))) # Make predictions from the two models: newdata$pred1 &lt;- predict(fit1, newdata=newdata) newdata$pred2 &lt;- predict(fit2, newdata=newdata) # Calculate sum of squared residuals: with(newdata, c(&quot;SS1&quot; = sum((y-pred1)^2), &quot;SS2&quot; = sum((y-pred2)^2))) ## SS1 SS2 ## 14.160736 7.734603 As you can see, the sum of squared residuals is almost twice as high for the complex model when applied to new observations, compared to the simpler model. In a preditive modelling situation, it is the prediction performance that is important and not as much the deviance explained on training data. Below is a figure with the two predicition curves on the new data. Since \\(x\\) is the only input to the models, the two lines are equal to the curves above, but the data is new. # Plot the new data: ggplot(data= newdata, aes(x=x,y=y)) + geom_point() + geom_line(aes(y=pred1, col = &quot;fit1&quot;))+ geom_line(aes(y=pred2, col = &quot;fit2&quot;)) 2.2 Training, validation and test split In order to evaluate a predictive models performance, we should test its predictive abilities on data the model has never seen before (during model fitting). Usually, you only have one dataset to start with, so it is common to split the original data into three subsets; a training set, a validation set and a test set. The training set is the data used to train the model, meaning estimating its parameters. If your original data is split, this will typically be the biggest portion of the data. One should measure the model performance on the training data, but this should not be used solely for model selection. If your model has a very good fit to the training data, this can often lead to overfitting issues when applying the model to new data. This is one of the reasons for using a validation set. The validation set is used for tackling overfitting and doing model selection. This is a smaller portion of the data not seen during training, which is key. We can therefore measure the models performance on these unseen data and if the model performs similarly as on the traning data, this means it generalizes well to new situations and overfitting is likely not a big issue. If the performance is high in training and low for validation, this is a sign of overfitting. If you are in a scenario where you have many different candidate models, either from different model families or the same model but with different setups, you can use the validation set to select the best model according to the performance criteria you have chosen. Once you have found a model that does not overfit to training data and performs well on the validation set, you can apply it to the test set. This should only be used to measure the models performance. In some cases, one sees modellers only splitting the data in two, a training and testing set. If you use the validation set to measure the models performance, this will give a biased estimate of model performance, since you have selected the model based on the validation set. Basically, you will not know whether your model performs better because of changes you made or because it just happened to fit the validation set better. Therefore, we need a separate test set. The performance on the test set should ideally be similar to the performance on the validation set. If it is significantly lower, this can indicate overfitting to the validation set. Figure2.1: Summary of the three-way data split. We will illustrate how this threeway split can be used on an example, after we have learned about multiple regression. 2.3 Multiple regression Multiple regression is an extension of simple linear regression, where more than one covariate is used to predict the value of the dependent variable (the Y). The form of the predictor is a linear combination of the set of explanatory variables, i.e. \\[\\widehat Y_i = \\beta_0 + \\beta_1X_{i1} + \\ldots + \\beta_p X_{ip},\\quad i=1,\\ldots,n,\\] where \\(\\widehat Y_i\\) is the predictor of observation \\(i\\), \\(\\beta_0, \\ldots, \\beta_p\\) are the parameters and \\(X_{\\cdot 1}, \\ldots, X_{\\cdot p}\\) are the vectors of explanatory variables. It is quite common to write the equation above on vector form. Define the vectors \\(\\widehat{\\mathbf{Y}}=(\\widehat Y_1, \\ldots, \\widehat Y_n)&#39;\\) and \\(\\boldsymbol{\\beta} = (\\beta_0, \\ldots, \\beta_p)&#39;\\), and the design matrix \\(\\mathbb X = (\\mathbf{1}, \\mathbf X_{\\cdot 1},\\ldots, \\mathbf X_{\\cdot p})\\). Then the equation above can be written as \\[\\widehat{\\mathbf{Y}} = \\mathbb X\\,\\boldsymbol\\beta.\\] The assumptions are that the residuals, \\(Z_i = Y_i - \\widehat Y_i\\), \\(i=1,\\ldots, n\\) are independent and normally distributed. The explanatory variables should not be correlated. Parameters are estimated using maximum likelihood estimation. Note that we are using capital letters here. This is a statistical convention when we are talking about variables. Once we introduce observations, we switch to small letters for the same quantities. To learn more about multiple regression, take the datacamp course Multiple and Logistic Regression in R. For now you can focus on the multiple regression part. We will simply go on to illustrate usage by an example. 2.3.1 Example In this example, we consider a dataset containing the impact of three advertising medias (youtube, facebook and newspaper) on sales for different companies. The advertising budgets and sales are in thousands of dollars and the advertising experiment has been repeated 200 times. We will use multiple regression to model the relationship between sales and the advertising budgets from the different medias. In the video below we walk you through the example, but you can also read it below. We start by looking at the data: # load data: data(&quot;marketing&quot;, package = &quot;datarium&quot;) head(marketing) ## youtube facebook newspaper sales ## 1 276.12 45.36 83.04 26.52 ## 2 53.40 47.16 54.12 12.48 ## 3 20.64 55.08 83.16 11.16 ## 4 181.80 49.56 70.20 22.20 ## 5 216.96 12.96 70.08 15.48 ## 6 10.44 58.68 90.00 8.64 plot(marketing) From looking at the plot above, it does not seem to be a very strong correlation between the three covariates: youtube, facebook and newspaper. This is based on that the first 3x3 scatters plots seem to be randomly distributed without any clear patterns. Looking at the last row of panels we see the marginal relationships between the covariates and the response; sales. Here it looks like the marginal relationship between youtube and facebook variables is close to linear, while for newspaper it does not look like a linear relationship is well suited. We will therefore use facebook and youtube to predict sales. We will also check if including newspaper can improve the fit. We create a train and a test set, by doing a 80-20 random split (80% for training set - 20% test set): set.seed(123) train.ind &lt;- sample(1:nrow(marketing), nrow(marketing)*.8, replace = FALSE) trainset &lt;- marketing[train.ind, ] testset &lt;- marketing[-train.ind, ] We start by fitting the following model: \\[\\mathrm{sales} = \\beta_0 + \\beta_1\\cdot \\mathrm{youtube} + \\beta_2\\cdot \\mathrm{facebook}+\\beta_3\\cdot \\mathrm{youtube}\\cdot\\mathrm{facebook}\\] This model can be set up by different formula arguments in R. The different model calls below are equivalent. mod1 &lt;- lm(sales ~ youtube + facebook + youtube:facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ youtube + facebook + youtube:facebook, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 mod1 &lt;- lm(sales ~ 1 + youtube + facebook + youtube:facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ 1 + youtube + facebook + youtube:facebook, ## data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 mod1 &lt;- lm(sales ~ youtube * facebook, data = trainset) summary(mod1) ## ## Call: ## lm(formula = sales ~ youtube * facebook, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.4636 -0.4788 0.2331 0.7317 1.7508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.077e+00 3.375e-01 23.929 &lt; 2e-16 *** ## youtube 1.914e-02 1.702e-03 11.248 &lt; 2e-16 *** ## facebook 2.639e-02 1.003e-02 2.631 0.00937 ** ## youtube:facebook 9.187e-04 4.956e-05 18.536 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.174 on 156 degrees of freedom ## Multiple R-squared: 0.9655, Adjusted R-squared: 0.9649 ## F-statistic: 1457 on 3 and 156 DF, p-value: &lt; 2.2e-16 As you can see from the outputs, all models are equivalent. If you want to learn more about setting the formula argument, check out the helpsite for the formula function: ?stats::formula We will choose the model that has the highest predictive ability. We will therefore suggest several models and choose the one that has the lowest value of Akaikes information criteria (AIC). We will also evaluate the different models on the test set. mod2 &lt;- lm(sales ~ youtube * facebook * newspaper, data = trainset) mod3 &lt;- lm(sales ~ youtube * facebook + newspaper, data = trainset) mod4 &lt;- lm(sales ~ youtube, data = trainset) mod5 &lt;- lm(sales ~ facebook, data = trainset) mod6 &lt;- lm(sales ~ newspaper, data = trainset) AIC(mod1, mod2, mod3, mod4, mod5, mod6) ## df AIC ## mod1 5 511.4668 ## mod2 9 517.5961 ## mod3 6 512.9814 ## mod4 3 894.2953 ## mod5 3 986.9928 ## mod6 3 1043.6815 We choose the model with lowest AIC value. As you can see from the output above, this is the model we named mod1. Adding newspaper as covariate will provide the model with more information, but the cost of adding more parameters to be estimated is deemed higher than the benefit of including this information in the model according to AIC. We can also use the predictive abilities of the models on the test set to choose model. We will restrict ourselves to the top three model based on AIC and use root mean square error (RMSE) to assess the quality of the predictions. First we evaluate the in-sample prediction. That is, we calculate predictions on the training set and summarize by RMSE. trainset %&gt;% bind_cols( mod1 = predict(mod1, newdata = trainset), mod2 = predict(mod2, newdata = trainset), mod3 = predict(mod3, newdata = trainset) ) %&gt;% pivot_longer(cols = 5:7, names_to = &quot;models&quot;, values_to = &quot;pred&quot;) %&gt;% group_by(models) %&gt;% summarize(RMSE = sqrt(mean((sales-pred)^2))) ## # A tibble: 3 x 2 ## models RMSE ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 1.16 ## 2 mod2 1.15 ## 3 mod3 1.16 Perhaps not very surpising, the most complex model has the lowest in-sample, but the differences seem small. Lets evaluate the models on the test set as well. predictions = testset %&gt;% bind_cols( mod1 = predict(mod1, newdata = testset), mod2 = predict(mod2, newdata = testset), mod3 = predict(mod3, newdata = testset) ) %&gt;% pivot_longer(cols = 5:7, names_to = &quot;models&quot;, values_to = &quot;pred&quot;) head(predictions) ## # A tibble: 6 x 6 ## youtube facebook newspaper sales models pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 240. 3.12 25.4 12.7 mod1 13.4 ## 2 240. 3.12 25.4 12.7 mod2 13.4 ## 3 240. 3.12 25.4 12.7 mod3 13.4 ## 4 245. 39.5 55.2 22.8 mod1 22.7 ## 5 245. 39.5 55.2 22.8 mod2 22.7 ## 6 245. 39.5 55.2 22.8 mod3 22.7 predictions %&gt;% group_by(models) %&gt;% summarize(RMSE = sqrt(mean((sales-pred)^2))) ## # A tibble: 3 x 2 ## models RMSE ## &lt;chr&gt; &lt;dbl&gt; ## 1 mod1 0.957 ## 2 mod2 0.916 ## 3 mod3 0.967 We see that the RMSE is a bit lower for the test set. This indicates that we are not overfitting our models at least, since the models perform better on the test set. Solely based on the prediction on the test set, we would choose mod2, which is the full model, including youtube, facebook and newspaper with all interaction terms included. Depending on what one will use the model for, one may choose mod1 or mod2. We can look at the summary output of mod2. summary(mod2) ## ## Call: ## lm(formula = sales ~ youtube * facebook * newspaper, data = trainset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.0556 -0.4672 0.2466 0.7246 1.7399 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.933e+00 6.277e-01 12.640 &lt; 2e-16 *** ## youtube 1.937e-02 2.996e-03 6.466 1.3e-09 *** ## facebook 1.860e-02 1.872e-02 0.994 0.322 ## newspaper 8.807e-03 1.945e-02 0.453 0.651 ## youtube:facebook 9.622e-04 9.108e-05 10.564 &lt; 2e-16 *** ## youtube:newspaper -2.513e-05 8.613e-05 -0.292 0.771 ## facebook:newspaper 2.382e-05 4.514e-04 0.053 0.958 ## youtube:facebook:newspaper -4.273e-07 2.092e-06 -0.204 0.838 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.183 on 152 degrees of freedom ## Multiple R-squared: 0.9659, Adjusted R-squared: 0.9644 ## F-statistic: 615.9 on 7 and 152 DF, p-value: &lt; 2.2e-16 Based on classical statistics, you would fix the non-significant estimates to zero (p-values below 5%) and choose a more parsimoneous model, like mod1. This example shows the difference between modelling for prediction (choose mod2) and modelling for interpretation (choose mod1). We can also look at the observations vs predictions plots for the three models we compared in terms of RMSE. It seems to be very small differences between the predictions from the different models. ggplot(predictions, aes(x = pred, y = sales)) + geom_point() + geom_abline(intercept = 0, slope = 1, col = 2, lty = 2) + facet_wrap(~models, ncol= 3) + xlab(&quot;Predicted&quot;) + ylab(&quot;Observed&quot;) "],["lecture3.html", " 3 Non-linear regression 3.1 GAM example", " 3 Non-linear regression As we saw in the previous lecture in the example illustrating the overfitting concept, we do not always have a linear relationship between the response and the covariates. In that example, the relationship was a periodic sine function. Other examples of non-linear relations can be polynomial-, exponential- and logistical functions. There are many non-linear regression models designed for solving different problems, but we will mainly focus on the generalized additive models (GAMs). These are quite closely related to generalized linear models (GLMs). There are a few options for R packages for GAMs, but we will focus on the mgcv package (Wood 2017). (#fig:examples of nonlinear relations)Examples of relationsships between x and y. In the figure above, we illustrate different types of relationships between x and y. The green one is a linear one, while the others are non-linear. The red curve is even an additive combination of some of the other types of curves, i.e.  \\[ y(x) = -0.3 x + 0.5 \\sin(x)+ 0.5 \\log(x+1) - 0.02 x^2 + \\exp((x+1)/10).\\] This is actually not so different from the principal idea behind GAMs. A GAM uses a set of basis functions, often called smoothers, to map the dependent variables and use the transformed variables as covariates in an additive manner. We will explain this further. For a simple linear model, we have that the expected value of the \\(i\\)th variable is \\(\\mathrm{E} Y_i = \\mu_i = \\beta_0+\\beta_1 X_i\\). For a generalized linear model, the relationship is mapped using a link function \\(g\\), such that \\(g(\\mu_i) = \\beta_0+\\beta_1X_i\\). For a generalized additive model we take it one step further, by also mapping the dependent variable: \\[g(\\mu_i) = \\beta_0 + \\sum_{j=0}^k \\beta_j f_j(X_i),\\] where \\(\\{f_j:\\, j = 1,\\ldots, k\\}\\) is a set of basis functions. The standard in the mgcv package is to use thin plate splines. 3.1 GAM example In the video below, Sondre goes through this sections example in a bit more detail and with more in-depth explanations of the code. You can also read the example below the video. We will simulate a dataset where the true relationship between X and Y is given by (for observation \\(i\\)), \\[Y_i = 0.5X_i + \\sin(X_i)+Z_i, \\quad \\text{where } Z_i\\sim N(0,0.29^2).\\] We start by generating the data in a tibble data frame: set.seed(123) # seed for reproducibility dat &lt;- tibble( x = seq(0, pi * 2, 0.1), sin_x = .5*x+sin(x), # expected value y = sin_x + rnorm(n = length(x), mean = 0, sd = sd(sin_x / 2))) # add noise head(dat) # print first 6 observations ## # A tibble: 6 x 3 ## x sin_x y ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 -0.162 ## 2 0.1 0.150 0.0833 ## 3 0.2 0.299 0.749 ## 4 0.3 0.446 0.466 ## 5 0.4 0.589 0.627 ## 6 0.5 0.729 1.22 We can plot the true underlying signal (black line) and the observations (black dots) adding a linear regression line (blue) ontop. Clearly a linear model would fit poorly to these data. ggplot(dat, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + geom_line(aes(y = sin_x)) ## `geom_smooth()` using formula &#39;y ~ x&#39; Instead, we will use a GAM model. As you will see below the syntax is very similar to the glm or even lm function in base R. library(mgcv) # gam package mod &lt;- gam(y ~ s(x), data = dat, method = &quot;REML&quot;) The mgcv package uses the syntax y ~ s(x) to specify the model. This means that y is modelled as a smooth function of x. If nothing else is specified, the procedure will use a thin plate spline and determine the number of basis functions to use itself. You can also fix this by setting the \\(k\\) argument of the \\(s()\\) function. When problems with overfitting occur, you can tackle this by setting a lower value for \\(k\\) or setting the smoothing parameter. This argument is \\(sp\\) in the \\(s()\\) function. We will not go into details about this here, but in general it is recommended to use method = REML. This means the model is using restricted maxmimum likelihood for the estimation and use this algorithm to set of the smoothing parameter for you. Lets look at the model summary. summary(mod) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## y ~ s(x) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.56585 0.03236 48.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(x) 6.367 7.517 37.88 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.821 Deviance explained = 83.9% ## -REML = 15.726 Scale est. = 0.065972 n = 63 Here you can see we have a significant intercept term and a approximate significance of the smooth term. The procedure uses an approximation here because the smooth term not really just one covariate. It is important to check that the estimated degree of freedom (edf) is not very close to the reference degree of freedom (Ref.df). This indicates that model has not been given enough flexibility and you could consider setting a higher \\(k\\) value for the smoother. Lets have a closer look at the smoothing terms being used in this model. In the code below we extract the model matrix and plot the basis function. To have a slightly deeper understanding of what the GAM model does is that it creates a weighted sum of these basis functions to, as optimally as possible, fit the underlying curve of the data. # Extract model matrix: MM &lt;- model.matrix(mod) MM &lt;- as_tibble(MM) MM$x &lt;- dat$x MM &lt;- pivot_longer(MM, -x) # Plot basis functions ggplot(MM, aes(x = x, y = value, group = name, col = name)) + geom_line() + theme_bw(12)+ theme(legend.position = &quot;none&quot;) + labs(y = &quot;Basis functions&quot;) If we multiply these basis functions with the weights for the fitted model, we get the additive terms that constitutes the full model prediction. # Extract coefficient from the model and merge model matrix: coefs &lt;- tibble(coef = coef(mod)) coefs$name &lt;- names(coef(mod)) MM &lt;- left_join(MM,coefs, by = &quot;name&quot;) # Plot model weigthed basis functions ggplot(MM, aes(x = x, y = value*coef, group = name, col = name)) + geom_line() + theme_bw(12)+ theme(legend.position = &quot;none&quot;) + labs(y = &quot;Model weigthed basis functions&quot;) Here you can see how the different curves contribute the full signal. Lets have a look at the final prediction and plot that along with a 95% confidence interval, the observations and the true signal. pred.mod &lt;- predict(mod, se.fit = TRUE) # predict values with standard error dat &lt;- dat %&gt;% mutate(pred = pred.mod$fit, lwr = pred - 1.96 * pred.mod$se.fit, upr = pred + 1.96 * pred.mod$se.fit) ggplot(data = dat, aes(x = x)) + geom_point(aes(y=y)) + geom_line(aes(y = pred), col = &quot;magenta&quot;)+ geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .3, fill = &quot;magenta&quot;) + theme_bw(12) + labs( x = &quot;x&quot;, y = &quot;y&quot; ) Normally, we would do the train-test split, but in this example we know the truth and the point of the exercise is to reproduce the underlying signal. As a last element of this example, we will see how well the model extrapolates as we try to predict the signal for values the model has not seen. For the training we used values of \\(x\\) between 0 and \\(2\\pi\\). Lets see how it performs for values of x between \\(2\\pi\\) and \\(3\\pi\\). # Setting up new data: dat.ext &lt;- tibble( x = seq(2*pi, 3*pi, 0.1), sin_x = 0.5*x+sin(x) # underlying signal ) # Predicting pred.ext &lt;- predict(mod, newdata = dat.ext, se.fit = TRUE, type =) # adding predictions to data frame: dat.ext &lt;- dat.ext %&gt;% mutate(pred = pred.ext$fit, lwr = pred - 1.96 * pred.ext$se.fit, upr = pred + 1.96 * pred.ext$se.fit) # Plotting results: ggplot(dat.ext, aes(x = x))+ geom_line(aes(y = pred), col = &quot;magenta&quot;)+ geom_ribbon(aes(ymin = lwr, ymax = upr), fill = &quot;magenta&quot;, alpha = 0.3)+ geom_line(aes(y = sin_x), col = &quot;red&quot;)+ theme_bw(12) First of all, note the width of the prediction intervals. As the model has not been trained on these values of \\(x\\), it is expected that these are quite wide. It is comforting that the underlying signal (in red) is inside the intervals, but as you can see the shape of the curve is not very near the true signal. Hence, such a prediction will not be very informative. Finally, lets make a plot, combining the prediction with the training data. Note that we add the y=NA since this column is not included in the dat.ext data frame. We also add the type column to separate the two data sources and split the coloring based on this column. dat.combined &lt;- rbind( dat %&gt;% mutate(type = &quot;training&quot;), # adding colum type to split dat.ext %&gt;% mutate(y=NA, type = &quot;extrapolation&quot;)# the two data sources by ) ggplot(dat.combined, aes(x=x, col = type, fill = type))+ geom_point(aes(y=y), color = &quot;black&quot;) + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = .4)+ geom_line(aes(y=pred))+ geom_line(aes(y=sin_x), col = &quot;yellow&quot;, lwd = 1.2, lty =2)+ theme_bw(12) ## Warning: Removed 32 rows containing missing values (geom_point). The warning about the 32 missing values are because we added y=NA to the dat.ext data frame. Data camp There is a very informative and useful GAM module on data camp that we can highly recommend called Nonlinear modeling in R with GAMs - especially chapter 1 and 2. References "],["lecture4.html", " 4 Classification methods 4.1 k-nearest neighbor 4.2 Naive bayes 4.3 Logistic regression", " 4 Classification methods Sometimes we are not interested in predicting a continuous output, but rather a factor or a class. We use classes or categories for many things and often we want a model to make a prediction into discrete categories. Is this an image of a cat or a dog? Given information of each passenger, is it likely that a certain individual would survive the shipwrecking of Titanic? Based on the score of certain hand-ins in a predictive modelling course, what final grade (A-F) is the student likely to get on the final exam? In this chapter we will learn about the classification methods k-nearest neighbor (knn), naive bayes and logistic regression. We will briefly consider all three methods here, and refer to the data camp course for further details (see data camp section below). library(class) data(&quot;titanic.raw&quot;, package = &quot;datarium&quot;) # test = titanic.raw[1,-4] # knn(train = titanic.raw[,-4],test =test, cl = titanic.raw[,4]) 4.1 k-nearest neighbor The method k-nearest neighbor (knn) is a very simple classificiation method, where for predicting a class of observations in the test set one find the \\(k\\) observations in the training set that has covariates nearest to the covariates of the test case in terms of Euclidean distance. There are many different implementations of knn, for instance in the class package (see ?class::knn). But since we will be using the package bundle tidymodels in the next lecture we will also use it here. Tidymodels is, similar to the tidyverse, a combination of many packages using a kind of pipe notation to build models. It is very well integrated with tidyverse and provides a general framework for many different models. We will use knn as an example of how we set up a model in the tidymodels setup. As an example for classification we will use the famous Iris dataset of Edgar Anderson (see ?iris). We start by loaded the packages and the data containing inforation of sepal- and petal- lengths and widths of three different iris flower species (Iris setosa, Iris versicolor and Iris virginica). The goal will be to make a model that can tell us which Iris flower species we are dealing with, based on the given lengths and widths. library(tidyverse) library(tidymodels) ## Registered S3 method overwritten by &#39;tune&#39;: ## method from ## required_pkgs.model_spec parsnip ## -- Attaching packages ------------------------------------------------------------------------------------------------------------ tidymodels 0.1.4 -- ## v broom 0.7.12 v rsample 0.1.1 ## v dials 0.1.0 v tune 0.1.6 ## v infer 1.0.0 v workflows 0.2.4 ## v modeldata 0.1.1 v workflowsets 0.1.0 ## v parsnip 0.1.7 v yardstick 0.0.9 ## v recipes 0.1.17 ## -- Conflicts --------------------------------------------------------------------------------------------------------------- tidymodels_conflicts() -- ## x nlme::collapse() masks dplyr::collapse() ## x scales::discard() masks purrr::discard() ## x dplyr::filter() masks stats::filter() ## x recipes::fixed() masks stringr::fixed() ## x dplyr::lag() masks stats::lag() ## x yardstick::spec() masks readr::spec() ## x recipes::step() masks stats::step() ## * Search for functions across packages at https://www.tidymodels.org/find/ df &lt;- as_tibble(iris) head(df) ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa We make our train-test split using the tidymodels::initial_split function to create the split and respecitvely the training and testing functions to extract the train and test sets. Note that we use strata = Specices. This means we are doing stratified subsampling so that we have the (roughly) the same proportion of the different species in the test and train sets. set.seed(999) split &lt;- initial_split(df, strata = Species) df_train &lt;- training(split) df_test &lt;- testing(split) To fit a model in tidymodels we must first specify which type of model (nearest neighbor) we want to create and set an engine and a mode. The engine is the package used to fit the model and the mode is usually classification or regression, depending on what type of y variable you are considering. knn_spec &lt;- nearest_neighbor() %&gt;% set_engine(&quot;kknn&quot;) %&gt;% # requires &quot;kknn&quot; package installed set_mode(&quot;classification&quot;) Then we are ready to fit the model using the fit function, where we specify a formula and which data to use. We will here use all covariates in the train set to predict Species. knn_fit &lt;- knn_spec %&gt;% fit(Species ~., data = df_train) knn_fit ## parsnip model object ## ## Fit time: 0ms ## ## Call: ## kknn::train.kknn(formula = Species ~ ., data = data, ks = min_rows(5, data, 5)) ## ## Type of response variable: nominal ## Minimal misclassification: 0.05405405 ## Best kernel: optimal ## Best k: 5 As you can see fromt he output, the best number of neighbors to be used was 5. The knn model will thus look for the 5 flowers that has the nearest covariates and then predict based on the majority vote of the five. For example if three of them are Iris Virginica and two are Iris versicolor, the prediction will be Iris Virginica. Let us predict on the test set and chech the accuracy and \\(\\kappa\\) using the metrics function. knn_fit %&gt;% predict(df_test) %&gt;% # predict on the test set bind_cols(df_test) %&gt;% # bind columns (adding the truth and covariates) metrics(truth = Species, estimate = .pred_class) # calculate metrics ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.949 ## 2 kap multiclass 0.923 The accurcay metric is simply the number of correct classifcation divided by the total number of predictions made. We get (roughly) 95% correct, which seems very acceptable. The Cohens kappa metric is useful when the data is imbalanced, e.g. you have many more Iris versicolor than Iris setosa in the data, such that predicting all flowers as Iris versicolor would also give a high accuracy. 4.2 Naive bayes 4.3 Logistic regression Logistic regression is a classical method for estimating the conditional probability of certain discrete outcomes given the value of covariates. In the Iris example, we can get estimates of the probability of the flower species being setosa, vercicolor or viriginca given the length and width of sepal and petal. For logistic regression the prediction is thus a value between 0 and 1, and this is achieved by mapping the linear prediction using the inverse logit link function, defined by \\[\\rm{logit}^{-1}(x) = \\frac{e^x}{1+e^x}, \\quad x\\in\\mathbb R.\\] We can also define the logit function directly, mapping probabilites \\(p\\in (0,1)\\) to \\(\\mathbb R\\), by \\[\\rm{logit}(p) = \\ln \\bigg(\\frac{p}{1-p}\\bigg)\\] Thus, if \\(x\\) is the vector of covariate for a certain flower and \\(\\beta\\) the parameter vector, we will model the probability of an Iris flower belonging to either of the three species given the set of lengths and widhts, by \\[P(Y=y|x) = \\rm{logit}^{-1}(x^\\prime \\beta)=\\frac{\\exp(x^\\prime \\beta)}{1+\\exp(x^\\prime \\beta)}.\\] Data camp We highly recommend the data camp course Supervised Learning in R: Classification - chapters 1-3. The subject of chapter 4 is covered separately in the next lecture 5. 4.3.1 Sources rpubs "],["lecture5.html", " 5 Trees 5.1 Decision trees 5.2 Random forrest", " 5 Trees In this lecture we will learn about tree based models. We start out with simple decision trees and end up with a random forrest. For this chapter we will use the tidymodels package (Kuhn and Wickham 2020). This can be installed by running install.packages(&quot;tidymodels&quot;) 5.1 Decision trees library(tidymodels) data(&quot;PimaIndiansDiabetes&quot;, package = &quot;mlbench&quot;) diabetes &lt;- PimaIndiansDiabetes diabetes_split &lt;- initial_split(diabetes, prop = 0.9, strata = diabetes) diabetes_train &lt;- training(diabetes_split) diabetes_test &lt;- testing(diabetes_split) counts_train &lt;- table(diabetes_train$diabetes) counts_test &lt;- table(diabetes_test$diabetes) 5.2 Random forrest Data camp We highly recommend the data camp course Machine Learning with Tree-Based Models in R chapters 1-4. References "],["lecture6.html", " 6 Support Vector Machines", " 6 Support Vector Machines SVM "],["lecture7.html", " 7 Artificial Neural networks 7.1 MLP 7.2 Objective functions and training 7.3 Validation of trained models 7.4 Implementation of MLP in Keras", " 7 Artificial Neural networks This section describes the basics theory and classical optimization of an artificial neural network. This includes defining a 2-layer feed forward neural network, cost and objective functions, regularization and the manner in which a network can be trained to produce desired output. The section will be limited to so-called supervised learning, i.e. where know the true state of an output. The main references for this chapter are the textbook Deep Learning by Goodfellow et al. (2016). 7.1 MLP Let \\(\\mathbf{X}^{(i)}\\) be an instance of data which is input to a deep learning model, with associated target values \\(\\mathbf{y}^{(i)}\\). A target value can be a class or as we will use later the data \\(\\mathbf{X}^{(i)}\\) itself. All the instances of \\(\\mathbf{X}^{(i)},\\) \\(i=1, \\ldots N,\\) constitute the data set \\(\\mathbf{X}^{(i)}\\), and all the target values \\(\\mathbf{y}^{(i)},\\) \\(i=1, \\ldots N,\\) comprise the data set \\(\\mathbf{Y}.\\) \\ A feed forward neural network is a hierarchical model that consists of nodes or computational units divided into subsequent layers. For each node, a non-linear activation function is applied. The nodes between each layer are connected, so that the input to a node is totally dependent on the output from the nodes of the previous layer. The model is called a if there are multiple hidden layers; see . The simplest deep learning model has at least one hidden layer: an input layer and an output layer. This hierarchical structure makes it possible to formulate the deep learning model as a linear system of equations. \\ Illustration of a deep neural network with three hidden layers The model input to a neural network is here defined as vector \\(\\mathbf{x}^{(i)}\\) with \\(Q\\) elements. The input is transformed linearly by \\(\\mathbf{W}_1\\) and \\(\\mathbf{b}\\) such that \\(f(\\mathbf{x}^{(i)}) = \\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b}\\). \\(\\mathbf{W}_1\\) transforms the input to a vector of \\(K\\) elements and is often called the weight matrix, while the translation \\(\\mathbf{b}\\) is referred to as the bias. The bias can be interpreted as a threshold for when the neuron activates. \\ A nonlinear activation function is applied element wise to the transformed data. The activation function is typically given as a rectified linear unit (ReLU) Nair and Hinton (2010) (legg inn Relu equation!!) or \\(\\tanh\\) function. This activation introduces non-linearity to the other linear operations. The superposition of the linear and nonlinear transformation is in combination with the activation function and is what we refer to as a hidden layer. \\ Applying another linear transformation \\(\\mathbf{W}_2\\) to the hidden layer results in this case to the model output or output layer. The size of the output layer is a row vector with \\(D\\) elements. Generally, many transformations and activations can be applied consecutively which will result in a more complex hierarchical model. A generalization to a network with several hidden layers is straightforward; to make this clear we here limit the notation to a single hidden layer. We note that \\(\\mathbf{x}^{(i)}\\) is vector of size \\(Q\\), \\(\\mathbf{W}_1\\) is a \\(K \\times Q\\) matrix that transforms the input to \\(K\\) elements, \\(\\mathbf{W}_2\\) is a \\(D \\times K\\) matrix, transforming the vector into \\(D\\) elements and \\(\\mathbf{b}\\) consists of \\(K\\) elements. We write this as linear system of equations transformed with the activation function \\(\\sigma\\) \\[\\widehat{\\mathbf{y}}^{(i)} = \\mathbf{W}_2 (\\sigma(\\mathbf{W}_1 \\mathbf{x}^{(i)} + \\mathbf{b})) := \\mathbf{f}^{\\omega}, \\qquad \\omega = \\{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}\\}\\] Depending on how the output layer is defined, we can use the network for classification or regression tasks. For classification purposes, the number of nodes in the output layer equals the number of classes, and typically transformed with a softmax function Goodfellow, Bengio, and Courville (2016). The softmax function is a generalization of the logistic map that normalizes the output relative to the different classes (Legg inn softmax equation !!). In regression problems we want to estimate relations between variables; we want to predict a continuous output based on some input (variables). To use a linear activation function on the output layer will serve this purpose. It has been shown that ANN is a universal approximation Hornik, Stinchcombe, and White (1989); thus, our goal is to find the weights of the given network to best approximate the map from the input to the output. This means that we want to estimate the weights of the ANN \\(\\mathbf{\\omega}\\), given the input data \\(\\mathbf{x}^{(i)}\\), the target \\(\\mathbf{y}^{(i)}\\) such that the predictions \\(\\widehat{\\mathbf{y}}^{(i)}\\) is minimized towards the true target values \\(\\mathbf{y}^{(i)}\\). This is a typical optimization problem, which can be minimized with an objective function and optimization procedure. 7.2 Objective functions and training An objective function for use in deep learning typically contains two terms: cost function and regularization. The cost function takes the predicted and the true values as input. Depending on the task and what one wants to minimize, the cost function maximizes a likelihood. In classification problems this is can be the negative cross entropy \\[\\mathcal{C}_1^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) = - \\frac{1}{N}\\sum\\limits_{j=1}^{N}\\mathbf{y}^{(i)}_j\\log(\\widehat{\\mathbf{y}}^{(i)}_j) = -\\log p(\\mathbf{Y}|\\mathbf{f}^{\\mathbf{\\omega}}(\\mathbf{X})),\\] and in regression problems the Mean Squared Error (MSE) \\[\\mathcal{C}_2^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(\\mathbf{y}^{(i)} - \\widehat{\\mathbf{y}}^{(i)})^2 = - \\log p(\\mathbf{Y}|\\mathbf{f}^\\mathbf{\\omega}(\\mathbf{X})),\\] Minimization of the negative cross entropy and the MSE is well known to be equivalent to minimize the negative log likelihood of the parameter estimation Tishby, Levin, and Solla (1989) for neural networks. Depending on the task, minimizing or with respect to the parameters \\(\\omega = \\{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}\\}\\) maximizes the likelihood of these parameters. The choice of the cost function is not restricted to those given above, and depend on the data, the model structure and what one wants to predict with the model.\\ One of the key problems in deep learning is a phenomenon called over-fitting (some blue Ref to Section ). Over-fitting occurs if the optimized model performs poorly on new unseen data, i.e. it does not generalize well. To address this problem, regularization is added to the cost function. \\ Regularization is a general technique, where the goal is to make an ill posed problem well-posed (Ref. her kanskje?). Over-fitting is basically one example of an ill-posed problem. For optimization problems, you could add a penalizing functional: L2 or L1 norm for the parameters; or use dropout (Ref. XX). \\ Regularization in neural networks work by penalizing the cost function, e.g. forcing the weights to become small. The idea behind a specific regularization term could be to minimize the weights of the ANN to generate a simpler model that helps against over-fitting. \\(L2\\) regularization multiplied with some penalizing factor \\(\\lambda_i\\) is one of the most common regularization techniques. The cost function with regularization is called the objective function. Adding \\(L2\\) regularization to equation or result in the objective function \\[\\mathcal{L}(\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}) = \\mathcal{C}^{\\mathbf{W}_1, \\mathbf{W}_2, \\mathbf{b}}(\\mathbf{X},\\mathbf{Y}) + \\lambda_1||\\mathbf{W}_1||^2 + \\lambda_2||\\mathbf{W}_2||^2 + \\lambda_3||\\mathbf{b}||^2\\] Another common way of regularizing the cost function is through dropout, which is a stochastic regularization technique. In the example later we will utelize dropout as regularization. \\ Minimizing the objective in with respect to the weights \\(\\mathbf{\\omega}\\) with an objective function and a gradient descent optimization method has proven to give good results in a wide range of applications. \\ The gradient descent method Curry (1944) updates the parameter \\(\\mathbf{\\omega}\\) using the entire data set \\[\\mathbf{\\omega}_t = \\mathbf{\\omega}_{t-1} - \\eta \\nabla \\mathcal{L}(\\mathbf{\\omega}_{t-1}). \\] Here \\(\\omega_t\\) represents the current configuration of the weights, while \\(\\omega_{t-1}\\) represents the previous one. The parameter \\(\\eta\\) is referred to as the learning rate, i.e. how large the step in the negative gradient direction the update of the weights should be. Too small steps can lead to poor convergence, while to large steps can lead to overshooting, i.e. missing local/global minimums. Usually it is too expensive to calculate the gradient over the entire dataset. This is solved by a technique called stochastic gradient descent Robbins and Monro (1951). Stochastic gradient descent performs a parameter update for each training example. A natural extension and a more cost-efficient approach is the mini-batch gradient descent approach. In mini-batch optimization, the gradient is approximated by calculating the mean of the gradients on sub-sets or batches of the entire data set, \\[\\mathbf{\\omega}_t = \\mathbf{\\omega}_{t-1} - \\frac{\\eta}{n}\\sum\\limits_{i=1}^{n}\\nabla \\mathcal{L}_i(\\mathbf{\\omega}_{t-1}).\\] The mini-batch gradient descent iterative process can be implemented in the neural network with the back-propagation algorithm Rumelhart et al. (1988). In back-propagation, the weights are updated through a forward and backward pass. In the forward pass, we predict with the current weight configuration and compare towards the target values. In the backward pass, we use the chain rule successively from the output to the input to calculate the gradient of \\(\\omega\\). Based on the gradient direction and the learning rate, the configuration of the weights is updated. To find the correct learning rate is difficult, hence, several methods has been developed to adjust the learning rate adaptively. One of the disadvantages of the vanilla gradient descent approach to the ANN optimization problem, is that it has a fixed leaning rate \\(\\eta.\\) In line with the development of ANN, methods dedicated to deep learning and adaptive adjustment of the learning rate have been developed. Besides SGD with momentum Sutskever et al. (2013), the two most used optimization methods for ANNs are ADAM Kingma and Ba (2014) and Root Mean Square Propagation (RMSProp) Tieleman and Hinton (2012). RMSProp adaptively adjusts the learning rate of the gradients based on a running average for each of the individual parameters. The ADAM-algorithm individually adjusts the weights in terms of both the running average, but also with respect to the running variance. \\ The use of back-propagation together with a stochastic gradient descent method, increase in available data and hardware have been the successes of deep learning during the past decade. 7.3 Validation of trained models To validate and ensure that the predictions of the deep learning model also performs well on new unseen instances, the data is split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. \\ After each epoch, we predict with this temporally model on the validation data set. Usually we put criteria on the performance on the validation data set for when to stop the optimization. We can use a so called early stopping regime, where the model stops training if it does not see improvement on the validation score after a certain number of epochs without improvement. \\ The purpose of the test data set is to validate on new unseen data that has not been part of the training or the continuous validation of the model. Lets look at an example of how to implement a classifier on the mnist data in keras   7.4 Implementation of MLP in Keras In this section we will show you how to implement a MLP with dropout as regularization with the low level API for tensorflow called Keras. A lot of the development within deep learning is concentrated around the python programming language. Keras is also a tool that is originally developed in keras, but also available in R. Lets start by installing tensorflow: 7.4.1 Installing Keras and Tensorflow First install tensorflow with the following commands install.packages(&#39;tensorflow&#39;) library(&#39;tensorflow&#39;) install_tensorflow() Then install keras with the following commands install.package(&#39;keras&#39;) library(keras) install_keras() 7.4.2 Importing MNIST We will use the famous MNIST data set as an example in this section (Ref. XX). The MNIST dataset contains 60000 handwritten digits for training the model, and 10000 for testing. The digits has a shape of 28x28 pixels in grey scale, that is, they only have one channel, describing the black/white intensity of the pixel value. Each picture/instance is associated with a label, that is a number from 0-9. The keras package contains a function for downloading the MNIST dataset from the source (Ref. XX) library(keras) mnist &lt;- dataset_mnist() To ensure that the predictions of the neural network model also performs well on new unseen instances, the data is often split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize/train the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. We will come back to validation and testing of the neural network later in this section. The MNIST dataset are split into a test and train data set from source # x_train &lt;- mnist$train$x y_train &lt;- mnist$train$y x_test &lt;- mnist$test$x y_test &lt;- mnist$test$y We can check out some of the training data #checking the dimension of the train/test data library(ggplot2) # visualize the digits par(mfcol=c(5,5)) par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) for (idx in 1:25) { im &lt;- x_train[idx,,] im &lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=&#39;n&#39;, main=paste(y_train[idx])) } #checking the dimension of the train/test data dim(y_train) head(y_train) Here we see that the training dataset has a shape of \\(60000 \\times 28 \\times28\\). This means that there are \\(60000\\) instances, or pictures of handwritten digits, where each of the pictures has \\(28 \\times 28\\) dimension in the horizontal and vertical direction. The dimension of the y_train variable represents the class or label of the x_train variable. 7.4.3 Preprocessing Pre-processing is the step where we transform, scale, removes, imputes etc. the data before we feed it to the deep learning model. Removing erroneous data, or impute NA values may be of crucial importance to get the algorithm run at all. Transforming or scaling the data helps improve the so called conditioning problem. Conditioning of a problem say something about the sensitivity of the solution to changes in the problem data. To create an easier problem to optimize, we pre-process the MNIST-data before we are feeding it to the neural network model. Here we reshape and scale the data. # reshape x_train &lt;- array_reshape(x_train, c(nrow(x_train), 784)) x_test &lt;- array_reshape(x_test, c(nrow(x_test), 784)) # rescale x_train &lt;- x_train / 255 x_test &lt;- x_test / 255 We also need to convert the y_train and y_test to categorical variables. Instead of having a vector with a number from 0-9, we convert it to a binary representation. That is, each label is represented by a vector of length 10 with one element is 1 and the rest 0. The element that is 1 represent the label of the digit. y_train &lt;- to_categorical(y_train, num_classes = NULL) y_test &lt;- to_categorical(y_test, num_classes = NULL) head(y_train) 7.4.4 MLP model In this example we want to create a so called dense neural network, or MLP. That is, each node are influenced from all of the nodes in the previous layer. This is contrast to e.g. convolutional neural networks that uses convolutions to reduce the connectivity between layers, thus reducing the connections. We construct a dense neural network. After reshaping the input, the network has a shape of \\(28 \\times 28 = 784\\) per instance. This is our input shape. This shape has to be specified in the model, as shown below. The first layer has \\(256\\) nodes, and we use the rectified linear unit as activations function. The next layer is a so called dropout layer. This layer drops randomly a proportion of the nodes out during a forward and backward pass. This will help avoiding overfitting, i.e. it is a form of regularization of the network. The next hidden layer has \\(128\\) nodes, and also ReLU activation function. The last layer of the network is the output. The output in this case, is to classify the label of the digit. There are 10 possible classes, 0-9 and we thus use a softmax activation function, with 10 units. model &lt;- keras_model_sequential() model %&gt;% layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(784)) %&gt;% layer_dropout(rate = 0.4) %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 10, activation = &#39;softmax&#39;) First we construct a sequential model object. Then we can use the pipe syntax to specify the layers of the neural network. We can use the summary function to show a nice overview of the neural network model. summary(model) 7.4.5 Compiling and training the model We now have a model, the next step is to compile it. Durring compiling we also have to specify the loss function, optimizer and metrics for evaluations and validation. model %&gt;% compile( loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39; ) Since we want to classify the digits, we have chosen the loss to be categorical cross entropy. We use the adam-algorithm during optimization and we also specify that we want to return the accuaracy of the classification during training and validation. After the compilation, we can call the fit() function to train our model. The fit function take x_train and y_train as inputs. We also have to specify how many epoch the model should be trained on (how many times we will see the entire data set), the batch size (how many/large the splits of the training data set) and how large proportion of the data should be used as online validation data. Test data are set aside for evaluation the model when it has finished training. history &lt;- model %&gt;% fit( x_train, y_train, epochs = 30, batch_size = 128, validation_split = 0.2 ) It is possible to specify specific validation data if that is desirable. Batch size, epochs, size of the neural network, e.g. number of layers nodes, type of network (e.g. cnn, rnns) are so called hyper parameters. This is parameters that are not optimized in the network itself, but has to be specified manually. Theses hyper parameters are important and there are different strategies for optimize them. Hyper parameter search could be done with traditional grid search, but also more sophisticated approaches such as Bayesian hyper optimization (Ref. XX). We have plotted the loss and accuaracy calculated after each training epoch. It can be observed that the loss after approx 10-15 epochs, is significantly lower than the validation loss. This is an indication that the model starts to overfit. 7.4.6 Evaluating the model To evaluate the trained model on the test data, we can use the pipe syntax together with the evaluate function model %&gt;% evaluate(x_test, y_test) The trained model performes pretty well on the test data set. It has an accuaracy of approximatley 0.98, that is 98 \\(\\%\\) of the data in the test data set is classified correctly. Lets ### Predicting We can also predict the most probable class for each of the instances. For this we can use the predict function. (Litt mer her) #model %&gt;% predict(x_test) %&gt;% k_argmax() prediction &lt;- model %&gt;% predict(x_test) #plot(prediction[1710,], ) dim(prediction) #Lets find one of the most uncertain predictions to view how it looks like #We find the most probable predictions in the test data set largest &lt;- apply(prediction,1,max,na.rm=TRUE) #Then we choose the predictions which has the lowest confident less_confident &lt;- sort(largest,index.return = TRUE)$ix[1:36] #We find out what the model predicts these less_confident predictions are pred_class &lt;- prediction %&gt;% k_argmax() #and plot the digits par(mfcol=c(6,6)) par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) for (idx in less_confident) { im &lt;- mnist$test$x[idx,,] im &lt;- t(apply(im, 2, rev)) image(1:28, 1:28, im, col=gray((0:255)/255), xaxt=&#39;n&#39;, main=paste(mnist$test$y[idx])) } References "],["lecture8.html", " 8 Unsupervised Learning", " 8 Unsupervised Learning We have finished a nice book. "],["lecture9.html", " 9 Feature selection/Explainable AI", " 9 Feature selection/Explainable AI "],["references.html", "References", " References "]]
