<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 5 Decision Trees and Bagged Trees | Data science with R: Applied Predictive Modelling</title>
  <meta name="description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content=" 5 Decision Trees and Bagged Trees | Data science with R: Applied Predictive Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 5 Decision Trees and Bagged Trees | Data science with R: Applied Predictive Modelling" />
  
  <meta name="twitter:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

<meta name="author" content="Sondre HÃ¸lleland and Kristian Gundersen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture4.html"/>
<link rel="next" href="lecture6.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">STAT623 Applied predictive modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes-and-objectives"><i class="fa fa-check"></i>Learning outcomes and objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecture-overview"><i class="fa fa-check"></i>Lecture overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#litterature"><i class="fa fa-check"></i>Litterature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>1</b> Introduction and short recap of R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="lecture1.html"><a href="lecture1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="lecture1.html"><a href="lecture1.html#prediction-or-interpretation"><i class="fa fa-check"></i><b>1.1.1</b> Prediction or interpretation?</a></li>
<li class="chapter" data-level="1.1.2" data-path="lecture1.html"><a href="lecture1.html#terminology"><i class="fa fa-check"></i><b>1.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="lecture1.html"><a href="lecture1.html#recap-of-r"><i class="fa fa-check"></i><b>1.2</b> Recap of R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="lecture1.html"><a href="lecture1.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Installing R and Rstudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="lecture1.html"><a href="lecture1.html#r-community-and-packages"><i class="fa fa-check"></i><b>1.2.2</b> R community and packages</a></li>
<li class="chapter" data-level="1.2.3" data-path="lecture1.html"><a href="lecture1.html#datacamp"><i class="fa fa-check"></i><b>1.2.3</b> Datacamp</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="lecture1.html"><a href="lecture1.html#data-preprocessing"><i class="fa fa-check"></i><b>1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="1.4" data-path="lecture1.html"><a href="lecture1.html#case-study"><i class="fa fa-check"></i><b>1.4</b> Case study</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="lecture1.html"><a href="lecture1.html#data-transformations-for-individual-predictors"><i class="fa fa-check"></i><b>1.4.1</b> Data transformations for individual predictors</a></li>
<li class="chapter" data-level="1.4.2" data-path="lecture1.html"><a href="lecture1.html#centering-and-scaling"><i class="fa fa-check"></i><b>1.4.2</b> Centering and scaling</a></li>
<li class="chapter" data-level="1.4.3" data-path="lecture1.html"><a href="lecture1.html#tranformations-to-resolve-skewness"><i class="fa fa-check"></i><b>1.4.3</b> Tranformations to resolve skewness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>2</b> Over-fitting and model tuning, selection and evaluation and multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lecture2.html"><a href="lecture2.html#overfitting"><i class="fa fa-check"></i><b>2.1</b> Overfitting</a></li>
<li class="chapter" data-level="2.2" data-path="lecture2.html"><a href="lecture2.html#training-validation-and-test-split"><i class="fa fa-check"></i><b>2.2</b> Training, validation and test split</a></li>
<li class="chapter" data-level="2.3" data-path="lecture2.html"><a href="lecture2.html#multiple-regression"><i class="fa fa-check"></i><b>2.3</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lecture2.html"><a href="lecture2.html#example"><i class="fa fa-check"></i><b>2.3.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lecture3.html"><a href="lecture3.html#gam-example"><i class="fa fa-check"></i><b>3.1</b> GAM example</a>
<ul>
<li class="chapter" data-level="" data-path="lecture3.html"><a href="lecture3.html#data-camp"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>4</b> Classification methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lecture4.html"><a href="lecture4.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>4.1</b> k-nearest neighbor</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-1"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture4.html"><a href="lecture4.html#logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-2"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture4.html"><a href="lecture4.html#naive-bayes"><i class="fa fa-check"></i><b>4.3</b> Naive bayes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture4.html"><a href="lecture4.html#wrap-up"><i class="fa fa-check"></i><b>4.4</b> Wrap-up</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#data-camp-1"><i class="fa fa-check"></i>Data camp</a></li>
<li class="chapter" data-level="4.4.1" data-path="lecture4.html"><a href="lecture4.html#sources"><i class="fa fa-check"></i><b>4.4.1</b> Sources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>5</b> Decision Trees and Bagged Trees</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lecture5.html"><a href="lecture5.html#decision-trees"><i class="fa fa-check"></i><b>5.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.2" data-path="lecture5.html"><a href="lecture5.html#bagged-trees"><i class="fa fa-check"></i><b>5.2</b> Bagged trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture5.html"><a href="lecture5.html#data-camp-2"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture6.html"><a href="lecture6.html"><i class="fa fa-check"></i><b>6</b> Random forrest and boosting</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lecture6.html"><a href="lecture6.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.1</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="6.2" data-path="lecture6.html"><a href="lecture6.html#random-forest"><i class="fa fa-check"></i><b>6.2</b> Random Forest</a>
<ul>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture6.html"><a href="lecture6.html#boosted-trees"><i class="fa fa-check"></i><b>6.3</b> Boosted trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#iris-data"><i class="fa fa-check"></i>Iris data</a></li>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#data-camp-3"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture7.html"><a href="lecture7.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lecture7.html"><a href="lecture7.html#linear-svms"><i class="fa fa-check"></i><b>7.1</b> Linear SVMs</a></li>
<li class="chapter" data-level="7.2" data-path="lecture7.html"><a href="lecture7.html#polynomial-svm"><i class="fa fa-check"></i><b>7.2</b> Polynomial SVM</a>
<ul>
<li class="chapter" data-level="" data-path="lecture7.html"><a href="lecture7.html#data-camp-4"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture8.html"><a href="lecture8.html"><i class="fa fa-check"></i><b>8</b> Artificial Neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lecture8.html"><a href="lecture8.html#feed-forward-neural-networks"><i class="fa fa-check"></i><b>8.1</b> Feed Forward Neural Networks</a></li>
<li class="chapter" data-level="8.2" data-path="lecture8.html"><a href="lecture8.html#objective-functions-and-training"><i class="fa fa-check"></i><b>8.2</b> Objective functions and training</a></li>
<li class="chapter" data-level="8.3" data-path="lecture8.html"><a href="lecture8.html#validation-of-trained-models"><i class="fa fa-check"></i><b>8.3</b> Validation of trained models</a></li>
<li class="chapter" data-level="8.4" data-path="lecture8.html"><a href="lecture8.html#implementation-of-feed-forward-neural-network-in-keras"><i class="fa fa-check"></i><b>8.4</b> Implementation of Feed Forward Neural Network in Keras</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lecture8.html"><a href="lecture8.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>8.4.1</b> Installing Keras and Tensorflow</a></li>
<li class="chapter" data-level="8.4.2" data-path="lecture8.html"><a href="lecture8.html#importing-mnist"><i class="fa fa-check"></i><b>8.4.2</b> Importing MNIST</a></li>
<li class="chapter" data-level="8.4.3" data-path="lecture8.html"><a href="lecture8.html#pre-processing"><i class="fa fa-check"></i><b>8.4.3</b> Pre-processing</a></li>
<li class="chapter" data-level="8.4.4" data-path="lecture8.html"><a href="lecture8.html#feed-forward-neural-network-model"><i class="fa fa-check"></i><b>8.4.4</b> Feed Forward Neural Network model</a></li>
<li class="chapter" data-level="8.4.5" data-path="lecture8.html"><a href="lecture8.html#compiling-and-training-the-model"><i class="fa fa-check"></i><b>8.4.5</b> Compiling and training the model</a></li>
<li class="chapter" data-level="8.4.6" data-path="lecture8.html"><a href="lecture8.html#evaluating-the-model"><i class="fa fa-check"></i><b>8.4.6</b> Evaluating the model</a></li>
<li class="chapter" data-level="8.4.7" data-path="lecture8.html"><a href="lecture8.html#predicting-with-the-model"><i class="fa fa-check"></i><b>8.4.7</b> Predicting with the model</a></li>
<li class="chapter" data-level="" data-path="lecture8.html"><a href="lecture8.html#data-camp-5"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lecture9.html"><a href="lecture9.html"><i class="fa fa-check"></i><b>9</b> Feature selection</a>
<ul>
<li class="chapter" data-level="" data-path="lecture9.html"><a href="lecture9.html#backward-feature-selection"><i class="fa fa-check"></i>Backward feature selection</a></li>
<li class="chapter" data-level="" data-path="lecture9.html"><a href="lecture9.html#forward-feature-selection"><i class="fa fa-check"></i>Forward feature selection</a></li>
<li class="chapter" data-level="" data-path="lecture9.html"><a href="lecture9.html#recursive-feature-elimination"><i class="fa fa-check"></i>Recursive feature elimination</a></li>
<li class="chapter" data-level="9.1" data-path="lecture9.html"><a href="lecture9.html#example-5"><i class="fa fa-check"></i><b>9.1</b> Example</a>
<ul>
<li class="chapter" data-level="" data-path="lecture9.html"><a href="lecture9.html#data-camp-6"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data science with R: Applied Predictive Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture5" class="section level1" number="5">
<h1><span class="header-section-number"> 5</span> Decision Trees and Bagged Trees</h1>
<p>In this lecture we will learn about decision trees and bagged trees for classification or regression models. As in the previous lecture, we will use the tidymodels framework <span class="citation">(<a href="#ref-tidymodels" role="doc-biblioref">Kuhn and Wickham 2020</a>)</span>. If you have not installed <em>tidymodels</em>, this can be done by</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="lecture5.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tidymodels&quot;</span>)</span></code></pre></div>
<p>This lecture is highly inspired by the data camp course <em>Machine learning with three based models in R</em> (see link at the buttom).</p>
<p>We start with decision trees.</p>
<div id="decision-trees" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Decision trees</h2>
<p>A decision tree is a classifier that can be represented as a flow chart that looks like a tree. In each prediction, we start at the root asking a question deciding to which branch we should go for the prediction. As an example, say we want to predict whether or not a person likes computer games and make a decision tree for solving that problem. We have illustrated such a tree below. First you must ask: Is the person younger than 15 years old? If yes, predict that the person likes computer games. If the answer is no, is the person a boy? If yes, predict YES, he likes computer games, or if no, then NO, she does not like computer games.</p>
<div class="figure">
<img src="ppt/decisiontree_computergames.png" alt="A simple decision tree" width="1520" />
<p class="caption">
(#fig:decisiontree_computergames)A simple decision tree
</p>
</div>
<p>In the illustration we have tried to make the decision tree âgrow from the ground-up,â but it is most common to print them the other way around.</p>
<p>The pros of decision trees are:</p>
<ul>
<li>Easy to explain and intuitive to understand</li>
<li>Possible to capture non-linear relationships</li>
<li>Require no standardization or normalization of numeric features</li>
<li>No need for dummy variables for categoric features</li>
<li>Robust to outliers</li>
<li>Fast for large datasets</li>
<li>Can be used for regression and classification</li>
</ul>
<p>The cons are:</p>
<ul>
<li>Hard to interpret if large, deep, or ensembled</li>
<li>High variance, complex trees are prone to overfitting</li>
</ul>
<div style="padding:75% 0 0 0;position:relative;">
<iframe src="https://player.vimeo.com/video/702611150?h=a793c1f096&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;" title="L5_decTrees.mp4">
</iframe>
</div>
<script src="https://player.vimeo.com/api/player.js"></script>
<p>To set up a tidymodels decision tree, we start by loading the package</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="lecture5.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span></code></pre></div>
<p>To make a decision tree, there are three elements we need. First a decision tree object, then the engine and finally the mode.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="lecture5.html#cb82-1" aria-hidden="true" tabindex="-1"></a>tree_spec <span class="ot">&lt;-</span> <span class="fu">decision_tree</span>() <span class="sc">%&gt;%</span></span>
<span id="cb82-2"><a href="lecture5.html#cb82-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb82-3"><a href="lecture5.html#cb82-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p>Here <em>decision_tree()</em> makes the decision tree object. There are different packages we can use for decision trees, but we select a package called <em>rpart</em>, which is a package for recursive partitioning. This is called the engine in the tidymodels framework. Finally, we must decide if we are to do classification or regression, and this is the mode. Here we use classification.</p>
<p>The <em>tree_spec</em> object is just a skeleton, we need data to make it useful. For example purposes, let use the same data as in lecture 4 (the iris data).</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="lecture5.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb83-2"><a href="lecture5.html#cb83-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(iris)</span>
<span id="cb83-3"><a href="lecture5.html#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">999</span>)</span>
<span id="cb83-4"><a href="lecture5.html#cb83-4" aria-hidden="true" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(df, <span class="at">strata =</span> Species) </span>
<span id="cb83-5"><a href="lecture5.html#cb83-5" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb83-6"><a href="lecture5.html#cb83-6" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
<p>Then we are ready to fit a decition tree to our training data. You will notice the procedure is very similar to the procedure we used in lecture 4. This is the largest benefit of using tidymodels.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="lecture5.html#cb84-1" aria-hidden="true" tabindex="-1"></a>tree_fit <span class="ot">&lt;-</span> tree_spec <span class="sc">%&gt;%</span> </span>
<span id="cb84-2"><a href="lecture5.html#cb84-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(Species <span class="sc">~</span>., <span class="at">data =</span> df_train)</span>
<span id="cb84-3"><a href="lecture5.html#cb84-3" aria-hidden="true" tabindex="-1"></a>tree_fit</span></code></pre></div>
<pre><code>## parsnip model object
## 
## n= 111 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
## 1) root 111 74 setosa (0.33333333 0.33333333 0.33333333)  
##   2) Petal.Length&lt; 2.6 37  0 setosa (1.00000000 0.00000000 0.00000000) *
##   3) Petal.Length&gt;=2.6 74 37 versicolor (0.00000000 0.50000000 0.50000000)  
##     6) Petal.Width&lt; 1.75 39  3 versicolor (0.00000000 0.92307692 0.07692308) *
##     7) Petal.Width&gt;=1.75 35  1 virginica (0.00000000 0.02857143 0.97142857) *</code></pre>
<p>Using the <em>rpart.plot</em> package, we can visualize the decision tree we have fitted by</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="lecture5.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb86-2"><a href="lecture5.html#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_fit<span class="sc">$</span>fit,<span class="at">roundint=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/tidytree5-1.png" width="672" /></p>
<p>So, to make a prediction, the decision tree asks first: âIs the petal length below 2.6?â If yes, predict <strong>Setosa</strong>. If no: âIs the petal width below 1.75 (rounded off to 1.8 in the illustration)?â If the answer is yes, it is a <strong>vercicolor</strong> and a <strong>virginica</strong> if no.</p>
<p>We can also make predictions on the test set and calculate the accuracy</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="lecture5.html#cb87-1" aria-hidden="true" tabindex="-1"></a>tree_fit <span class="sc">%&gt;%</span> </span>
<span id="cb87-2"><a href="lecture5.html#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(df_test) <span class="sc">%&gt;%</span>      <span class="co"># predict on the test set</span></span>
<span id="cb87-3"><a href="lecture5.html#cb87-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_cols</span>(df_test) <span class="sc">%&gt;%</span>    <span class="co"># bind columns (adding the truth and covariates) </span></span>
<span id="cb87-4"><a href="lecture5.html#cb87-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">metrics</span>(<span class="at">truth =</span> Species, <span class="at">estimate =</span> .pred_class) <span class="co"># calculate metrics</span></span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.949
## 2 kap      multiclass     0.923</code></pre>
<p>and the confusion matrix</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="lecture5.html#cb89-1" aria-hidden="true" tabindex="-1"></a>tree_fit <span class="sc">%&gt;%</span> </span>
<span id="cb89-2"><a href="lecture5.html#cb89-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(df_test) <span class="sc">%&gt;%</span>      <span class="co"># predict on the test set</span></span>
<span id="cb89-3"><a href="lecture5.html#cb89-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_cols</span>(df_test) <span class="sc">%&gt;%</span>    <span class="co"># bind columns (adding the truth and covariates) </span></span>
<span id="cb89-4"><a href="lecture5.html#cb89-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">conf_mat</span>(<span class="at">truth =</span> Species, <span class="at">estimate =</span> .pred_class) <span class="co"># confusion matrix</span></span></code></pre></div>
<pre><code>##             Truth
## Prediction   setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         13         2
##   virginica       0          0        11</code></pre>
<p>If you remember from the last lecture, this is the same result as we got using knn and naive bayes.</p>
</div>
<div id="bagged-trees" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Bagged trees</h2>
<p>Bagged threes is an ensemble method. This means, instead of fitting just one decision tree, we fit many, and aggregate the predictions from all of them to improve our final prediction. Bagging is short for bootstrap aggregation. Bootstrap is a method for resampling that we use on the training set to get many training sets with the same properties as the original one. We sample random rows of the training set with replacement to make these bootstrapped datasets and fit a decision tree to each of these.</p>
<div class="figure">
<img src="ppt/bootstrap_ensemble.png" alt="A bagged tree" width="1983" />
<p class="caption">
(#fig:bagged_ensemble)A bagged tree
</p>
</div>
<p>This will give us many predictions that we then aggregate to arrive at our bagged prediction. If we are in a regression setting, the aggregated prediction is simply the mean, while in a classification setting the majority vote becomes the final prediction. Bagging can reduce the variance of our prediction significantly.</p>
<div class="figure">
<img src="ppt/esemble_prediction.png" alt="A bagged tree prediction" width="1648" />
<p class="caption">
(#fig:bagged_ensemble_pred)A bagged tree prediction
</p>
</div>
<div style="padding:56.25% 0 0 0;position:relative;">
<iframe src="https://player.vimeo.com/video/703581002?h=1aad939c3d&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;" title="STAT623: Bagged Trees">
</iframe>
</div>
<script src="https://player.vimeo.com/api/player.js"></script>
<p>Let us fit a bagged tree to the iris data. Here we need to install an additional package called <em>baguette</em>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="lecture5.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;baguette&quot;</span>)</span></code></pre></div>
<p>This package contains the function <strong>bag_tree</strong> that we will use in our model spec.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="lecture5.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(baguette)</span>
<span id="cb92-2"><a href="lecture5.html#cb92-2" aria-hidden="true" tabindex="-1"></a>bag_spec <span class="ot">&lt;-</span> <span class="fu">bag_tree</span>() <span class="sc">%&gt;%</span></span>
<span id="cb92-3"><a href="lecture5.html#cb92-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_engine</span>(<span class="st">&quot;rpart&quot;</span>, <span class="at">times =</span> <span class="dv">100</span>) <span class="sc">%&gt;%</span></span>
<span id="cb92-4"><a href="lecture5.html#cb92-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_mode</span>(<span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p>As you have learned by now, the structure is the same, but notice in the engine specification we have set <em>times = 100</em>. This is how we specify how many trees we want to include in our bag.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="lecture5.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb93-2"><a href="lecture5.html#cb93-2" aria-hidden="true" tabindex="-1"></a>bag_fit <span class="ot">&lt;-</span> bag_spec <span class="sc">%&gt;%</span> </span>
<span id="cb93-3"><a href="lecture5.html#cb93-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fit</span>(Species <span class="sc">~</span>., <span class="at">data =</span> df_train)</span>
<span id="cb93-4"><a href="lecture5.html#cb93-4" aria-hidden="true" tabindex="-1"></a>bag_fit</span></code></pre></div>
<pre><code>## parsnip model object
## 
## Bagged CART (classification with 100 members)
## 
## Variable importance scores include:
## 
## # A tibble: 4 x 4
##   term         value std.error  used
##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;
## 1 Petal.Length  66.8     0.320   100
## 2 Petal.Width   66.7     0.277   100
## 3 Sepal.Length  48.2     0.489   100
## 4 Sepal.Width   27.9     0.487   100</code></pre>
<p>In the output, we see the variable importance scores. As you can see the <em>Petal.Length</em> is the most important covariate.</p>
<p>We can also make predictions on the test set and calculate the accuracy</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="lecture5.html#cb95-1" aria-hidden="true" tabindex="-1"></a>bag_fit <span class="sc">%&gt;%</span> </span>
<span id="cb95-2"><a href="lecture5.html#cb95-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(df_test) <span class="sc">%&gt;%</span>    </span>
<span id="cb95-3"><a href="lecture5.html#cb95-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_cols</span>(df_test) <span class="sc">%&gt;%</span>    </span>
<span id="cb95-4"><a href="lecture5.html#cb95-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">metrics</span>(<span class="at">truth =</span> Species, <span class="at">estimate =</span> .pred_class) </span></code></pre></div>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.949
## 2 kap      multiclass     0.923</code></pre>
<p>and the confusion matrix</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="lecture5.html#cb97-1" aria-hidden="true" tabindex="-1"></a>bag_fit <span class="sc">%&gt;%</span> </span>
<span id="cb97-2"><a href="lecture5.html#cb97-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">predict</span>(df_test) <span class="sc">%&gt;%</span>     </span>
<span id="cb97-3"><a href="lecture5.html#cb97-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_cols</span>(df_test) <span class="sc">%&gt;%</span>    </span>
<span id="cb97-4"><a href="lecture5.html#cb97-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">conf_mat</span>(<span class="at">truth =</span> Species, <span class="at">estimate =</span> .pred_class) </span></code></pre></div>
<pre><code>##             Truth
## Prediction   setosa versicolor virginica
##   setosa         13          0         0
##   versicolor      0         13         2
##   virginica       0          0        11</code></pre>
<p>It seems this is the best we can do with the data that we have - same result as for knn, naive bayes and a simple decision tree.</p>
<p>On data camp you will learn more about decision trees and bagged trees - also how to use them in a regression setting.</p>
<div id="data-camp-2" class="section level3 unnumbered">
<h3>Data camp</h3>
<p>We highly recommend the data camp course <a href="https://app.datacamp.com/learn/courses/machine-learning-with-tree-based-models-in-r">Machine Learning with Tree-Based Models in R</a> chapters 1-3.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-tidymodels" class="csl-entry">
Kuhn, Max, and Hadley Wickham. 2020. <em>Tidymodels: A Collection of Packages for Modeling and Machine Learning Using Tidyverse Principles.</em> <a href="https://www.tidymodels.org">https://www.tidymodels.org</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STAT623 compendium.pdf", "STAT623 compendium.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
