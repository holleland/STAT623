<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 7 Support vector machines | Data science with R: Applied Predictive Modelling</title>
  <meta name="description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content=" 7 Support vector machines | Data science with R: Applied Predictive Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 7 Support vector machines | Data science with R: Applied Predictive Modelling" />
  
  <meta name="twitter:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

<meta name="author" content="Sondre HÃ¸lleland and Kristian Gundersen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture6.html"/>
<link rel="next" href="lecture8.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">STAT623 Applied predictive modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes-and-objectives"><i class="fa fa-check"></i>Learning outcomes and objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecture-overview"><i class="fa fa-check"></i>Lecture overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#litterature"><i class="fa fa-check"></i>Litterature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>1</b> Introduction and short recap of R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="lecture1.html"><a href="lecture1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="lecture1.html"><a href="lecture1.html#prediction-or-interpretation"><i class="fa fa-check"></i><b>1.1.1</b> Prediction or interpretation?</a></li>
<li class="chapter" data-level="1.1.2" data-path="lecture1.html"><a href="lecture1.html#terminology"><i class="fa fa-check"></i><b>1.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="lecture1.html"><a href="lecture1.html#recap-of-r"><i class="fa fa-check"></i><b>1.2</b> Recap of R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="lecture1.html"><a href="lecture1.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Installing R and Rstudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="lecture1.html"><a href="lecture1.html#r-community-and-packages"><i class="fa fa-check"></i><b>1.2.2</b> R community and packages</a></li>
<li class="chapter" data-level="1.2.3" data-path="lecture1.html"><a href="lecture1.html#datacamp"><i class="fa fa-check"></i><b>1.2.3</b> Datacamp</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="lecture1.html"><a href="lecture1.html#data-preprocessing"><i class="fa fa-check"></i><b>1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="1.4" data-path="lecture1.html"><a href="lecture1.html#case-study"><i class="fa fa-check"></i><b>1.4</b> Case study</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="lecture1.html"><a href="lecture1.html#data-transformations-for-individual-predictors"><i class="fa fa-check"></i><b>1.4.1</b> Data transformations for individual predictors</a></li>
<li class="chapter" data-level="1.4.2" data-path="lecture1.html"><a href="lecture1.html#centering-and-scaling"><i class="fa fa-check"></i><b>1.4.2</b> Centering and scaling</a></li>
<li class="chapter" data-level="1.4.3" data-path="lecture1.html"><a href="lecture1.html#tranformations-to-resolve-skewness"><i class="fa fa-check"></i><b>1.4.3</b> Tranformations to resolve skewness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>2</b> Over-fitting and model tuning, selection and evaluation and multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lecture2.html"><a href="lecture2.html#overfitting"><i class="fa fa-check"></i><b>2.1</b> Overfitting</a></li>
<li class="chapter" data-level="2.2" data-path="lecture2.html"><a href="lecture2.html#training-validation-and-test-split"><i class="fa fa-check"></i><b>2.2</b> Training, validation and test split</a></li>
<li class="chapter" data-level="2.3" data-path="lecture2.html"><a href="lecture2.html#multiple-regression"><i class="fa fa-check"></i><b>2.3</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lecture2.html"><a href="lecture2.html#example"><i class="fa fa-check"></i><b>2.3.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lecture3.html"><a href="lecture3.html#gam-example"><i class="fa fa-check"></i><b>3.1</b> GAM example</a>
<ul>
<li class="chapter" data-level="" data-path="lecture3.html"><a href="lecture3.html#data-camp"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>4</b> Classification methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lecture4.html"><a href="lecture4.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>4.1</b> k-nearest neighbor</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-1"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture4.html"><a href="lecture4.html#logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-2"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture4.html"><a href="lecture4.html#naive-bayes"><i class="fa fa-check"></i><b>4.3</b> Naive bayes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture4.html"><a href="lecture4.html#wrap-up"><i class="fa fa-check"></i><b>4.4</b> Wrap-up</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#data-camp-1"><i class="fa fa-check"></i>Data camp</a></li>
<li class="chapter" data-level="4.4.1" data-path="lecture4.html"><a href="lecture4.html#sources"><i class="fa fa-check"></i><b>4.4.1</b> Sources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>5</b> Decision Trees and Bagged Trees</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lecture5.html"><a href="lecture5.html#decision-trees"><i class="fa fa-check"></i><b>5.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.2" data-path="lecture5.html"><a href="lecture5.html#bagged-trees"><i class="fa fa-check"></i><b>5.2</b> Bagged trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture5.html"><a href="lecture5.html#data-camp-2"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture6.html"><a href="lecture6.html"><i class="fa fa-check"></i><b>6</b> Random forrest and boosting</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lecture6.html"><a href="lecture6.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.1</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="6.2" data-path="lecture6.html"><a href="lecture6.html#random-forest"><i class="fa fa-check"></i><b>6.2</b> Random Forest</a>
<ul>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture6.html"><a href="lecture6.html#boosted-trees"><i class="fa fa-check"></i><b>6.3</b> Boosted trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#iris-data"><i class="fa fa-check"></i>Iris data</a></li>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#data-camp-3"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture7.html"><a href="lecture7.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lecture7.html"><a href="lecture7.html#linear-svms"><i class="fa fa-check"></i><b>7.1</b> Linear SVMs</a></li>
<li class="chapter" data-level="" data-path="lecture7.html"><a href="lecture7.html#polynomial-svm"><i class="fa fa-check"></i>Polynomial SVM</a>
<ul>
<li class="chapter" data-level="" data-path="lecture7.html"><a href="lecture7.html#data-camp-4"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture8.html"><a href="lecture8.html"><i class="fa fa-check"></i><b>8</b> Artificial Neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lecture8.html"><a href="lecture8.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>8.1</b> Multilayer Perceptron (MLP)</a></li>
<li class="chapter" data-level="8.2" data-path="lecture8.html"><a href="lecture8.html#objective-functions-and-training"><i class="fa fa-check"></i><b>8.2</b> Objective functions and training</a></li>
<li class="chapter" data-level="8.3" data-path="lecture8.html"><a href="lecture8.html#validation-of-trained-models"><i class="fa fa-check"></i><b>8.3</b> Validation of trained models</a></li>
<li class="chapter" data-level="8.4" data-path="lecture8.html"><a href="lecture8.html#implementation-of-mlp-in-keras"><i class="fa fa-check"></i><b>8.4</b> Implementation of MLP in Keras</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lecture8.html"><a href="lecture8.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>8.4.1</b> Installing Keras and Tensorflow</a></li>
<li class="chapter" data-level="8.4.2" data-path="lecture8.html"><a href="lecture8.html#importing-mnist"><i class="fa fa-check"></i><b>8.4.2</b> Importing MNIST</a></li>
<li class="chapter" data-level="8.4.3" data-path="lecture8.html"><a href="lecture8.html#preprocessing"><i class="fa fa-check"></i><b>8.4.3</b> Preprocessing</a></li>
<li class="chapter" data-level="8.4.4" data-path="lecture8.html"><a href="lecture8.html#mlp-model"><i class="fa fa-check"></i><b>8.4.4</b> MLP model</a></li>
<li class="chapter" data-level="8.4.5" data-path="lecture8.html"><a href="lecture8.html#compiling-and-training-the-model"><i class="fa fa-check"></i><b>8.4.5</b> Compiling and training the model</a></li>
<li class="chapter" data-level="8.4.6" data-path="lecture8.html"><a href="lecture8.html#evaluating-the-model"><i class="fa fa-check"></i><b>8.4.6</b> Evaluating the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lecture9.html"><a href="lecture9.html"><i class="fa fa-check"></i><b>9</b> Feature selection/Explainable AI</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data science with R: Applied Predictive Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture7" class="section level1" number="7">
<h1><span class="header-section-number"> 7</span> Support vector machines</h1>
<p>Support vector machines is, similarly to the tree based models we have looked at already, a supervised learning method for classification. They can also be used in regression settings, but my impression is that they are mostly applied for classification in cases with continuous predictors. The reason for this is that with a support vector machine (often abbreviated svm) the outcome is a (continuous) decision boundary splitting the p-dimensional predictor space into regions where the y has different classes. We will only cover the case with two possible outcomes for y here and also only two predictors, x1 and x2. We will also only use simulated data. We will not go into the mathematical details of how the algorithm works, but focus on applications and how it may look like for a user to use svmâs. The lecture is highly inspired by the data camp course <em>Support Vector Machines in R</em> (see link at the buttom) and the main R package for this lecture implementing all the svm related functions is called <em>e1071</em> <span class="citation">(<a href="#ref-e1071" role="doc-biblioref">Meyer et al. 2021</a>)</span>. For this lecture we have not made a video.</p>
<p>Say you have a data frame with two numerical explanatory variables, <em>x1</em> and <em>x2</em>, and a response variable, <em>y</em>, that takes the values -1 and 1. You want to make a model for predicting the outcome of y given x1 and x2. Since <em>y</em> only takes two values and x1 and x2 are continuous numerical predictors, a classical decision tree would find some threshold for x1 and x2 and say that all x1 &gt; c1 is classified as Y=-1, for instance. A support vector machine will find a continuous line that splits x1 and x2 into regions where Y=-1 and y=1. In our first example the separating line is linear and our svm will be using a linear kernel, but there are other options also (in the <em>e1071</em> package: polynomial, radial basis, sigmoid).</p>
<p>Situations with two predictors are most interesting from a pedagogical perspective, because it is easy to visualize the separating line. In three dimensions, it will be a separating plane, while in one dimension it will only be a threshold (much like a simple decision three).</p>
<div id="linear-svms" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Linear SVMs</h2>
<p>We start illustrating how it works with a simple example. The first 6 rows of the data frame looks like this:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="lecture7.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##       x1     x2 y    
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;
## 1 0.288  0.354  -1   
## 2 0.788  0.366  1    
## 3 0.409  0.287  1    
## 4 0.883  0.0800 1    
## 5 0.940  0.365  1    
## 6 0.0456 0.178  -1</code></pre>
<p>Let us do a train-test split. The following code should be very familiar by now.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="lecture7.html#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb143-2"><a href="lecture7.html#cb143-2" aria-hidden="true" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(df, <span class="at">prop =</span> .<span class="dv">8</span>, <span class="at">strata =</span> y)</span>
<span id="cb143-3"><a href="lecture7.html#cb143-3" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> <span class="fu">training</span>(split)</span>
<span id="cb143-4"><a href="lecture7.html#cb143-4" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> <span class="fu">testing</span>(split)</span></code></pre></div>
<p>We can plot the training data with <em>x1</em> on the x axis and <em>x2</em> on the y axis and color the dots by the categorical y response variable.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="lecture7.html#cb144-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df_train, <span class="fu">aes</span>(<span class="at">x=</span>x1,<span class="at">y=</span>x2,<span class="at">col =</span> y)) <span class="sc">+</span></span>
<span id="cb144-2"><a href="lecture7.html#cb144-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb144-3"><a href="lecture7.html#cb144-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>))</span>
<span id="cb144-4"><a href="lecture7.html#cb144-4" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Here it is clear that a linear split will fit perfectly from just looking at the figure. But let us use the <em>e1071</em> package and fit an svm for this problem and see the line. The different arguments in the function call below are quite self-explanatory, but we specify the formula and the data first. Then we specify that it is classification we want to do and that the kernel should be linear. This means we want the curve that divides the data set to be a straight line. The scale argument is set to false, as we do not want to scale the predictors nor response variables here. If scale is true, the algorithm will standardize both the x and y variables.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="lecture7.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb145-2"><a href="lecture7.html#cb145-2" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb145-3"><a href="lecture7.html#cb145-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train,</span>
<span id="cb145-4"><a href="lecture7.html#cb145-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb145-5"><a href="lecture7.html#cb145-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>,</span>
<span id="cb145-6"><a href="lecture7.html#cb145-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb145-7"><a href="lecture7.html#cb145-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(svm_fit)</span></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  95
## 
##  ( 48 47 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1</code></pre>
<p>There is not a lot of information here, but note especially the number of support vectors (95). We will come back to this later.</p>
<p>Now, let us plot the points again and highlight the support vectors from the model fit.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="lecture7.html#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract support vectors</span></span>
<span id="cb147-2"><a href="lecture7.html#cb147-2" aria-hidden="true" tabindex="-1"></a>df_sv <span class="ot">&lt;-</span> df_train[ svm_fit<span class="sc">$</span>index, ]</span>
<span id="cb147-3"><a href="lecture7.html#cb147-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-4"><a href="lecture7.html#cb147-4" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span></span>
<span id="cb147-5"><a href="lecture7.html#cb147-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> df_sv, <span class="at">color =</span> <span class="st">&quot;green&quot;</span>, <span class="at">alpha =</span> .<span class="dv">2</span>, <span class="at">size =</span> <span class="dv">4</span>) </span>
<span id="cb147-6"><a href="lecture7.html#cb147-6" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>As you can see, the support vectors are clustered around the decision boundary. One could say that they support the decision boundary, hence the name. In general, the support vectors are data points that are closer to the hyperplane and influence the postion and orientation of the hyperplane. The idea is to use the support vectors to find the hyperplane that maximizes the margin of the classifier. By looking at the figure you can imagine drawing many straight lines that will split the blue and purple dots, but by maximizing the margin we get closer to a unique solution.</p>
<p>Next we will add in the decision boundary and margins. It is not as straight forward as one might imagine to extract the intercept and slope from the fitted object. The reason for this is likely that the implementation is more general than a linear kernel in two dimensions. Still we can derive this information from the information in the fitted object.</p>
<p>First we must build the weight vector, <span class="math inline">\(w\)</span>, from the coefficients and the support vectors matrix.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="lecture7.html#cb148-1" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_fit<span class="sc">$</span>SV</span>
<span id="cb148-2"><a href="lecture7.html#cb148-2" aria-hidden="true" tabindex="-1"></a>w</span></code></pre></div>
<pre><code>##             x1      x2
## [1,] -5.180669 5.60003</code></pre>
<p>The slope is then given by the negative ratio between the first and second weight</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="lecture7.html#cb150-1" aria-hidden="true" tabindex="-1"></a>svm_slope <span class="ot">&lt;-</span> <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>]</span>
<span id="cb150-2"><a href="lecture7.html#cb150-2" aria-hidden="true" tabindex="-1"></a>svm_slope</span></code></pre></div>
<pre><code>## [1] 0.9251146</code></pre>
<p>and the intercept is found by the ratio between the rho element and the second weight</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="lecture7.html#cb152-1" aria-hidden="true" tabindex="-1"></a>svm_intercept <span class="ot">&lt;-</span> svm_fit<span class="sc">$</span>rho <span class="sc">/</span> w[<span class="dv">2</span>]</span>
<span id="cb152-2"><a href="lecture7.html#cb152-2" aria-hidden="true" tabindex="-1"></a>svm_intercept</span></code></pre></div>
<pre><code>## [1] -0.02130264</code></pre>
<p>Let us add it to our figure</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="lecture7.html#cb154-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span></span>
<span id="cb154-2"><a href="lecture7.html#cb154-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> svm_slope, <span class="at">intercept =</span> svm_intercept, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb154-3"><a href="lecture7.html#cb154-3" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The margins are parallel to the decision boundary with an offset of <em>1/w[2]</em>.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="lecture7.html#cb155-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> p <span class="sc">+</span></span>
<span id="cb155-2"><a href="lecture7.html#cb155-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> svm_slope, <span class="at">intercept =</span> svm_intercept<span class="dv">-1</span><span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>)<span class="sc">+</span></span>
<span id="cb155-3"><a href="lecture7.html#cb155-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> svm_slope, <span class="at">intercept =</span> svm_intercept<span class="sc">+</span><span class="dv">1</span><span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) </span>
<span id="cb155-4"><a href="lecture7.html#cb155-4" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Note that the decision boundary is supported by roughly the same number of support vectors on either side. In this example we do not have any instances violating the boundary, so our accuracy on the test set will be perfect (100%), but let us test it anyway.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="lecture7.html#cb156-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="sc">%&gt;%</span> <span class="fu">predict</span>(df_test) <span class="sc">%&gt;%</span></span>
<span id="cb156-2"><a href="lecture7.html#cb156-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(df_test) <span class="sc">%&gt;%</span></span>
<span id="cb156-3"><a href="lecture7.html#cb156-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">conf_mat</span>(<span class="at">truth =</span> <span class="st">&quot;y&quot;</span>, <span class="at">estimate =</span> <span class="st">&quot;...1&quot;</span>) <span class="co"># default name of new column is &quot;...1&quot;</span></span></code></pre></div>
<pre><code>## New names:
## * `` -&gt; ...1</code></pre>
<pre><code>##           Truth
## Prediction -1  1
##         -1 51  0
##         1   0 40</code></pre>
<p>As we can see, there is no need to calculate the accuracy as all predictions are correct it will be 1.</p>
<p>There is also a built-in plotting function for svmâs in the e1071 package. It highlights the support vectors by xâes and observations by oâs, and separate the decision boundary by the different colored areas. It takes the fitted object and the data set you want to predict on. Here we use the training set, but you can also use the test set.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="lecture7.html#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_fit, <span class="at">data =</span> df_train)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Note that here x1 is on the y-axis and x2 on the x-axis (opposite of what we did in ggplot above).</p>
<p>Now, we will play a little bit with the hyperparameters. Remember that we had 95 support vectors in our first attempt. Now we will increase the cost hyperparameter, i.e.Â the cost of constraint violations.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="lecture7.html#cb160-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb160-2"><a href="lecture7.html#cb160-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train,</span>
<span id="cb160-3"><a href="lecture7.html#cb160-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb160-4"><a href="lecture7.html#cb160-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>,</span>
<span id="cb160-5"><a href="lecture7.html#cb160-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb160-6"><a href="lecture7.html#cb160-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">cost  =</span> <span class="dv">100</span>)</span>
<span id="cb160-7"><a href="lecture7.html#cb160-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(svm_fit)</span></code></pre></div>
<pre><code>## 
## Call:
## svm(formula = y ~ ., data = df_train, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, cost = 100, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  100 
## 
## Number of Support Vectors:  7
## 
##  ( 3 4 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  -1 1</code></pre>
<p>As you can see from the model output, the number of support vectors has drastically reduced from 95 to 7. We can also look at the equivalent plot (code as above is not included here).
<img src="STAT623-compendium_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Here you see that the margin is much narrower. Increasing the cost like this for linear support vector machines can be useful in situations where the decision boundary is known to be linear. We will now consider a situation where the decision boundary is non-linear - in fact a circle.</p>
</div>
<div id="polynomial-svm" class="section level2 unnumbered">
<h2>Polynomial SVM</h2>
<p>To generate circles for plotting, we introduce the function <em>circle</em>. This creates a tibble data frame with <em>npoints</em> rows containing x1 and x2 coordinates for a circle of radius r.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="lecture7.html#cb162-1" aria-hidden="true" tabindex="-1"></a>circle <span class="ot">&lt;-</span> <span class="cf">function</span>(x1_center, x2_center, r, <span class="at">npoints =</span> <span class="dv">100</span>){</span>
<span id="cb162-2"><a href="lecture7.html#cb162-2" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="at">length.out=</span>npoints)</span>
<span id="cb162-3"><a href="lecture7.html#cb162-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb162-4"><a href="lecture7.html#cb162-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x1c =</span> x1_center <span class="sc">+</span> r<span class="sc">*</span><span class="fu">cos</span>(theta),</span>
<span id="cb162-5"><a href="lecture7.html#cb162-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">x2c =</span> x2_center <span class="sc">+</span> r<span class="sc">*</span><span class="fu">sin</span>(theta)</span>
<span id="cb162-6"><a href="lecture7.html#cb162-6" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb162-7"><a href="lecture7.html#cb162-7" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We generate the data and plot it with the decision boundary being a circle of radius 0.7.</p>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Let us first try to fit a linear svm to these data. We expect it not to perform very well, but let us see how bad it will be. For a quick check we just plot the result using the plot function.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="lecture7.html#cb163-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb163-2"><a href="lecture7.html#cb163-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train,</span>
<span id="cb163-3"><a href="lecture7.html#cb163-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb163-4"><a href="lecture7.html#cb163-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>,</span>
<span id="cb163-5"><a href="lecture7.html#cb163-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb163-6"><a href="lecture7.html#cb163-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">cost  =</span> <span class="dv">1</span>)</span>
<span id="cb163-7"><a href="lecture7.html#cb163-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_fit, <span class="at">data =</span> df_train)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>As expected, the linear svm performs bad and predicts all observations to belong to the class <span class="math inline">\(y=1\)</span>. The number of support vectors (114) is also very high, which is a sign of poor performance. We can try to increase the cost, as we did before.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="lecture7.html#cb164-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb164-2"><a href="lecture7.html#cb164-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train,</span>
<span id="cb164-3"><a href="lecture7.html#cb164-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb164-4"><a href="lecture7.html#cb164-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>,</span>
<span id="cb164-5"><a href="lecture7.html#cb164-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb164-6"><a href="lecture7.html#cb164-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">cost  =</span> <span class="dv">100</span>)</span>
<span id="cb164-7"><a href="lecture7.html#cb164-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_fit, <span class="at">data =</span> df_train)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>More or less the same result. Increasing the cost reduced the number of support vectors to 119.</p>
<p>The solution to this problem is to use another kernel than the linear one, but before we to that, we will use the transformation trick. As is often the case, a smart transformation can often take a non-linear problem and make it linear. In this case, we know the boundary is a circle and the formula for a circle is
<span class="math display">\[ x_1^2 + x_2^2  = r^2, \]</span>
where <span class="math inline">\(r\)</span> is the radius. If we transform the data, by squaring the observations, say
<span class="math display">\[z_1 = x_1^2 \quad\text{and}\quad z_2=x_2^2,\]</span>
we get that <span class="math display">\[x_1^2+x_2^2 = z_1+z_2 = r^2 \quad\Rightarrow \quad z_2 = r^2-z_1.\]</span>
That is the relationship between <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> is linear with a slope of <span class="math inline">\(-1\)</span> and an intercept of <span class="math inline">\(r^2\)</span>. Since we know the true radius, we can plot this directly:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="lecture7.html#cb165-1" aria-hidden="true" tabindex="-1"></a>df_train2 <span class="ot">&lt;-</span> df_train <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">z1=</span>x1<span class="sc">^</span><span class="dv">2</span>, <span class="at">z2 =</span> x2<span class="sc">^</span><span class="dv">2</span>) <span class="sc">%&gt;%</span> <span class="fu">select</span>(z1,z2,y)</span>
<span id="cb165-2"><a href="lecture7.html#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df_train2, <span class="fu">aes</span>(<span class="at">x=</span>z1,<span class="at">y=</span>z2,<span class="at">col=</span>y)) <span class="sc">+</span> </span>
<span id="cb165-3"><a href="lecture7.html#cb165-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb165-4"><a href="lecture7.html#cb165-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">intercept =</span> radius<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Let us fit a linear svm to the transformed data.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="lecture7.html#cb166-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb166-2"><a href="lecture7.html#cb166-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train2,</span>
<span id="cb166-3"><a href="lecture7.html#cb166-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb166-4"><a href="lecture7.html#cb166-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;linear&quot;</span>,</span>
<span id="cb166-5"><a href="lecture7.html#cb166-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb166-6"><a href="lecture7.html#cb166-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">cost  =</span> <span class="dv">100</span>)</span>
<span id="cb166-7"><a href="lecture7.html#cb166-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_fit, <span class="at">data =</span> df_train2)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Clearly, this is more satisfying.</p>
<p>The next step we will test is to use the original data without transforming, but using a different kernel. Instead of squaring the data, we will use a polynomial kernel of 2nd degree. Using the polynomial kernel option, there are three hyperparameters: <em>degree</em>, <em>gamma</em> and <em>coef0</em>. The polynomial kernel is parameterized as
<span class="math inline">\((\text{gamma*u&#39;*v}+ \text{coef0})\text{^degree}\)</span>, where <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are data vectors. We set the kernel argument to âpolynomialâ and degree to 2, and for now we use standard options for gamma and coef0.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="lecture7.html#cb167-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb167-2"><a href="lecture7.html#cb167-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train,</span>
<span id="cb167-3"><a href="lecture7.html#cb167-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb167-4"><a href="lecture7.html#cb167-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;polynomial&quot;</span>,</span>
<span id="cb167-5"><a href="lecture7.html#cb167-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">degree =</span> <span class="dv">2</span>,</span>
<span id="cb167-6"><a href="lecture7.html#cb167-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb167-7"><a href="lecture7.html#cb167-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">cost  =</span> <span class="dv">1</span>)</span>
<span id="cb167-8"><a href="lecture7.html#cb167-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_fit, <span class="at">data =</span> df_train)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="lecture7.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(df_train<span class="sc">$</span>y <span class="sc">==</span> <span class="fu">predict</span>(svm_fit, df_train)) <span class="co">#Accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.9874214</code></pre>
<p>The accuracy is certainly very high (98.7%), but for the sake of learning, we will try to improve this further by tuning the hyperparameters instead of using the defaults. We do this by using the <em>tune.svm</em> function in <em>e1071</em>. The syntax is very similar to the svm function, but we need to feed in the xâs and yâs separately as below and also feed in a vector for each of the hyperparaemters with the values we want to consider. We use costs of 0.1,1,10 and 100, gamma values of 0.1,1 and 10 and coef0 of 0,0.1,1 and 10. The tuning can take some time, but on this example it is quite fast.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="lecture7.html#cb170-1" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(df_train)</span>
<span id="cb170-2"><a href="lecture7.html#cb170-2" aria-hidden="true" tabindex="-1"></a>tuning <span class="ot">&lt;-</span> <span class="fu">tune.svm</span>(</span>
<span id="cb170-3"><a href="lecture7.html#cb170-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> df_train[,<span class="sc">-</span><span class="dv">3</span>], </span>
<span id="cb170-4"><a href="lecture7.html#cb170-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> df_train[,<span class="dv">3</span>],</span>
<span id="cb170-5"><a href="lecture7.html#cb170-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb170-6"><a href="lecture7.html#cb170-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">&quot;polynomial&quot;</span>,</span>
<span id="cb170-7"><a href="lecture7.html#cb170-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">degree =</span> <span class="dv">2</span>,</span>
<span id="cb170-8"><a href="lecture7.html#cb170-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#Suggestions for the hyperparameters:</span></span>
<span id="cb170-9"><a href="lecture7.html#cb170-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost =</span> <span class="dv">10</span><span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span> <span class="sc">:</span> <span class="dv">2</span>),</span>
<span id="cb170-10"><a href="lecture7.html#cb170-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>),</span>
<span id="cb170-11"><a href="lecture7.html#cb170-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">coef0 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.1</span>,<span class="dv">1</span>,<span class="dv">10</span>)</span>
<span id="cb170-12"><a href="lecture7.html#cb170-12" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb170-13"><a href="lecture7.html#cb170-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the &quot;best&quot; parametrs:</span></span>
<span id="cb170-14"><a href="lecture7.html#cb170-14" aria-hidden="true" tabindex="-1"></a>tuning<span class="sc">$</span>best.parameters</span></code></pre></div>
<pre><code>##   degree gamma coef0 cost
## 6      2    10   0.1  0.1</code></pre>
<p>We fit the model using the best parameters from the tuning:</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="lecture7.html#cb172-1" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., </span>
<span id="cb172-2"><a href="lecture7.html#cb172-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> df_train,</span>
<span id="cb172-3"><a href="lecture7.html#cb172-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">type =</span> <span class="st">&quot;C-classification&quot;</span>,</span>
<span id="cb172-4"><a href="lecture7.html#cb172-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">&quot;polynomial&quot;</span>,</span>
<span id="cb172-5"><a href="lecture7.html#cb172-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">degree =</span> <span class="dv">2</span>,</span>
<span id="cb172-6"><a href="lecture7.html#cb172-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb172-7"><a href="lecture7.html#cb172-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">cost  =</span> tuning<span class="sc">$</span>best.parameters<span class="sc">$</span>cost,</span>
<span id="cb172-8"><a href="lecture7.html#cb172-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">gamma =</span> tuning<span class="sc">$</span>best.parameters<span class="sc">$</span>gamma, </span>
<span id="cb172-9"><a href="lecture7.html#cb172-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">coef0 =</span> tuning<span class="sc">$</span>best.parameters<span class="sc">$</span>coef0)</span>
<span id="cb172-10"><a href="lecture7.html#cb172-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_fit, <span class="at">data =</span> df_train)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="lecture7.html#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(df_train<span class="sc">$</span>y <span class="sc">==</span> <span class="fu">predict</span>(svm_fit, df_train)) <span class="co">#Accuracy</span></span></code></pre></div>
<pre><code>## [1] 0.9937107</code></pre>
<p>The tuning resulted in a slight improvement of roughly half a percent of accuracy going from 98.7% to 99.4%, but we are already very close to 100%.</p>
<p>On the data camp course recommended below you will learn more about this and in chapter 4 you will also go a bit further than we have here.</p>
<div id="data-camp-4" class="section level3 unnumbered">
<h3>Data camp</h3>
<p>We highly recommend the data camp course <a href="https://app.datacamp.com/learn/courses/support-vector-machines-in-r">Support Vector Machines in R</a> chapters 1-4.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-e1071" class="csl-entry">
Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2021. <em>E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STAT623 compendium.pdf", "STAT623 compendium.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
