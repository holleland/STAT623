<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 7 Artificial Neural networks | Data science with R: Applied Predictive Modelling</title>
  <meta name="description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content=" 7 Artificial Neural networks | Data science with R: Applied Predictive Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 7 Artificial Neural networks | Data science with R: Applied Predictive Modelling" />
  
  <meta name="twitter:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

<meta name="author" content="Sondre Hølleland" />


<meta name="date" content="2022-03-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture6.html"/>
<link rel="next" href="lecture8.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">STAT623 Applied predictive modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecture-overview"><i class="fa fa-check"></i>Lecture overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#litterature"><i class="fa fa-check"></i>Litterature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>1</b> Introduction and short recap of R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="lecture1.html"><a href="lecture1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="lecture1.html"><a href="lecture1.html#prediction-or-interpretation"><i class="fa fa-check"></i><b>1.1.1</b> Prediction or interpretation?</a></li>
<li class="chapter" data-level="1.1.2" data-path="lecture1.html"><a href="lecture1.html#terminology"><i class="fa fa-check"></i><b>1.1.2</b> Terminology</a></li>
<li class="chapter" data-level="1.1.3" data-path="lecture1.html"><a href="lecture1.html#overview"><i class="fa fa-check"></i><b>1.1.3</b> Overview</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="lecture1.html"><a href="lecture1.html#recap-of-r"><i class="fa fa-check"></i><b>1.2</b> Recap of R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="lecture1.html"><a href="lecture1.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Installing R and Rstudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="lecture1.html"><a href="lecture1.html#r-community-and-packages"><i class="fa fa-check"></i><b>1.2.2</b> R community and packages</a></li>
<li class="chapter" data-level="1.2.3" data-path="lecture1.html"><a href="lecture1.html#datacamp"><i class="fa fa-check"></i><b>1.2.3</b> Datacamp</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="lecture1.html"><a href="lecture1.html#data-preprocessing"><i class="fa fa-check"></i><b>1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="1.4" data-path="lecture1.html"><a href="lecture1.html#case-study"><i class="fa fa-check"></i><b>1.4</b> Case study</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="lecture1.html"><a href="lecture1.html#data-transformations-for-individual-predictors"><i class="fa fa-check"></i><b>1.4.1</b> Data transformations for individual predictors</a></li>
<li class="chapter" data-level="1.4.2" data-path="lecture1.html"><a href="lecture1.html#centering-and-scaling"><i class="fa fa-check"></i><b>1.4.2</b> Centering and scaling</a></li>
<li class="chapter" data-level="1.4.3" data-path="lecture1.html"><a href="lecture1.html#tranformations-to-resolve-skewness"><i class="fa fa-check"></i><b>1.4.3</b> Tranformations to resolve skewness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>2</b> Over-fitting and model tuning, selection and evaluation and multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lecture2.html"><a href="lecture2.html#overfitting"><i class="fa fa-check"></i><b>2.1</b> Overfitting</a></li>
<li class="chapter" data-level="2.2" data-path="lecture2.html"><a href="lecture2.html#training-validation-and-test-split"><i class="fa fa-check"></i><b>2.2</b> Training, validation and test split</a></li>
<li class="chapter" data-level="2.3" data-path="lecture2.html"><a href="lecture2.html#multiple-regression"><i class="fa fa-check"></i><b>2.3</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lecture2.html"><a href="lecture2.html#example"><i class="fa fa-check"></i><b>2.3.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lecture3.html"><a href="lecture3.html#gam-example"><i class="fa fa-check"></i><b>3.1</b> GAM example</a>
<ul>
<li class="chapter" data-level="" data-path="lecture3.html"><a href="lecture3.html#data-camp"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>4</b> Classification methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lecture4.html"><a href="lecture4.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>4.1</b> k-nearest neighbor</a></li>
<li class="chapter" data-level="4.2" data-path="lecture4.html"><a href="lecture4.html#naive-bayes"><i class="fa fa-check"></i><b>4.2</b> Naive bayes</a></li>
<li class="chapter" data-level="4.3" data-path="lecture4.html"><a href="lecture4.html#logistic-regression"><i class="fa fa-check"></i><b>4.3</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#data-camp-1"><i class="fa fa-check"></i>Data camp</a></li>
<li class="chapter" data-level="4.3.1" data-path="lecture4.html"><a href="lecture4.html#sources"><i class="fa fa-check"></i><b>4.3.1</b> Sources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>5</b> Trees</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lecture5.html"><a href="lecture5.html#decision-trees"><i class="fa fa-check"></i><b>5.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.2" data-path="lecture5.html"><a href="lecture5.html#random-forrest"><i class="fa fa-check"></i><b>5.2</b> Random forrest</a>
<ul>
<li class="chapter" data-level="" data-path="lecture5.html"><a href="lecture5.html#data-camp-2"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture6.html"><a href="lecture6.html"><i class="fa fa-check"></i><b>6</b> Support Vector Machines</a></li>
<li class="chapter" data-level="7" data-path="lecture7.html"><a href="lecture7.html"><i class="fa fa-check"></i><b>7</b> Artificial Neural networks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lecture7.html"><a href="lecture7.html#mlp"><i class="fa fa-check"></i><b>7.1</b> MLP</a></li>
<li class="chapter" data-level="7.2" data-path="lecture7.html"><a href="lecture7.html#objective-functions-and-training"><i class="fa fa-check"></i><b>7.2</b> Objective functions and training</a></li>
<li class="chapter" data-level="7.3" data-path="lecture7.html"><a href="lecture7.html#validation-of-trained-models"><i class="fa fa-check"></i><b>7.3</b> Validation of trained models</a></li>
<li class="chapter" data-level="7.4" data-path="lecture7.html"><a href="lecture7.html#implementation-of-mlp-in-keras"><i class="fa fa-check"></i><b>7.4</b> Implementation of MLP in Keras</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="lecture7.html"><a href="lecture7.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>7.4.1</b> Installing Keras and Tensorflow</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture7.html"><a href="lecture7.html#importing-mnist"><i class="fa fa-check"></i><b>7.4.2</b> Importing MNIST</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture7.html"><a href="lecture7.html#preprocessing"><i class="fa fa-check"></i><b>7.4.3</b> Preprocessing</a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture7.html"><a href="lecture7.html#mlp-model"><i class="fa fa-check"></i><b>7.4.4</b> MLP model</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture7.html"><a href="lecture7.html#compiling-and-training-the-model"><i class="fa fa-check"></i><b>7.4.5</b> Compiling and training the model</a></li>
<li class="chapter" data-level="7.4.6" data-path="lecture7.html"><a href="lecture7.html#evaluating-the-model"><i class="fa fa-check"></i><b>7.4.6</b> Evaluating the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture8.html"><a href="lecture8.html"><i class="fa fa-check"></i><b>8</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="9" data-path="lecture9.html"><a href="lecture9.html"><i class="fa fa-check"></i><b>9</b> Feature selection/Explainable AI</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data science with R: Applied Predictive Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture7" class="section level1" number="7">
<h1><span class="header-section-number"> 7</span> Artificial Neural networks</h1>
<p>This section describes the basics theory and classical optimization of an artificial neural network. This includes defining a 2-layer feed forward neural network, cost and objective functions, regularization and the manner in which a network can be trained to produce desired output. The section will be limited to so-called supervised learning, i.e. where know the true state of an output. The main references for this chapter are the textbook “Deep Learning” by <span class="citation"><a href="#ref-goodfellow2016deep" role="doc-biblioref">Goodfellow et al.</a> (<a href="#ref-goodfellow2016deep" role="doc-biblioref">2016</a>)</span>.</p>
<div id="mlp" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> MLP</h2>
<p>Let <span class="math inline">\(\mathbf{X}^{(i)}\)</span> be an instance of data which is input to a deep learning model, with associated target values <span class="math inline">\(\mathbf{y}^{(i)}\)</span>. A target value can be a class or as we will use later the data <span class="math inline">\(\mathbf{X}^{(i)}\)</span> itself. All the instances of <span class="math inline">\(\mathbf{X}^{(i)},\)</span> <span class="math inline">\(i=1, \ldots N,\)</span> constitute the data set <span class="math inline">\(\mathbf{X}^{(i)}\)</span>, and all the target values <span class="math inline">\(\mathbf{y}^{(i)},\)</span> <span class="math inline">\(i=1, \ldots N,\)</span> comprise the data set <span class="math inline">\(\mathbf{Y}.\)</span> \</p>
<p>A feed forward neural network is a hierarchical model that consists of nodes or computational units divided into subsequent layers. For each node, a non-linear activation function is applied. The nodes between each layer are connected, so that the input to a node is totally dependent on the output from the nodes of the previous layer. The model is called a  if there are multiple hidden layers; see . The simplest deep learning model has at least one hidden layer: an input layer and an output layer. This hierarchical structure makes it possible to formulate the deep learning model as a linear system of equations. \</p>
<div class="figure">
<img src="~/Dropbox/STAT623%20Applied%20predictive%20modelling/STAT623_compendium/_book/STAT623_compendium_files/figure-html/neuralnet_fig.png" id="fig:NN_model" alt="" />
<p class="caption">Illustration of a deep neural network with three hidden layers</p>
</div>
<p>The model input to a neural network is here defined as vector <span class="math inline">\(\mathbf{x}^{(i)}\)</span> with <span class="math inline">\(Q\)</span> elements. The input is transformed linearly by <span class="math inline">\(\mathbf{W}_1\)</span> and <span class="math inline">\(\mathbf{b}\)</span> such that <span class="math inline">\(f(\mathbf{x}^{(i)}) = \mathbf{W}_1 \mathbf{x}^{(i)} + \mathbf{b}\)</span>. <span class="math inline">\(\mathbf{W}_1\)</span> transforms the input to a vector of <span class="math inline">\(K\)</span> elements and is often called the weight matrix, while the translation <span class="math inline">\(\mathbf{b}\)</span> is referred to as the bias. The bias can be interpreted as a threshold for when the neuron activates. \</p>
<p>A nonlinear activation function is applied element wise to the transformed data. The activation function is typically given as a rectified linear unit (ReLU) <span class="citation"><a href="#ref-nair2010rectified" role="doc-biblioref">Nair and Hinton</a> (<a href="#ref-nair2010rectified" role="doc-biblioref">2010</a>)</span> (legg inn Relu equation!!) or <span class="math inline">\(\tanh\)</span> function. This activation introduces non-linearity to the other linear operations. The superposition of the linear and nonlinear transformation is in combination with the activation function and is what we refer to as a hidden layer. \</p>
<p>Applying another linear transformation <span class="math inline">\(\mathbf{W}_2\)</span> to the hidden layer results in this case to the model output or output layer. The size of the output layer is a row vector with <span class="math inline">\(D\)</span> elements. Generally, many transformations and activations can be applied consecutively which will result in a more complex hierarchical model. A generalization to a network with several hidden layers is straightforward; to make this clear we here limit the notation to a single hidden layer. We note that <span class="math inline">\(\mathbf{x}^{(i)}\)</span> is vector of size <span class="math inline">\(Q\)</span>, <span class="math inline">\(\mathbf{W}_1\)</span> is a <span class="math inline">\(K \times Q\)</span> matrix that transforms the input to <span class="math inline">\(K\)</span> elements, <span class="math inline">\(\mathbf{W}_2\)</span> is a <span class="math inline">\(D \times K\)</span> matrix, transforming the vector into <span class="math inline">\(D\)</span> elements and <span class="math inline">\(\mathbf{b}\)</span> consists of <span class="math inline">\(K\)</span> elements. We write this as linear system of equations transformed with the activation function <span class="math inline">\(\sigma\)</span></p>
<p><span class="math display">\[\widehat{\mathbf{y}}^{(i)} = \mathbf{W}_2 (\sigma(\mathbf{W}_1 \mathbf{x}^{(i)} + \mathbf{b})) := \mathbf{f}^{\omega}, \qquad \omega = \{\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}\}\]</span>
Depending on how the output layer is defined, we can use the network for classification or regression tasks. For classification purposes, the number of nodes in the output layer equals the number of classes, and typically transformed with a softmax function <span class="citation"><a href="#ref-goodfellow2016@softmax" role="doc-biblioref">Goodfellow, Bengio, and Courville</a> (<a href="#ref-goodfellow2016@softmax" role="doc-biblioref">2016</a>)</span>. The softmax function is a generalization of the logistic map that normalizes the output relative to the different classes (Legg inn softmax equation !!). In regression problems we want to estimate relations between variables; we want to predict a continuous output based on some input (variables). To use a linear activation function on the output layer will serve this purpose.<br />
</p>
<p>It has been shown that ANN is a universal approximation <span class="citation"><a href="#ref-hornik1989multilayer" role="doc-biblioref">Hornik, Stinchcombe, and White</a> (<a href="#ref-hornik1989multilayer" role="doc-biblioref">1989</a>)</span>; thus, our goal is to find the weights of the given network to best approximate the map from the input to the output. This means that we want to estimate the weights of the ANN <span class="math inline">\(\mathbf{\omega}\)</span>, given the input data <span class="math inline">\(\mathbf{x}^{(i)}\)</span>, the target <span class="math inline">\(\mathbf{y}^{(i)}\)</span> such that the predictions <span class="math inline">\(\widehat{\mathbf{y}}^{(i)}\)</span> is minimized towards the true target values <span class="math inline">\(\mathbf{y}^{(i)}\)</span>. This is a typical optimization problem, which can be minimized with an objective function and optimization procedure.</p>
</div>
<div id="objective-functions-and-training" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Objective functions and training</h2>
<p>An objective function for use in deep learning typically contains two terms: cost function and regularization. The cost function takes the predicted and the true values as input. Depending on the task and what one wants to minimize, the cost function maximizes a likelihood. In classification problems this is can be the negative cross entropy</p>
<p><span class="math display">\[\mathcal{C}_1^{\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}}(\mathbf{X},\mathbf{Y}) = -             \frac{1}{N}\sum\limits_{j=1}^{N}\mathbf{y}^{(i)}_j\log(\widehat{\mathbf{y}}^{(i)}_j) = -\log p(\mathbf{Y}|\mathbf{f}^{\mathbf{\omega}}(\mathbf{X})),\]</span>
and in regression problems the Mean Squared Error (MSE)</p>
<p><span class="math display">\[\mathcal{C}_2^{\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}}(\mathbf{X},\mathbf{Y}) =              \frac{1}{N}\sum\limits_{i=1}^{N}(\mathbf{y}^{(i)} - \widehat{\mathbf{y}}^{(i)})^2 = - \log p(\mathbf{Y}|\mathbf{f}^\mathbf{\omega}(\mathbf{X})),\]</span></p>
<p>Minimization of the negative cross entropy and the MSE is well known to be equivalent to minimize the negative log likelihood of the parameter estimation <span class="citation"><a href="#ref-tishby1989consistent" role="doc-biblioref">Tishby, Levin, and Solla</a> (<a href="#ref-tishby1989consistent" role="doc-biblioref">1989</a>)</span> for neural networks.
Depending on the task, minimizing  or  with respect to the parameters <span class="math inline">\(\omega = \{\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}\}\)</span> maximizes the likelihood of these parameters. The choice of the cost function is not restricted to those given above, and depend on the data, the model structure and what one wants to predict with the model.\</p>
<p>One of the key problems in deep learning is a phenomenon called over-fitting (<span style="color:blue">some <em>blue</em> Ref to Section </span>). Over-fitting occurs if the optimized model performs poorly on new unseen data, i.e. it does not generalize well. To address this problem, regularization is added to the cost function. \</p>
<p>Regularization is a general technique, where the goal is to make an ill posed problem well-posed (Ref. her kanskje?). Over-fitting is basically one example of an ill-posed problem. For optimization problems, you could add a penalizing functional: L2 or L1 norm for the parameters; or use dropout (Ref. XX). \</p>
<p>Regularization in neural networks work by penalizing the cost function, e.g. forcing the weights to become small. The idea behind a specific regularization term could be to minimize the weights of the ANN to generate a simpler model that helps against over-fitting. <span class="math inline">\(L2\)</span> regularization multiplied with some penalizing factor <span class="math inline">\(\lambda_i\)</span> is one of the most common regularization techniques. The cost function with regularization is called the objective function. Adding <span class="math inline">\(L2\)</span> regularization to equation  or  result in the objective function</p>
<p><span class="math display">\[\mathcal{L}(\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}) = \mathcal{C}^{\mathbf{W}_1, \mathbf{W}_2, \mathbf{b}}(\mathbf{X},\mathbf{Y}) + \lambda_1||\mathbf{W}_1||^2 + \lambda_2||\mathbf{W}_2||^2 + \lambda_3||\mathbf{b}||^2\]</span></p>
<p>Another common way of regularizing the cost function is through dropout, which is a stochastic regularization technique. In the example later we will utelize dropout as regularization. \</p>
<p>Minimizing the objective in  with respect to the weights <span class="math inline">\(\mathbf{\omega}\)</span> with an objective function and a gradient descent optimization method has proven to give good results in a wide range of applications. \</p>
<p>The gradient descent method <span class="citation"><a href="#ref-curry1944method" role="doc-biblioref">Curry</a> (<a href="#ref-curry1944method" role="doc-biblioref">1944</a>)</span> updates the parameter <span class="math inline">\(\mathbf{\omega}\)</span> using the entire data set</p>
<p><span class="math display">\[\mathbf{\omega}_t = \mathbf{\omega}_{t-1} - \eta \nabla \mathcal{L}(\mathbf{\omega}_{t-1}). \]</span></p>
<p>Here <span class="math inline">\(\omega_t\)</span> represents the current configuration of the weights, while <span class="math inline">\(\omega_{t-1}\)</span> represents the previous one. The parameter <span class="math inline">\(\eta\)</span> is referred to as the learning rate, i.e. how large the step in the negative gradient direction the update of the weights should be. Too small steps can lead to poor convergence, while to large steps can lead to overshooting, i.e. missing local/global minimums. Usually it is too expensive to calculate the gradient over the entire dataset. This is solved by a technique called stochastic gradient descent <span class="citation"><a href="#ref-robbins1951stochastic" role="doc-biblioref">Robbins and Monro</a> (<a href="#ref-robbins1951stochastic" role="doc-biblioref">1951</a>)</span>. Stochastic gradient descent performs a parameter update for each training example. A natural extension and a more cost-efficient approach is the mini-batch gradient descent approach. In mini-batch optimization, the gradient is approximated by calculating the mean of the gradients on sub-sets or batches of the entire data set,</p>
<p><span class="math display">\[\mathbf{\omega}_t =  \mathbf{\omega}_{t-1} - \frac{\eta}{n}\sum\limits_{i=1}^{n}\nabla \mathcal{L}_i(\mathbf{\omega}_{t-1}).\]</span></p>
<p>The mini-batch gradient descent iterative process can be implemented in the neural network with the back-propagation algorithm <span class="citation"><a href="#ref-rumelhart1988learning" role="doc-biblioref">Rumelhart et al.</a> (<a href="#ref-rumelhart1988learning" role="doc-biblioref">1988</a>)</span>. In back-propagation, the weights are updated through a forward and backward pass. In the forward pass, we predict with the current weight configuration and compare towards the target values. In the backward pass, we use the chain rule successively from the output to the input to calculate the gradient of <span class="math inline">\(\omega\)</span>. Based on the gradient direction and the learning rate, the configuration of the weights is updated.<br />
</p>
<p>To find the correct learning rate is difficult, hence, several methods has been developed to adjust the learning rate adaptively. One of the disadvantages of the vanilla gradient descent approach to the ANN optimization problem, is that it has a fixed leaning rate <span class="math inline">\(\eta.\)</span> In line with the development of ANN, methods dedicated to deep learning and adaptive adjustment of the learning rate have been developed. Besides SGD with momentum <span class="citation"><a href="#ref-sutskever2013importance" role="doc-biblioref">Sutskever et al.</a> (<a href="#ref-sutskever2013importance" role="doc-biblioref">2013</a>)</span>, the two most used optimization methods for ANNs are ADAM <span class="citation"><a href="#ref-kingma2014adam" role="doc-biblioref">Kingma and Ba</a> (<a href="#ref-kingma2014adam" role="doc-biblioref">2014</a>)</span> and Root Mean Square Propagation (RMSProp) <span class="citation"><a href="#ref-tieleman2012lecture" role="doc-biblioref">Tieleman and Hinton</a> (<a href="#ref-tieleman2012lecture" role="doc-biblioref">2012</a>)</span>. RMSProp adaptively adjusts the learning rate of the gradients based on a running average for each of the individual parameters. The ADAM-algorithm individually adjusts the weights in terms of both the running average, but also with respect to the running variance. \</p>
<p>The use of back-propagation together with a stochastic gradient descent method, increase in available data and hardware have been the successes of deep learning during the past decade.</p>
</div>
<div id="validation-of-trained-models" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Validation of trained models</h2>
<p>To validate and ensure that the predictions of the deep learning model also performs well on new unseen instances, the data is split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. \</p>
<p>After each epoch, we predict with this temporally model on the validation data set. Usually we put criteria on the performance on the validation data set for when to stop the optimization. We can use a so called early stopping regime, where the model stops training if it does not see improvement on the validation score after a certain number of epochs without improvement. \</p>
<p>The purpose of the test data set is to validate on new unseen data that has not been part of the training or the continuous validation of the model. Lets look at an example of how to implement a classifier on the mnist data in keras  </p>
</div>
<div id="implementation-of-mlp-in-keras" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Implementation of MLP in Keras</h2>
<p>In this section we will show you how to implement a MLP with dropout as regularization with the low level API for tensorflow called Keras. A lot of the development within deep learning is concentrated around the python programming language. Keras is also a tool that is originally developed in keras, but also available in R. Lets start by installing tensorflow:</p>
<div id="installing-keras-and-tensorflow" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Installing Keras and Tensorflow</h3>
<p>First install tensorflow with the following commands</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="lecture7.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&#39;tensorflow&#39;</span>)</span>
<span id="cb63-2"><a href="lecture7.html#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&#39;tensorflow&#39;</span>)</span>
<span id="cb63-3"><a href="lecture7.html#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="fu">install_tensorflow</span>()</span></code></pre></div>
<p>Then install keras with the following commands</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="lecture7.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.package</span>(<span class="st">&#39;keras&#39;</span>)</span>
<span id="cb64-2"><a href="lecture7.html#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb64-3"><a href="lecture7.html#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="fu">install_keras</span>()</span></code></pre></div>
</div>
<div id="importing-mnist" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Importing MNIST</h3>
<p>We will use the famous MNIST data set as an example in this section (Ref. XX). The MNIST dataset contains 60000 handwritten digits for training the model, and 10000 for testing. The digits has a shape of 28x28 pixels in grey scale, that is, they only have one channel, describing the black/white intensity of the pixel value. Each picture/instance is associated with a label, that is a number from 0-9. The keras package contains a function for downloading the MNIST dataset from the source (Ref. XX)</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="lecture7.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb65-2"><a href="lecture7.html#cb65-2" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span></code></pre></div>
<p>To ensure that the predictions of the neural network model also performs well on new unseen instances, the data is often split into three independent sub-data sets: a training, a validation and a test data set. The training data set is directly used to optimize/train the parameters of the model. The validation data set is indirectly used to optimize the model, that is, we monitor the performance on the validation dataset after each epoch. An epoch is one pass in the optimization over the entire training dataset. During training, the model sees the same training data multiple times, however, the instances are usually randomly shuffled before a new epoch starts. We will come back to validation and testing of the neural network later in this section. The MNIST dataset are split into a test and train data set from source</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="lecture7.html#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb66-2"><a href="lecture7.html#cb66-2" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb66-3"><a href="lecture7.html#cb66-3" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb66-4"><a href="lecture7.html#cb66-4" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb66-5"><a href="lecture7.html#cb66-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span></code></pre></div>
<p>We can check out some of the training data</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="lecture7.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">#checking the dimension of the train/test data</span></span>
<span id="cb67-2"><a href="lecture7.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb67-3"><a href="lecture7.html#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="lecture7.html#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># visualize the digits</span></span>
<span id="cb67-5"><a href="lecture7.html#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfcol=</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb67-6"><a href="lecture7.html#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>), <span class="at">xaxs=</span><span class="st">&#39;i&#39;</span>, <span class="at">yaxs=</span><span class="st">&#39;i&#39;</span>)</span>
<span id="cb67-7"><a href="lecture7.html#cb67-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-8"><a href="lecture7.html#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (idx <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>) { </span>
<span id="cb67-9"><a href="lecture7.html#cb67-9" aria-hidden="true" tabindex="-1"></a>    im <span class="ot">&lt;-</span> x_train[idx,,]</span>
<span id="cb67-10"><a href="lecture7.html#cb67-10" aria-hidden="true" tabindex="-1"></a>    im <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(im, <span class="dv">2</span>, rev)) </span>
<span id="cb67-11"><a href="lecture7.html#cb67-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, im, <span class="at">col=</span><span class="fu">gray</span>((<span class="dv">0</span><span class="sc">:</span><span class="dv">255</span>)<span class="sc">/</span><span class="dv">255</span>), </span>
<span id="cb67-12"><a href="lecture7.html#cb67-12" aria-hidden="true" tabindex="-1"></a>          <span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">main=</span><span class="fu">paste</span>(y_train[idx]))</span>
<span id="cb67-13"><a href="lecture7.html#cb67-13" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="lecture7.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co">#checking the dimension of the train/test data</span></span>
<span id="cb68-2"><a href="lecture7.html#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(y_train)</span>
<span id="cb68-3"><a href="lecture7.html#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(y_train)</span></code></pre></div>
<p>Here we see that the training dataset has a shape of <span class="math inline">\(60000 \times 28 \times28\)</span>. This means that there are <span class="math inline">\(60000\)</span> instances, or pictures of handwritten digits, where each of the pictures has <span class="math inline">\(28 \times 28\)</span> dimension in the horizontal and vertical direction. The dimension of the y_train variable represents the class or label of the x_train variable.</p>
</div>
<div id="preprocessing" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Preprocessing</h3>
<p>Pre-processing is the step where we transform, scale, removes, imputes etc. the data before we feed it to the deep learning model. Removing erroneous data, or impute NA values may be of crucial importance to get the algorithm run at all. Transforming or scaling the data helps improve the so called conditioning problem. Conditioning of a problem say something about the sensitivity of the solution to changes in the problem data. To create an easier problem to optimize, we pre-process the MNIST-data before we are feeding it to the neural network model. Here we reshape and scale the data.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="lecture7.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reshape</span></span>
<span id="cb69-2"><a href="lecture7.html#cb69-2" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_train, <span class="fu">c</span>(<span class="fu">nrow</span>(x_train), <span class="dv">784</span>))</span>
<span id="cb69-3"><a href="lecture7.html#cb69-3" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_test, <span class="fu">c</span>(<span class="fu">nrow</span>(x_test), <span class="dv">784</span>))</span>
<span id="cb69-4"><a href="lecture7.html#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="co"># rescale</span></span>
<span id="cb69-5"><a href="lecture7.html#cb69-5" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> x_train <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb69-6"><a href="lecture7.html#cb69-6" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test <span class="sc">/</span> <span class="dv">255</span></span></code></pre></div>
<p>We also need to convert the y_train and y_test to categorical variables. Instead of having a vector with a number from 0-9, we convert it to a binary representation. That is, each label is represented by a vector of length 10 with one element is 1 and the rest 0. The element that is 1 represent the label of the digit.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="lecture7.html#cb70-1" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_train, <span class="at">num_classes =</span> <span class="cn">NULL</span>)</span>
<span id="cb70-2"><a href="lecture7.html#cb70-2" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_test, <span class="at">num_classes =</span> <span class="cn">NULL</span>)</span>
<span id="cb70-3"><a href="lecture7.html#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="lecture7.html#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(y_train)</span></code></pre></div>
</div>
<div id="mlp-model" class="section level3" number="7.4.4">
<h3><span class="header-section-number">7.4.4</span> MLP model</h3>
<p>In this example we want to create a so called dense neural network, or MLP. That is, each node are influenced from all of the nodes in the previous layer. This is contrast to e.g. convolutional neural networks that uses convolutions to reduce the connectivity between layers, thus reducing the connections.</p>
<p>We construct a dense neural network. After reshaping the input, the network has a shape of <span class="math inline">\(28 \times 28 = 784\)</span> per instance. This is our input shape. This shape has to be specified in the model, as shown below. The first layer has <span class="math inline">\(256\)</span> nodes, and we use the rectified linear unit as activations function. The next layer is a so called dropout layer. This layer drops randomly a proportion of the nodes out during a forward and backward pass. This will help avoiding overfitting, i.e. it is a form of regularization of the network. The next hidden layer has <span class="math inline">\(128\)</span> nodes, and also ReLU activation function. The last layer of the network is the output. The output in this case, is to classify the label of the digit. There are 10 possible classes, 0-9 and we thus use a softmax activation function, with 10 units.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="lecture7.html#cb71-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>() </span>
<span id="cb71-2"><a href="lecture7.html#cb71-2" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> </span>
<span id="cb71-3"><a href="lecture7.html#cb71-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">256</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">784</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb71-4"><a href="lecture7.html#cb71-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb71-5"><a href="lecture7.html#cb71-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">128</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb71-6"><a href="lecture7.html#cb71-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate =</span> <span class="fl">0.3</span>) <span class="sc">%&gt;%</span></span>
<span id="cb71-7"><a href="lecture7.html#cb71-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">10</span>, <span class="at">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span></code></pre></div>
<p>First we construct a sequential model object. Then we can use the pipe syntax to specify the layers of the neural network. We can use the summary function to show a nice overview of the neural network model.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="lecture7.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
</div>
<div id="compiling-and-training-the-model" class="section level3" number="7.4.5">
<h3><span class="header-section-number">7.4.5</span> Compiling and training the model</h3>
<p>We now have a model, the next step is to compile it. Durring compiling we also have to specify the loss function, optimizer and metrics for evaluations and validation.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="lecture7.html#cb73-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb73-2"><a href="lecture7.html#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&#39;categorical_crossentropy&#39;</span>,</span>
<span id="cb73-3"><a href="lecture7.html#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&#39;adam&#39;</span>,</span>
<span id="cb73-4"><a href="lecture7.html#cb73-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="st">&#39;accuracy&#39;</span></span>
<span id="cb73-5"><a href="lecture7.html#cb73-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Since we want to classify the digits, we have chosen the loss to be categorical cross entropy. We use the adam-algorithm during optimization and we also specify that we want to return the accuaracy of the classification during training and validation.</p>
<p>After the compilation, we can call the fit() function to train our model. The fit function take x_train and y_train as inputs. We also have to specify how many epoch the model should be trained on (how many times we will see the entire data set), the batch size (how many/large the splits of the training data set) and how large proportion of the data should be used as online validation data. Test data are set aside for evaluation the model when it has finished training.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="lecture7.html#cb74-1" aria-hidden="true" tabindex="-1"></a>history <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb74-2"><a href="lecture7.html#cb74-2" aria-hidden="true" tabindex="-1"></a>  x_train, y_train, </span>
<span id="cb74-3"><a href="lecture7.html#cb74-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">30</span>,</span>
<span id="cb74-4"><a href="lecture7.html#cb74-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>, </span>
<span id="cb74-5"><a href="lecture7.html#cb74-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb74-6"><a href="lecture7.html#cb74-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>It is possible to specify specific validation data if that is desirable. Batch size, epochs, size of the neural network, e.g. number of layers nodes, type of network (e.g. cnn, rnns) are so called hyper parameters. This is parameters that are not optimized in the network itself, but has to be specified manually. Theses hyper parameters are important and there are different strategies for optimize them. Hyper parameter search could be done with traditional grid search, but also more sophisticated approaches such as Bayesian hyper optimization (Ref. XX).</p>
<p>We have plotted the loss and accuaracy calculated after each training epoch. It can be observed that the loss after approx 10-15 epochs, is significantly lower than the validation loss. This is an indication that the model starts to overfit.</p>
</div>
<div id="evaluating-the-model" class="section level3" number="7.4.6">
<h3><span class="header-section-number">7.4.6</span> Evaluating the model</h3>
<p>To evaluate the trained model on the test data, we can use the pipe syntax together with the evaluate function</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="lecture7.html#cb75-1" aria-hidden="true" tabindex="-1"></a>model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(x_test, y_test)</span></code></pre></div>
<p>The trained model performes pretty well on the test data set. It has an accuaracy of approximatley 0.98, that is 98 <span class="math inline">\(\%\)</span> of the data in the test data set is classified correctly. Lets<br />
### Predicting
We can also predict the most probable class for each of the instances. For this we can use the predict function. (Litt mer her…)</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="lecture7.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co">#model %&gt;% predict(x_test) %&gt;% k_argmax()</span></span>
<span id="cb76-2"><a href="lecture7.html#cb76-2" aria-hidden="true" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> model <span class="sc">%&gt;%</span> <span class="fu">predict</span>(x_test)</span>
<span id="cb76-3"><a href="lecture7.html#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="co">#plot(prediction[1710,], )</span></span>
<span id="cb76-4"><a href="lecture7.html#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(prediction)</span>
<span id="cb76-5"><a href="lecture7.html#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="lecture7.html#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Lets find one of the most uncertain predictions to view how it looks like</span></span>
<span id="cb76-7"><a href="lecture7.html#cb76-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-8"><a href="lecture7.html#cb76-8" aria-hidden="true" tabindex="-1"></a><span class="co">#We find the most probable predictions in the test data set</span></span>
<span id="cb76-9"><a href="lecture7.html#cb76-9" aria-hidden="true" tabindex="-1"></a>largest <span class="ot">&lt;-</span> <span class="fu">apply</span>(prediction,<span class="dv">1</span>,max,<span class="at">na.rm=</span><span class="cn">TRUE</span>)</span>
<span id="cb76-10"><a href="lecture7.html#cb76-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-11"><a href="lecture7.html#cb76-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Then we choose the predictions which has the lowest confident </span></span>
<span id="cb76-12"><a href="lecture7.html#cb76-12" aria-hidden="true" tabindex="-1"></a>less_confident <span class="ot">&lt;-</span> <span class="fu">sort</span>(largest,<span class="at">index.return =</span> <span class="cn">TRUE</span>)<span class="sc">$</span>ix[<span class="dv">1</span><span class="sc">:</span><span class="dv">36</span>]</span>
<span id="cb76-13"><a href="lecture7.html#cb76-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-14"><a href="lecture7.html#cb76-14" aria-hidden="true" tabindex="-1"></a><span class="co">#We find out what the model predicts these less_confident predictions are</span></span>
<span id="cb76-15"><a href="lecture7.html#cb76-15" aria-hidden="true" tabindex="-1"></a>pred_class <span class="ot">&lt;-</span> prediction <span class="sc">%&gt;%</span> <span class="fu">k_argmax</span>() </span>
<span id="cb76-16"><a href="lecture7.html#cb76-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-17"><a href="lecture7.html#cb76-17" aria-hidden="true" tabindex="-1"></a><span class="co">#and plot the digits</span></span>
<span id="cb76-18"><a href="lecture7.html#cb76-18" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfcol=</span><span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb76-19"><a href="lecture7.html#cb76-19" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>), <span class="at">xaxs=</span><span class="st">&#39;i&#39;</span>, <span class="at">yaxs=</span><span class="st">&#39;i&#39;</span>)</span>
<span id="cb76-20"><a href="lecture7.html#cb76-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-21"><a href="lecture7.html#cb76-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (idx <span class="cf">in</span> less_confident) { </span>
<span id="cb76-22"><a href="lecture7.html#cb76-22" aria-hidden="true" tabindex="-1"></a>    im <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x[idx,,]</span>
<span id="cb76-23"><a href="lecture7.html#cb76-23" aria-hidden="true" tabindex="-1"></a>    im <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(im, <span class="dv">2</span>, rev)) </span>
<span id="cb76-24"><a href="lecture7.html#cb76-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, im, <span class="at">col=</span><span class="fu">gray</span>((<span class="dv">0</span><span class="sc">:</span><span class="dv">255</span>)<span class="sc">/</span><span class="dv">255</span>), </span>
<span id="cb76-25"><a href="lecture7.html#cb76-25" aria-hidden="true" tabindex="-1"></a>          <span class="at">xaxt=</span><span class="st">&#39;n&#39;</span>, <span class="at">main=</span><span class="fu">paste</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>y[idx]))</span>
<span id="cb76-26"><a href="lecture7.html#cb76-26" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-curry1944method" class="csl-entry">
Curry, Haskell B. 1944. <span>“The Method of Steepest Descent for Non-Linear Minimization Problems.”</span> <em>Quarterly of Applied Mathematics</em> 2 (3): 258–61.
</div>
<div id="ref-goodfellow2016@softmax" class="csl-entry">
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>6.2.2.3 Softmax Units for Multinoulli Output Distributions.</em> MIT press.
</div>
<div id="ref-goodfellow2016deep" class="csl-entry">
Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. <em>Deep Learning</em>. Vol. 1. MIT press Cambridge.
</div>
<div id="ref-hornik1989multilayer" class="csl-entry">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>“Multilayer Feedforward Networks Are Universal Approximators.”</span> <em>Neural Networks</em> 2 (5): 359–66.
</div>
<div id="ref-kingma2014adam" class="csl-entry">
Kingma, Diederik P, and Jimmy Ba. 2014. <span>“Adam: A Method for Stochastic Optimization.”</span> <em>arXiv Preprint arXiv:1412.6980</em>.
</div>
<div id="ref-nair2010rectified" class="csl-entry">
Nair, Vinod, and Geoffrey E Hinton. 2010. <span>“Rectified Linear Units Improve Restricted Boltzmann Machines.”</span> In <em>Proceedings of the 27th International Conference on Machine Learning (ICML-10)</em>, 807–14.
</div>
<div id="ref-robbins1951stochastic" class="csl-entry">
Robbins, Herbert, and Sutton Monro. 1951. <span>“A Stochastic Approximation Method.”</span> <em>The Annals of Mathematical Statistics</em>, 400–407.
</div>
<div id="ref-rumelhart1988learning" class="csl-entry">
Rumelhart, David E, Geoffrey E Hinton, Ronald J Williams, et al. 1988. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Cognitive Modeling</em> 5 (3): 1.
</div>
<div id="ref-sutskever2013importance" class="csl-entry">
Sutskever, Ilya, James Martens, George Dahl, and Geoffrey Hinton. 2013. <span>“On the Importance of Initialization and Momentum in Deep Learning.”</span> In <em>International Conference on Machine Learning</em>, 1139–47.
</div>
<div id="ref-tieleman2012lecture" class="csl-entry">
Tieleman, Tijmen, and Geoffrey Hinton. 2012. <span>“Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude.”</span> <em>COURSERA: Neural Networks for Machine Learning</em> 4 (2): 26–31.
</div>
<div id="ref-tishby1989consistent" class="csl-entry">
Tishby, Naftali, Esther Levin, and Sara A Solla. 1989. <span>“Consistent Inference of Probabilities in Layered Networks: Predictions and Generalization.”</span> In <em>International Joint Conference on Neural Networks</em>, 2:403–9.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STAT623 compendium.pdf", "STAT623 compendium.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
