<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 2 Over-fitting and model tuning, selection and evaluation and multiple regression | Data science with R: Applied Predictive Modelling</title>
  <meta name="description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content=" 2 Over-fitting and model tuning, selection and evaluation and multiple regression | Data science with R: Applied Predictive Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 2 Over-fitting and model tuning, selection and evaluation and multiple regression | Data science with R: Applied Predictive Modelling" />
  
  <meta name="twitter:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

<meta name="author" content="Sondre Hølleland and Kristian Gundersen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture1.html"/>
<link rel="next" href="lecture3.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">STAT623 Applied predictive modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes-and-objectives"><i class="fa fa-check"></i>Learning outcomes and objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecture-overview"><i class="fa fa-check"></i>Lecture overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#litterature"><i class="fa fa-check"></i>Litterature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>1</b> Introduction and short recap of R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="lecture1.html"><a href="lecture1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="lecture1.html"><a href="lecture1.html#prediction-or-interpretation"><i class="fa fa-check"></i><b>1.1.1</b> Prediction or interpretation?</a></li>
<li class="chapter" data-level="1.1.2" data-path="lecture1.html"><a href="lecture1.html#terminology"><i class="fa fa-check"></i><b>1.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="lecture1.html"><a href="lecture1.html#recap-of-r"><i class="fa fa-check"></i><b>1.2</b> Recap of R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="lecture1.html"><a href="lecture1.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Installing R and Rstudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="lecture1.html"><a href="lecture1.html#r-community-and-packages"><i class="fa fa-check"></i><b>1.2.2</b> R community and packages</a></li>
<li class="chapter" data-level="1.2.3" data-path="lecture1.html"><a href="lecture1.html#datacamp"><i class="fa fa-check"></i><b>1.2.3</b> Datacamp</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="lecture1.html"><a href="lecture1.html#data-preprocessing"><i class="fa fa-check"></i><b>1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="1.4" data-path="lecture1.html"><a href="lecture1.html#case-study"><i class="fa fa-check"></i><b>1.4</b> Case study</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="lecture1.html"><a href="lecture1.html#data-transformations-for-individual-predictors"><i class="fa fa-check"></i><b>1.4.1</b> Data transformations for individual predictors</a></li>
<li class="chapter" data-level="1.4.2" data-path="lecture1.html"><a href="lecture1.html#centering-and-scaling"><i class="fa fa-check"></i><b>1.4.2</b> Centering and scaling</a></li>
<li class="chapter" data-level="1.4.3" data-path="lecture1.html"><a href="lecture1.html#tranformations-to-resolve-skewness"><i class="fa fa-check"></i><b>1.4.3</b> Tranformations to resolve skewness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>2</b> Over-fitting and model tuning, selection and evaluation and multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lecture2.html"><a href="lecture2.html#overfitting"><i class="fa fa-check"></i><b>2.1</b> Overfitting</a></li>
<li class="chapter" data-level="2.2" data-path="lecture2.html"><a href="lecture2.html#training-validation-and-test-split"><i class="fa fa-check"></i><b>2.2</b> Training, validation and test split</a></li>
<li class="chapter" data-level="2.3" data-path="lecture2.html"><a href="lecture2.html#multiple-regression"><i class="fa fa-check"></i><b>2.3</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lecture2.html"><a href="lecture2.html#example"><i class="fa fa-check"></i><b>2.3.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lecture3.html"><a href="lecture3.html#gam-example"><i class="fa fa-check"></i><b>3.1</b> GAM example</a>
<ul>
<li class="chapter" data-level="" data-path="lecture3.html"><a href="lecture3.html#data-camp"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>4</b> Classification methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lecture4.html"><a href="lecture4.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>4.1</b> k-nearest neighbor</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-1"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture4.html"><a href="lecture4.html#logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-2"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture4.html"><a href="lecture4.html#naive-bayes"><i class="fa fa-check"></i><b>4.3</b> Naive bayes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture4.html"><a href="lecture4.html#wrap-up"><i class="fa fa-check"></i><b>4.4</b> Wrap-up</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#data-camp-1"><i class="fa fa-check"></i>Data camp</a></li>
<li class="chapter" data-level="4.4.1" data-path="lecture4.html"><a href="lecture4.html#sources"><i class="fa fa-check"></i><b>4.4.1</b> Sources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>5</b> Decision Trees and Bagged Trees</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lecture5.html"><a href="lecture5.html#decision-trees"><i class="fa fa-check"></i><b>5.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.2" data-path="lecture5.html"><a href="lecture5.html#bagged-trees"><i class="fa fa-check"></i><b>5.2</b> Bagged trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture5.html"><a href="lecture5.html#data-camp-2"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture6.html"><a href="lecture6.html"><i class="fa fa-check"></i><b>6</b> Random forrest and boosting</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lecture6.html"><a href="lecture6.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>6.1</b> Hyperparameter tuning</a></li>
<li class="chapter" data-level="6.2" data-path="lecture6.html"><a href="lecture6.html#random-forest"><i class="fa fa-check"></i><b>6.2</b> Random Forest</a>
<ul>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#example-4"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lecture6.html"><a href="lecture6.html#boosted-trees"><i class="fa fa-check"></i><b>6.3</b> Boosted trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#iris-data"><i class="fa fa-check"></i>Iris data</a></li>
<li class="chapter" data-level="" data-path="lecture6.html"><a href="lecture6.html#data-camp-3"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture7.html"><a href="lecture7.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lecture7.html"><a href="lecture7.html#linear-svms"><i class="fa fa-check"></i><b>7.1</b> Linear SVMs</a></li>
<li class="chapter" data-level="7.2" data-path="lecture7.html"><a href="lecture7.html#polynomial-svm"><i class="fa fa-check"></i><b>7.2</b> Polynomial SVM</a>
<ul>
<li class="chapter" data-level="" data-path="lecture7.html"><a href="lecture7.html#data-camp-4"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture8.html"><a href="lecture8.html"><i class="fa fa-check"></i><b>8</b> Artificial Neural networks</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lecture8.html"><a href="lecture8.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>8.1</b> Multilayer Perceptron (MLP)</a></li>
<li class="chapter" data-level="8.2" data-path="lecture8.html"><a href="lecture8.html#objective-functions-and-training"><i class="fa fa-check"></i><b>8.2</b> Objective functions and training</a></li>
<li class="chapter" data-level="8.3" data-path="lecture8.html"><a href="lecture8.html#validation-of-trained-models"><i class="fa fa-check"></i><b>8.3</b> Validation of trained models</a></li>
<li class="chapter" data-level="8.4" data-path="lecture8.html"><a href="lecture8.html#implementation-of-mlp-in-keras"><i class="fa fa-check"></i><b>8.4</b> Implementation of MLP in Keras</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="lecture8.html"><a href="lecture8.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>8.4.1</b> Installing Keras and Tensorflow</a></li>
<li class="chapter" data-level="8.4.2" data-path="lecture8.html"><a href="lecture8.html#importing-mnist"><i class="fa fa-check"></i><b>8.4.2</b> Importing MNIST</a></li>
<li class="chapter" data-level="8.4.3" data-path="lecture8.html"><a href="lecture8.html#preprocessing"><i class="fa fa-check"></i><b>8.4.3</b> Preprocessing</a></li>
<li class="chapter" data-level="8.4.4" data-path="lecture8.html"><a href="lecture8.html#mlp-model"><i class="fa fa-check"></i><b>8.4.4</b> MLP model</a></li>
<li class="chapter" data-level="8.4.5" data-path="lecture8.html"><a href="lecture8.html#compiling-and-training-the-model"><i class="fa fa-check"></i><b>8.4.5</b> Compiling and training the model</a></li>
<li class="chapter" data-level="8.4.6" data-path="lecture8.html"><a href="lecture8.html#evaluating-the-model"><i class="fa fa-check"></i><b>8.4.6</b> Evaluating the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lecture9.html"><a href="lecture9.html"><i class="fa fa-check"></i><b>9</b> Feature selection/Explainable AI</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data science with R: Applied Predictive Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture2" class="section level1" number="2">
<h1><span class="header-section-number"> 2</span> Over-fitting and model tuning, selection and evaluation and multiple regression</h1>
<p>In this lecture we will cover the terms <em>overfitting</em>, <em>training-</em>, <em>validation-</em> and <em>test</em> sets, <em>model selection</em> and <em>-evaluation</em>. These terms are part of the chargong of predictive modelling and is something we need to get familiar with before learning about specific models. Towards the end of this lecture, we will learn about multiple regression.</p>
<div id="overfitting" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Overfitting</h2>
<p><strong>Overfitting</strong> is the phenomenon when you build a model that fits (almost) “perfectly” to the training set, but when applied to new data the performance is low. For the modeller there is a balance between choosing a model that has a good fit to the training data, but also generalize well to new data. The goal for the model should be to discover the underlying signal and not fit to all the noise.</p>
<p>We will illustrate overfitting by an example with simulated data and GAM models (topic for the lecture <a href="lecture3.html#lecture3">Lecture 3</a>)</p>
<!-- Could make this into a movie from here and out -->
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="lecture2.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb7-2"><a href="lecture2.html#cb7-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>pi,<span class="fl">0.1</span>)</span>
<span id="cb7-3"><a href="lecture2.html#cb7-3" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">sin</span>(x)</span>
<span id="cb7-4"><a href="lecture2.html#cb7-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> z <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">sd</span>(z), <span class="at">n=</span><span class="fu">length</span>(x))</span>
<span id="cb7-5"><a href="lecture2.html#cb7-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(x,y,z)</span>
<span id="cb7-6"><a href="lecture2.html#cb7-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>z), <span class="at">lwd =</span> .<span class="dv">8</span>, <span class="at">col =</span> <span class="dv">2</span>) </span>
<span id="cb7-7"><a href="lecture2.html#cb7-7" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/overfitting-1.png" width="672" /></p>
<p>In the figure above, the “true” underlying signal is the red curve while the black dots are the observations. If we fit a model with a high level of flexibility, we can make it fit well to the observations.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="lecture2.html#cb8-1" aria-hidden="true" tabindex="-1"></a>p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;gam&quot;</span>, <span class="at">formula =</span> y  <span class="sc">~</span> <span class="fu">s</span>(x,<span class="at">k =</span> <span class="dv">50</span>, <span class="at">sp =</span> <span class="dv">0</span>))</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/overfitting2-1.png" width="672" /></p>
<p>We will come back to the details of the model setup when discussing GAM models in the next lecture, but the point here is that we have “turned off” the likelihood penalization by setting <em>sp=0</em> in the smoother function <em>s</em> and set the order of the smoother to a high number (<em>k = 50</em>). If we set the order to <em>k = 4</em> and let the GAM procedure set the penalization itself, we get a much smoother curve that lies closer to the true signal.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="lecture2.html#cb9-1" aria-hidden="true" tabindex="-1"></a>p <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;gam&quot;</span>, <span class="at">formula =</span> y  <span class="sc">~</span> <span class="fu">s</span>(x,<span class="at">k =</span> <span class="dv">4</span>))</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/overfitting3-1.png" width="672" /></p>
<p>The first model explains 96.3% of the deviance, while the second explains 81.7%.</p>
<p>In a normal situation, we would of course not know the true underlying signal, but overfitting can be suspected when you get such a wiggly prediction curve that seems to follow every little move in the observations. Judging by the fit to the points, the first GAM model is much closer to the observations, but we can check how well it would perform on a new set of data with the same structure.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="lecture2.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mgcv) <span class="co"># package for fitting gam models</span></span>
<span id="cb10-2"><a href="lecture2.html#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model with high level of complexity: </span></span>
<span id="cb10-3"><a href="lecture2.html#cb10-3" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">gam</span>(y <span class="sc">~</span> <span class="fu">s</span>(x, <span class="at">k=</span> <span class="dv">50</span>, <span class="at">sp =</span> <span class="dv">0</span>), <span class="at">data =</span> df)</span>
<span id="cb10-4"><a href="lecture2.html#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ s(x, k = 50, sp = 0)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.02501    0.04060  -0.616    0.549
## 
## Approximate significance of smooth terms:
##      edf Ref.df     F  p-value    
## s(x)  49     49 6.895 0.000262 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.823   Deviance explained = 96.3%
## GCV = 0.50314  Scale est. = 0.10382   n = 63</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="lecture2.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit simpler model: </span></span>
<span id="cb12-2"><a href="lecture2.html#cb12-2" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">gam</span>(y <span class="sc">~</span> <span class="fu">s</span>(x, <span class="at">k=</span> <span class="dv">4</span>), <span class="at">data =</span> df)</span>
<span id="cb12-3"><a href="lecture2.html#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ s(x, k = 4)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.02501    0.04238   -0.59    0.557
## 
## Approximate significance of smooth terms:
##        edf Ref.df     F p-value    
## s(x) 2.988      3 87.32  &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.807   Deviance explained = 81.7%
## GCV = 0.12081  Scale est. = 0.11317   n = 63</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="lecture2.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate new set of data: </span></span>
<span id="cb14-2"><a href="lecture2.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>)</span>
<span id="cb14-3"><a href="lecture2.html#cb14-3" aria-hidden="true" tabindex="-1"></a>newdata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, z, </span>
<span id="cb14-4"><a href="lecture2.html#cb14-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">y =</span> z<span class="sc">+</span><span class="fu">rnorm</span>(<span class="at">mean=</span><span class="dv">0</span>, <span class="at">sd=</span><span class="fl">0.5</span><span class="sc">*</span><span class="fu">sd</span>(z), <span class="at">n=</span><span class="fu">length</span>(x)))</span>
<span id="cb14-5"><a href="lecture2.html#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="lecture2.html#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions from the two models: </span></span>
<span id="cb14-7"><a href="lecture2.html#cb14-7" aria-hidden="true" tabindex="-1"></a>newdata<span class="sc">$</span>pred1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit1, <span class="at">newdata=</span>newdata)</span>
<span id="cb14-8"><a href="lecture2.html#cb14-8" aria-hidden="true" tabindex="-1"></a>newdata<span class="sc">$</span>pred2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit2, <span class="at">newdata=</span>newdata)</span>
<span id="cb14-9"><a href="lecture2.html#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="lecture2.html#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate sum of squared residuals:</span></span>
<span id="cb14-11"><a href="lecture2.html#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(newdata, </span>
<span id="cb14-12"><a href="lecture2.html#cb14-12" aria-hidden="true" tabindex="-1"></a>     <span class="fu">c</span>(<span class="st">&quot;SS1&quot;</span> <span class="ot">=</span> <span class="fu">sum</span>((y<span class="sc">-</span>pred1)<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb14-13"><a href="lecture2.html#cb14-13" aria-hidden="true" tabindex="-1"></a>       <span class="st">&quot;SS2&quot;</span> <span class="ot">=</span> <span class="fu">sum</span>((y<span class="sc">-</span>pred2)<span class="sc">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>##       SS1       SS2 
## 14.160736  7.734603</code></pre>
<p>As you can see, the sum of squared residuals is almost twice as high for the complex model when applied to new observations, compared to the simpler model. In a preditive modelling situation, it is the prediction performance that is important and not as much the <em>deviance explained</em> on training data. Below is a figure with the two predicition curves on the new data. Since <span class="math inline">\(x\)</span> is the only input to the models, the two lines are equal to the curves above, but the data is new.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="lecture2.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the new data: </span></span>
<span id="cb16-2"><a href="lecture2.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span> newdata, <span class="fu">aes</span>(<span class="at">x=</span>x,<span class="at">y=</span>y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb16-3"><a href="lecture2.html#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>pred1, <span class="at">col =</span> <span class="st">&quot;fit1&quot;</span>))<span class="sc">+</span></span>
<span id="cb16-4"><a href="lecture2.html#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>pred2, <span class="at">col =</span> <span class="st">&quot;fit2&quot;</span>))</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/plotoverfit-1.png" width="672" /></p>
</div>
<div id="training-validation-and-test-split" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Training, validation and test split</h2>
<p>In order to evaluate a predictive model’s performance, we should test its predictive abilities on data the model has never seen before (during model fitting). Usually, you only have one dataset to start with, so it is common to split the original data into three subsets; a training set, a validation set and a test set.</p>
<p><strong>The training set</strong> is the data used to “train” the model, meaning estimating its parameters. If your original data is split, this will typically be the biggest portion of the data. One should measure the model performance on the training data, but this should not be used solely for model selection. If your model has a very good fit to the training data, this can often lead to overfitting issues when applying the model to new data. This is one of the reasons for using a validation set.</p>
<p><strong>The validation set</strong> is used for tackling overfitting and doing model selection. This is a smaller portion of the data not seen during training, which is key. We can therefore measure the models performance on these unseen data and if the model performs similarly as on the traning data, this means it generalizes well to new situations and overfitting is likely not a big issue. If the performance is high in training and low for validation, this is a sign of overfitting. If you are in a scenario where you have many different candidate models, either from different model families or the same model but with different setups, you can use the validation set to select “the best” model according to the performance criteria you have chosen.</p>
<p>Once you have found a model that does not overfit to training data and performs well on the validation set, you can apply it to <strong>the test set</strong>. This should only be used to measure the models performance. In some cases, one sees modellers only splitting the data in two, a training and testing set. If you use the validation set to measure the models performance, this will give a biased estimate of model performance, since you have selected the model based on the validation set. Basically, you will not know whether your model performs better because of changes you made or because it just happened to fit the validation set better. Therefore, we need a separate test set. The performance on the test set should ideally be similar to the performance on the validation set. If it is significantly lower, this can indicate overfitting to the validation set.</p>
<div class="figure"><span style="display:block;" id="fig:trainvalidatetest"></span>
<img src="ppt/trainvalidatetestfig.png" alt="Summary of the three-way data split." width="1470" />
<p class="caption">
Figure2.1: Summary of the three-way data split.
</p>
</div>
<p>We will illustrate how this three-way split can be used on an example, after we have learned about multiple regression.</p>
</div>
<div id="multiple-regression" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Multiple regression</h2>
<p>Multiple regression is an extension of simple linear regression, where more than one covariate is used to predict the value of the dependent variable (the <em>Y</em>). The form of the predictor is a linear combination of the set of explanatory variables, i.e.
<span class="math display">\[\widehat Y_i = \beta_0 + \beta_1X_{i1}  + \ldots + \beta_p X_{ip},\quad i=1,\ldots,n,\]</span>
where <span class="math inline">\(\widehat Y_i\)</span> is the predictor of observation <span class="math inline">\(i\)</span>, <span class="math inline">\(\beta_0, \ldots, \beta_p\)</span> are the parameters and <span class="math inline">\(X_{\cdot 1}, \ldots, X_{\cdot p}\)</span> are the vectors of explanatory variables. It is quite common to write the equation above on vector form. Define the vectors <span class="math inline">\(\widehat{\mathbf{Y}}=(\widehat Y_1, \ldots, \widehat Y_n)&#39;\)</span> and <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \ldots, \beta_p)&#39;\)</span>, and the <em>design matrix</em> <span class="math inline">\(\mathbb X = (\mathbf{1}, \mathbf X_{\cdot 1},\ldots, \mathbf X_{\cdot p})\)</span>. Then the equation above can be written as</p>
<p><span class="math display">\[\widehat{\mathbf{Y}} = \mathbb X\,\boldsymbol\beta.\]</span></p>
<p>The assumptions are that the residuals, <span class="math inline">\(Z_i = Y_i - \widehat Y_i\)</span>, <span class="math inline">\(i=1,\ldots, n\)</span> are independent and normally distributed. The explanatory variables should not be correlated. Parameters are estimated using maximum likelihood estimation. Note that we are using capital letters here. This is a statistical convention when we are talking about variables. Once we introduce observations, we switch to small letters for the same quantities.</p>
<p>To learn more about multiple regression, take the datacamp course <a href="https://app.datacamp.com/learn/courses/multiple-and-logistic-regression-in-r"><em>Multiple and Logistic Regression in R</em></a>. For now you can focus on the multiple regression part. We will simply go on to illustrate usage by an example.</p>
<div id="example" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Example</h3>
<p>In this example, we consider a dataset containing the impact of three advertising medias (youtube, facebook and newspaper) on sales for different companies. The advertising budgets and sales are in thousands of dollars and the advertising experiment has been repeated 200 times. We will use multiple regression to model the relationship between sales and the advertising budgets from the different medias. In the video below we walk you through the example, but you can also read it below.</p>
<div style="padding:56.25% 0 0 0;position:relative;">
<iframe src="https://player.vimeo.com/video/690634468?h=816a975dee&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;">
</iframe>
</div>
<script src="https://player.vimeo.com/api/player.js"></script>
<p>We start by looking at the data:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="lecture2.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data: </span></span>
<span id="cb17-2"><a href="lecture2.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;marketing&quot;</span>, <span class="at">package =</span> <span class="st">&quot;datarium&quot;</span>)</span>
<span id="cb17-3"><a href="lecture2.html#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(marketing)</span></code></pre></div>
<pre><code>##   youtube facebook newspaper sales
## 1  276.12    45.36     83.04 26.52
## 2   53.40    47.16     54.12 12.48
## 3   20.64    55.08     83.16 11.16
## 4  181.80    49.56     70.20 22.20
## 5  216.96    12.96     70.08 15.48
## 6   10.44    58.68     90.00  8.64</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="lecture2.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(marketing)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/load_data_marketing-1.png" width="672" />
From looking at the plot above, it does not seem to be a very strong correlation between the three covariates: youtube, facebook and newspaper. This is based on that the first 3x3 scatters plots seem to be randomly distributed without any clear patterns. Looking at the last row of panels we see the marginal relationships between the covariates and the response; sales. Here it looks like the marginal relationship between youtube and facebook variables is close to linear, while for newspaper it does not look like a linear relationship is well suited. We will therefore use facebook and youtube to predict sales. We will also check if including newspaper can improve the fit.</p>
<p>We create a train and a test set, by doing a 80-20 random split (80% for training set - 20% test set):</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="lecture2.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb20-2"><a href="lecture2.html#cb20-2" aria-hidden="true" tabindex="-1"></a>train.ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(marketing), <span class="fu">nrow</span>(marketing)<span class="sc">*</span>.<span class="dv">8</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-3"><a href="lecture2.html#cb20-3" aria-hidden="true" tabindex="-1"></a>trainset <span class="ot">&lt;-</span> marketing[train.ind, ]</span>
<span id="cb20-4"><a href="lecture2.html#cb20-4" aria-hidden="true" tabindex="-1"></a>testset <span class="ot">&lt;-</span> marketing[<span class="sc">-</span>train.ind, ]</span></code></pre></div>
<p>We start by fitting the following model:</p>
<p><span class="math display">\[\mathrm{sales} = \beta_0 + \beta_1\cdot \mathrm{youtube} + \beta_2\cdot \mathrm{facebook}+\beta_3\cdot \mathrm{youtube}\cdot\mathrm{facebook}\]</span>
This model can be set up by different <em>formula</em> arguments in R. The different model calls below are equivalent.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="lecture2.html#cb21-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> youtube <span class="sc">+</span> facebook <span class="sc">+</span> youtube<span class="sc">:</span>facebook, <span class="at">data =</span> trainset)</span>
<span id="cb21-2"><a href="lecture2.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ youtube + facebook + youtube:facebook, data = trainset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.4636 -0.4788  0.2331  0.7317  1.7508 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      8.077e+00  3.375e-01  23.929  &lt; 2e-16 ***
## youtube          1.914e-02  1.702e-03  11.248  &lt; 2e-16 ***
## facebook         2.639e-02  1.003e-02   2.631  0.00937 ** 
## youtube:facebook 9.187e-04  4.956e-05  18.536  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.174 on 156 degrees of freedom
## Multiple R-squared:  0.9655, Adjusted R-squared:  0.9649 
## F-statistic:  1457 on 3 and 156 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="lecture2.html#cb23-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> youtube <span class="sc">+</span> facebook <span class="sc">+</span> youtube<span class="sc">:</span>facebook, <span class="at">data =</span> trainset)</span>
<span id="cb23-2"><a href="lecture2.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ 1 + youtube + facebook + youtube:facebook, 
##     data = trainset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.4636 -0.4788  0.2331  0.7317  1.7508 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      8.077e+00  3.375e-01  23.929  &lt; 2e-16 ***
## youtube          1.914e-02  1.702e-03  11.248  &lt; 2e-16 ***
## facebook         2.639e-02  1.003e-02   2.631  0.00937 ** 
## youtube:facebook 9.187e-04  4.956e-05  18.536  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.174 on 156 degrees of freedom
## Multiple R-squared:  0.9655, Adjusted R-squared:  0.9649 
## F-statistic:  1457 on 3 and 156 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="lecture2.html#cb25-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> youtube <span class="sc">*</span> facebook, <span class="at">data =</span> trainset)</span>
<span id="cb25-2"><a href="lecture2.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ youtube * facebook, data = trainset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.4636 -0.4788  0.2331  0.7317  1.7508 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      8.077e+00  3.375e-01  23.929  &lt; 2e-16 ***
## youtube          1.914e-02  1.702e-03  11.248  &lt; 2e-16 ***
## facebook         2.639e-02  1.003e-02   2.631  0.00937 ** 
## youtube:facebook 9.187e-04  4.956e-05  18.536  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.174 on 156 degrees of freedom
## Multiple R-squared:  0.9655, Adjusted R-squared:  0.9649 
## F-statistic:  1457 on 3 and 156 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>As you can see from the outputs, all models are equivalent. If you want to learn more about setting the formula argument, check out the helpsite for the formula function:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="lecture2.html#cb27-1" aria-hidden="true" tabindex="-1"></a>?stats<span class="sc">::</span>formula</span></code></pre></div>
<p>We will choose the model that has the highest predictive ability. We will therefore suggest several models and choose the one that has the lowest value of Akaike’s information criteria (AIC). We will also evaluate the different models on the test set.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="lecture2.html#cb28-1" aria-hidden="true" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> youtube <span class="sc">*</span> facebook <span class="sc">*</span> newspaper, <span class="at">data =</span> trainset)</span>
<span id="cb28-2"><a href="lecture2.html#cb28-2" aria-hidden="true" tabindex="-1"></a>mod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> youtube <span class="sc">*</span> facebook <span class="sc">+</span> newspaper, <span class="at">data =</span> trainset)</span>
<span id="cb28-3"><a href="lecture2.html#cb28-3" aria-hidden="true" tabindex="-1"></a>mod4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> youtube,   <span class="at">data =</span> trainset)</span>
<span id="cb28-4"><a href="lecture2.html#cb28-4" aria-hidden="true" tabindex="-1"></a>mod5 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> facebook,  <span class="at">data =</span> trainset)</span>
<span id="cb28-5"><a href="lecture2.html#cb28-5" aria-hidden="true" tabindex="-1"></a>mod6 <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> newspaper, <span class="at">data =</span> trainset)</span>
<span id="cb28-6"><a href="lecture2.html#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(mod1, mod2, mod3, mod4, mod5, mod6)</span></code></pre></div>
<pre><code>##      df       AIC
## mod1  5  511.4668
## mod2  9  517.5961
## mod3  6  512.9814
## mod4  3  894.2953
## mod5  3  986.9928
## mod6  3 1043.6815</code></pre>
<p>We choose the model with <strong>lowest AIC value</strong>. As you can see from the output above, this is the model we named <em>mod1</em>. Adding <em>newspaper</em> as covariate will provide the model with more information, but the cost of adding more parameters to be estimated is deemed higher than the benefit of including this information in the model according to AIC.</p>
<p>We can also use the predictive abilities of the models on the test set to choose model. We will restrict ourselves to the top three model based on AIC and use root mean square error (RMSE) to assess the quality of the predictions. First we evaluate the in-sample prediction. That is, we calculate predictions on the training set and summarize by RMSE.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="lecture2.html#cb30-1" aria-hidden="true" tabindex="-1"></a>trainset <span class="sc">%&gt;%</span> </span>
<span id="cb30-2"><a href="lecture2.html#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(</span>
<span id="cb30-3"><a href="lecture2.html#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">mod1 =</span> <span class="fu">predict</span>(mod1, <span class="at">newdata =</span> trainset),</span>
<span id="cb30-4"><a href="lecture2.html#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">mod2 =</span> <span class="fu">predict</span>(mod2, <span class="at">newdata =</span> trainset),</span>
<span id="cb30-5"><a href="lecture2.html#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">mod3 =</span> <span class="fu">predict</span>(mod3, <span class="at">newdata =</span> trainset)</span>
<span id="cb30-6"><a href="lecture2.html#cb30-6" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb30-7"><a href="lecture2.html#cb30-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="dv">5</span><span class="sc">:</span><span class="dv">7</span>, <span class="at">names_to =</span> <span class="st">&quot;models&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;pred&quot;</span>)  <span class="sc">%&gt;%</span></span>
<span id="cb30-8"><a href="lecture2.html#cb30-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(models) <span class="sc">%&gt;%</span></span>
<span id="cb30-9"><a href="lecture2.html#cb30-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">RMSE =</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((sales<span class="sc">-</span>pred)<span class="sc">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## # A tibble: 3 x 2
##   models  RMSE
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 mod1    1.16
## 2 mod2    1.15
## 3 mod3    1.16</code></pre>
<p>Perhaps not very surpising, the most complex model has the lowest in-sample, but the differences seem small. Let’s evaluate the models on the test set as well.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="lecture2.html#cb32-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">=</span>  testset <span class="sc">%&gt;%</span> </span>
<span id="cb32-2"><a href="lecture2.html#cb32-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(</span>
<span id="cb32-3"><a href="lecture2.html#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">mod1 =</span> <span class="fu">predict</span>(mod1, <span class="at">newdata =</span> testset),</span>
<span id="cb32-4"><a href="lecture2.html#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">mod2 =</span> <span class="fu">predict</span>(mod2, <span class="at">newdata =</span> testset),</span>
<span id="cb32-5"><a href="lecture2.html#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">mod3 =</span> <span class="fu">predict</span>(mod3, <span class="at">newdata =</span> testset)</span>
<span id="cb32-6"><a href="lecture2.html#cb32-6" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb32-7"><a href="lecture2.html#cb32-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="dv">5</span><span class="sc">:</span><span class="dv">7</span>, <span class="at">names_to =</span> <span class="st">&quot;models&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;pred&quot;</span>) </span>
<span id="cb32-8"><a href="lecture2.html#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(predictions)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 6
##   youtube facebook newspaper sales models  pred
##     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;
## 1    240.     3.12      25.4  12.7 mod1    13.4
## 2    240.     3.12      25.4  12.7 mod2    13.4
## 3    240.     3.12      25.4  12.7 mod3    13.4
## 4    245.    39.5       55.2  22.8 mod1    22.7
## 5    245.    39.5       55.2  22.8 mod2    22.7
## 6    245.    39.5       55.2  22.8 mod3    22.7</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="lecture2.html#cb34-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="sc">%&gt;%</span></span>
<span id="cb34-2"><a href="lecture2.html#cb34-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(models) <span class="sc">%&gt;%</span></span>
<span id="cb34-3"><a href="lecture2.html#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">RMSE =</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((sales<span class="sc">-</span>pred)<span class="sc">^</span><span class="dv">2</span>)))</span></code></pre></div>
<pre><code>## # A tibble: 3 x 2
##   models  RMSE
##   &lt;chr&gt;  &lt;dbl&gt;
## 1 mod1   0.957
## 2 mod2   0.916
## 3 mod3   0.967</code></pre>
<p>We see that the RMSE is a bit lower for the test set. This indicates that we are not overfitting our models at least, since the models perform better on the test set.</p>
<p>Solely based on the prediction on the test set, we would choose <em>mod2</em>, which is the full model, including <em>youtube</em>, <em>facebook</em> and <em>newspaper</em> with all interaction terms included. Depending on what one will use the model for, one may choose mod1 or mod2. We can look at the summary output of mod2.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="lecture2.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ youtube * facebook * newspaper, data = trainset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.0556 -0.4672  0.2466  0.7246  1.7399 
## 
## Coefficients:
##                              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                 7.933e+00  6.277e-01  12.640  &lt; 2e-16 ***
## youtube                     1.937e-02  2.996e-03   6.466  1.3e-09 ***
## facebook                    1.860e-02  1.872e-02   0.994    0.322    
## newspaper                   8.807e-03  1.945e-02   0.453    0.651    
## youtube:facebook            9.622e-04  9.108e-05  10.564  &lt; 2e-16 ***
## youtube:newspaper          -2.513e-05  8.613e-05  -0.292    0.771    
## facebook:newspaper          2.382e-05  4.514e-04   0.053    0.958    
## youtube:facebook:newspaper -4.273e-07  2.092e-06  -0.204    0.838    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.183 on 152 degrees of freedom
## Multiple R-squared:  0.9659, Adjusted R-squared:  0.9644 
## F-statistic: 615.9 on 7 and 152 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Based on classical statistics, you would fix the non-significant estimates to zero (p-values below 5%) and choose a more parsimoneous model, like <em>mod1</em>. This example shows the difference between modelling for prediction (choose <em>mod2</em>) and modelling for interpretation (choose <em>mod1</em>). We can also look at the observations vs predictions plots for the three models we compared in terms of RMSE. It seems to be very small differences between the predictions from the different models.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="lecture2.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(predictions, <span class="fu">aes</span>(<span class="at">x =</span> pred, <span class="at">y =</span> sales)) <span class="sc">+</span> </span>
<span id="cb38-2"><a href="lecture2.html#cb38-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb38-3"><a href="lecture2.html#cb38-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb38-4"><a href="lecture2.html#cb38-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>models, <span class="at">ncol=</span> <span class="dv">3</span>) <span class="sc">+</span> </span>
<span id="cb38-5"><a href="lecture2.html#cb38-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Predicted&quot;</span>) <span class="sc">+</span> </span>
<span id="cb38-6"><a href="lecture2.html#cb38-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Observed&quot;</span>)</span></code></pre></div>
<p><img src="STAT623-compendium_files/figure-html/fitted_plots_marketing-1.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STAT623 compendium.pdf", "STAT623 compendium.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
