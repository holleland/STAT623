<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 1 Introduction and short recap of R | Data science with R: Applied Predictive Modelling</title>
  <meta name="description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content=" 1 Introduction and short recap of R | Data science with R: Applied Predictive Modelling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 1 Introduction and short recap of R | Data science with R: Applied Predictive Modelling" />
  
  <meta name="twitter:description" content="Course in applied predictive modelling made for teaching STAT623 at University of Bergen" />
  

<meta name="author" content="Sondre Hølleland and Kristian Gundersen" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="lecture2.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">STAT623 Applied predictive modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes-and-objectives"><i class="fa fa-check"></i>Learning outcomes and objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecture-overview"><i class="fa fa-check"></i>Lecture overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#litterature"><i class="fa fa-check"></i>Litterature</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="lecture1.html"><a href="lecture1.html"><i class="fa fa-check"></i><b>1</b> Introduction and short recap of R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="lecture1.html"><a href="lecture1.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="lecture1.html"><a href="lecture1.html#prediction-or-interpretation"><i class="fa fa-check"></i><b>1.1.1</b> Prediction or interpretation?</a></li>
<li class="chapter" data-level="1.1.2" data-path="lecture1.html"><a href="lecture1.html#terminology"><i class="fa fa-check"></i><b>1.1.2</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="lecture1.html"><a href="lecture1.html#recap-of-r"><i class="fa fa-check"></i><b>1.2</b> Recap of R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="lecture1.html"><a href="lecture1.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Installing R and Rstudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="lecture1.html"><a href="lecture1.html#r-community-and-packages"><i class="fa fa-check"></i><b>1.2.2</b> R community and packages</a></li>
<li class="chapter" data-level="1.2.3" data-path="lecture1.html"><a href="lecture1.html#datacamp"><i class="fa fa-check"></i><b>1.2.3</b> Datacamp</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="lecture1.html"><a href="lecture1.html#data-preprocessing"><i class="fa fa-check"></i><b>1.3</b> Data preprocessing</a></li>
<li class="chapter" data-level="1.4" data-path="lecture1.html"><a href="lecture1.html#case-study"><i class="fa fa-check"></i><b>1.4</b> Case study</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="lecture1.html"><a href="lecture1.html#data-transformations-for-individual-predictors"><i class="fa fa-check"></i><b>1.4.1</b> Data transformations for individual predictors</a></li>
<li class="chapter" data-level="1.4.2" data-path="lecture1.html"><a href="lecture1.html#centering-and-scaling"><i class="fa fa-check"></i><b>1.4.2</b> Centering and scaling</a></li>
<li class="chapter" data-level="1.4.3" data-path="lecture1.html"><a href="lecture1.html#tranformations-to-resolve-skewness"><i class="fa fa-check"></i><b>1.4.3</b> Tranformations to resolve skewness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lecture2.html"><a href="lecture2.html"><i class="fa fa-check"></i><b>2</b> Over-fitting and model tuning, selection and evaluation and multiple regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="lecture2.html"><a href="lecture2.html#overfitting"><i class="fa fa-check"></i><b>2.1</b> Overfitting</a></li>
<li class="chapter" data-level="2.2" data-path="lecture2.html"><a href="lecture2.html#training-validation-and-test-split"><i class="fa fa-check"></i><b>2.2</b> Training, validation and test split</a></li>
<li class="chapter" data-level="2.3" data-path="lecture2.html"><a href="lecture2.html#multiple-regression"><i class="fa fa-check"></i><b>2.3</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="lecture2.html"><a href="lecture2.html#example"><i class="fa fa-check"></i><b>2.3.1</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="lecture3.html"><a href="lecture3.html"><i class="fa fa-check"></i><b>3</b> Non-linear regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lecture3.html"><a href="lecture3.html#gam-example"><i class="fa fa-check"></i><b>3.1</b> GAM example</a>
<ul>
<li class="chapter" data-level="" data-path="lecture3.html"><a href="lecture3.html#data-camp"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture4.html"><a href="lecture4.html"><i class="fa fa-check"></i><b>4</b> Classification methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lecture4.html"><a href="lecture4.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>4.1</b> k-nearest neighbor</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-1"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="lecture4.html"><a href="lecture4.html#logistic-regression"><i class="fa fa-check"></i><b>4.2</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-2"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="lecture4.html"><a href="lecture4.html#naive-bayes"><i class="fa fa-check"></i><b>4.3</b> Naive bayes</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#example-3"><i class="fa fa-check"></i>Example</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="lecture4.html"><a href="lecture4.html#wrap-up"><i class="fa fa-check"></i><b>4.4</b> Wrap-up</a>
<ul>
<li class="chapter" data-level="" data-path="lecture4.html"><a href="lecture4.html#data-camp-1"><i class="fa fa-check"></i>Data camp</a></li>
<li class="chapter" data-level="4.4.1" data-path="lecture4.html"><a href="lecture4.html#sources"><i class="fa fa-check"></i><b>4.4.1</b> Sources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture5.html"><a href="lecture5.html"><i class="fa fa-check"></i><b>5</b> Decision Trees and Bagged Trees</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lecture5.html"><a href="lecture5.html#decision-trees"><i class="fa fa-check"></i><b>5.1</b> Decision trees</a></li>
<li class="chapter" data-level="5.2" data-path="lecture5.html"><a href="lecture5.html#bagged-trees"><i class="fa fa-check"></i><b>5.2</b> Bagged trees</a>
<ul>
<li class="chapter" data-level="" data-path="lecture5.html"><a href="lecture5.html#data-camp-2"><i class="fa fa-check"></i>Data camp</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lecture6.html"><a href="lecture6.html"><i class="fa fa-check"></i><b>6</b> Random forrest and boosting</a></li>
<li class="chapter" data-level="7" data-path="lecture7.html"><a href="lecture7.html"><i class="fa fa-check"></i><b>7</b> Artificial Neural networks</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lecture7.html"><a href="lecture7.html#mlp"><i class="fa fa-check"></i><b>7.1</b> MLP</a></li>
<li class="chapter" data-level="7.2" data-path="lecture7.html"><a href="lecture7.html#objective-functions-and-training"><i class="fa fa-check"></i><b>7.2</b> Objective functions and training</a></li>
<li class="chapter" data-level="7.3" data-path="lecture7.html"><a href="lecture7.html#validation-of-trained-models"><i class="fa fa-check"></i><b>7.3</b> Validation of trained models</a></li>
<li class="chapter" data-level="7.4" data-path="lecture7.html"><a href="lecture7.html#implementation-of-mlp-in-keras"><i class="fa fa-check"></i><b>7.4</b> Implementation of MLP in Keras</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="lecture7.html"><a href="lecture7.html#installing-keras-and-tensorflow"><i class="fa fa-check"></i><b>7.4.1</b> Installing Keras and Tensorflow</a></li>
<li class="chapter" data-level="7.4.2" data-path="lecture7.html"><a href="lecture7.html#importing-mnist"><i class="fa fa-check"></i><b>7.4.2</b> Importing MNIST</a></li>
<li class="chapter" data-level="7.4.3" data-path="lecture7.html"><a href="lecture7.html#preprocessing"><i class="fa fa-check"></i><b>7.4.3</b> Preprocessing</a></li>
<li class="chapter" data-level="7.4.4" data-path="lecture7.html"><a href="lecture7.html#mlp-model"><i class="fa fa-check"></i><b>7.4.4</b> MLP model</a></li>
<li class="chapter" data-level="7.4.5" data-path="lecture7.html"><a href="lecture7.html#compiling-and-training-the-model"><i class="fa fa-check"></i><b>7.4.5</b> Compiling and training the model</a></li>
<li class="chapter" data-level="7.4.6" data-path="lecture7.html"><a href="lecture7.html#evaluating-the-model"><i class="fa fa-check"></i><b>7.4.6</b> Evaluating the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture8.html"><a href="lecture8.html"><i class="fa fa-check"></i><b>8</b> Support vector machines</a></li>
<li class="chapter" data-level="9" data-path="lecture9.html"><a href="lecture9.html"><i class="fa fa-check"></i><b>9</b> Feature selection/Explainable AI</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data science with R: Applied Predictive Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture1" class="section level1" number="1">
<h1><span class="header-section-number"> 1</span> Introduction and short recap of R</h1>
<p>Goals: In this <a href="lecture1.html#lecture1">Lecture 1</a> we will give a introduction to the course and a short recap of using R.</p>
<div id="introduction" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>This course is called <em>applied predictive modelling</em>. What do we actually mean by these three words? Well, applied is as opposed to <em>theoretical</em> - indicating that this will not be a very theoretical course. We will focus on usage of the different methods that you will learn throughout the course. <em>Predictive modelling</em> means that the overall goal of the modelling is to predict something or make a prediction. <span class="citation"><a href="#ref-gkisser1993" role="doc-biblioref">Gkisser</a> (<a href="#ref-gkisser1993" role="doc-biblioref">1993</a>)</span> defines predictive modelling as “the process by which a model is created or chosen to try to best predict the probability of an outcome,” while <span class="citation"><a href="#ref-kuhn2016" role="doc-biblioref">Kuhn and Johnson</a> (<a href="#ref-kuhn2016" role="doc-biblioref">2016</a>)</span> define it as “the process of developing a mathematical tool or model that generates an accurate prediction” and the give the following examples of types of questions one could be interested in predicting:</p>
<ul>
<li>How many copies will a book sell?</li>
<li>Will this customer move their buisness to a different company?</li>
<li>How much will my house sell for in the current market?</li>
<li>Does a patient have a specific disease?</li>
<li>Based on past choices, which movies will interest this viewer?</li>
<li>Should I sell this stock?</li>
<li>Which people should we match in our online dating service?</li>
<li>Is an email spam?</li>
<li>Will this patient respond to this therapy?</li>
</ul>
<p>Examples of stakeholders or users of predictive modelling can be insurance companies. They need to quantify individual risk of potential policy holders. If the risk is too high, they may not offer the potential customer insurance or they may use the quantified risk to set the insurance premium. Governments may use predictive models to detect fraud or identify terror suspects. When you go to a grocery store and sign up for some discounts, usually either by creating an account or registering your credit card, your purchase information is being collected and analyzed in an attempt to who you are and what you want.</p>
<p>Predictive modelling can often a good thing. When for instance Netflix learns what kind of TV-shows you prefer to watch, they can come up with suggestions for other TV-shows that you are likely to also enjoy. This can often be recognized by statements like “people who liked TV-series A, also liked TV-series B.” For the streaming provider the goal is to keep you as an entertained, satisfied and paying customer, but it is also in your interest to find entertainment that you like. But you have probably also noticed that sometimes, these predictions are quite inaccurate and provide the wrong answer. For instance, you did not like the suggested series or you did not receive an important email because it was wrongly classified as spam by (predictive) spam filter.</p>
<p><span class="citation"><a href="#ref-kuhn2016" role="doc-biblioref">Kuhn and Johnson</a> (<a href="#ref-kuhn2016" role="doc-biblioref">2016</a>)</span> give four reasons for why predictive models fail:</p>
<ol style="list-style-type: decimal">
<li>Inadequate pre-processing of the data.</li>
<li>Inadequate model validation.</li>
<li>Unjustified extrapolation.</li>
<li>Over-fitting the model to existing data.</li>
</ol>
<p>They also mention the fact that modellers tend to explore realtively few models when searching for predictive realationships. This is usually because the modeler has a preference for a certain type of model (the one he/she has used before and know well). It can also be due to software availability. We will cover <strong>(at least)</strong> some of these aspects in this course.</p>
<div id="prediction-or-interpretation" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Prediction or interpretation?</h3>
<p>In the examples listed above, the main goal is to predict something and there are likely data available to train a statistical model to do so in most of the cases. Note that we are not so interested in answering questions of <em>why</em> something happens or not. Our primary interest is to accurately predict the probability that something will, or will not, happen. In situations like these, we should not be worried with having interpretative models, and focus our attention on prediction accuracy. If your spam filter classifies an email as spam, you would not care why the filter did so, as long as you receive the emails you care about and not the ones you do not. An interpretative model can be a model for a certain stock’s value that can support statements like “the stock value prediction went up, because the company released information about a big contract.” One can explain why the model behaves the way it does. The alternative is often called a “black box,” meaning that you put your data in on one side of the box and on the other the prediction pops out, without you knowing what happened inside the box.</p>
<p>So, our primary interest is prediction accuracy. Cannot interpretability be our secondary target, so we can understand why it works? <span class="citation"><a href="#ref-kuhn2016" role="doc-biblioref">Kuhn and Johnson</a> (<a href="#ref-kuhn2016" role="doc-biblioref">2016</a>)</span> writes that “the unfortunate reality is that as we push towards higher accuracy, models become more complex and their interpretability becomes more difficult.”</p>
</div>
<div id="terminology" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Terminology</h3>
<p>In the predictive modelling terminology there are quite a lot of things that mean the same thing. We will list some terminology here with some explanations (see <span class="citation"><a href="#ref-kuhn2016" role="doc-biblioref">Kuhn and Johnson</a> (<a href="#ref-kuhn2016" role="doc-biblioref">2016, p6</a>)</span>):</p>
<ul>
<li>The terms <em>sample</em>, <em>data point</em>, <em>observation</em>, or <em>instance</em> refer to a single, independent unit of data, e.g. a customer, a patient, a transaction, an individual. Sample can also refer to a collection of data points (a subset).</li>
<li>The <em>training set</em> consists of data used to develop the models while the <em>test</em> and <em>validation sets</em> are used solely for evaluating the performance of the final model.</li>
<li>The <em>predictors</em>, <em>idendependent variables</em>, <em>attributes</em>, <em>descriptors</em>, or <em>covariates</em> are the data used as input for the prediction equation (e.g. what you feed into the black box).</li>
<li><em>Outcome</em>, <em>dependent variable</em>, <em>target</em>, <em>class</em>, or <em>response</em> refer to the outcome of the event or qunatity that is being predicted (e.g. what comes out of the black box).</li>
<li><em>Continuous data</em> have natural, numeric scales (e.g. blood pressure, price of an item, a persons body mass index, number of bathrooms, etc).</li>
<li><em>Categorical</em>, <em>nominal</em>, <em>attribute</em> or <em>discrete</em> data all mean the same thing. These are data types that take on specific values that have no scale (e.g. credit status (“good” or “bad”) or color (“red,” “white,” “blue”))</li>
<li><em>Model building</em>, <em>model training</em>, and <em>parameter estimation</em> all refer to the process of using data to determine values of model equations.</li>
</ul>
</div>
</div>
<div id="recap-of-r" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Recap of R</h2>
<div id="installing-r-and-rstudio" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Installing R and Rstudio</h3>
<p>If you have not already installed the newest version of R and Rstudio on your personal computer, we will now tell you how. This installation guide will be based on Windows, but iOS and Linux are very similar and we will indicate where to deviate. The video below shows the same steps as listed.</p>
<div style="padding:56.25% 0 0 0;position:relative;">
<iframe src="https://player.vimeo.com/video/690661471?h=356d0a51fa&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;">
</iframe>
</div>
<script src="https://player.vimeo.com/api/player.js"></script>
<p>First we install R:</p>
<ol style="list-style-type: decimal">
<li>Go to <a href="https://cran.uib.no">https://cran.uib.no</a>.</li>
<li>Select Download R for Windows (or macOS/Linux) (this will be the newest R version).</li>
<li>Go to your “Downloads” folder and run the .exe file.</li>
<li>Use standard settings throughout the installation steps (click next-next-next-etc.).</li>
<li>R is now installed.</li>
</ol>
<p>Now that R is installed, we can install Rstudio:</p>
<ol style="list-style-type: decimal">
<li>Go to <a href="https://www.rstudio.com/products/rstudio/download/">https://www.rstudio.com/products/rstudio/download/</a>.</li>
<li>Press “Download” under the Rstudio Desktop - Open source licenece - Free.</li>
<li>Press “Download Rstudio for Windows.” I guess if you have macOS/Linux it will detect that automatically. If not, select it manually from the list below.</li>
<li>Go to your “Downloads” folder and run the Rstudio.exe file.</li>
<li>Use standard settings throughout the installation steps (click next-next-next-etc.).</li>
<li>RStudio is now installed.</li>
</ol>
<p>Open RStudio and check that the print-out in the console matches the R version you just installed. At the time when this is written the newest version is 4.1.1 (2021-08-10) “Kick Things.” The printout at the top looks like this:</p>
<p>R version 4.1.1 (2021-08-10) – “Kick Things”<br />
Copyright (C) 2021 The R Foundation for Statistical Computing<br />
Platform: i386-w64-mingw32/i386 (32-bit)</p>
<p>If you had R and Rstudio already, but updated to the newest version, remember that you will have to also install all your packages again.</p>
</div>
<div id="r-community-and-packages" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> R community and packages</h3>
<p>Currently, there are over 18 000 packages available on the official CRAN server - open source and free. If you have a problem and need a smart function for solving that, you will most likely find that someone else have had the same problem, already solved it and made an R package that you can use. The large R community is also very active on the net forum stackexchange.com. If you have a programming issue and google it, you will typically end up in one of these forums where someone else has posted a question similar to yours and others from the community has provided a solution. I have been using R for over 10 years now and this strategy for solving programming issues has not failed me yet.</p>
<p>Once you have found an R package you want to install, you can install it using the <code>install.packages</code> function. We will use the <code>tidyverse</code> package developed by <span class="citation"><a href="#ref-tidyverse" role="doc-biblioref">Wickham et al.</a> (<a href="#ref-tidyverse" role="doc-biblioref">2019</a>)</span> as an example. This is a bundling package built up of many packages, which we will use throughout the course.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="lecture1.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tidyverse&quot;</span>)</span></code></pre></div>
<p>To load a package into you R environment, making all its functionality available to you, you can use the <code>library</code> or <code>require</code> functions. The two are quite equal, but if you do not have the package installed <code>require</code> will cast a warning and continue, while <code>library</code> will cast an error and stop.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="lecture1.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## -- Attaching packages -------------------------------------------------------------------------------- tidyverse 1.3.1 --</code></pre>
<pre><code>## v tibble  3.1.6     v dplyr   1.0.8
## v tidyr   1.2.0     v stringr 1.4.0
## v readr   2.1.2     v forcats 0.5.1
## v purrr   0.3.4</code></pre>
<pre><code>## -- Conflicts ----------------------------------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<p>As part of the output here, you can see all the packages that tidyverse attaches to your working environment. We will be using many of these.</p>
</div>
<div id="datacamp" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Datacamp</h3>
<p>As a part of the course, you will be given access to Datacamp. We will suggest courses to take there in combination with the lectures provided.</p>
<p>If you want a more thorough recap of using R, you can take the course <a href="https://app.datacamp.com/learn/courses/free-introduction-to-r">Introduction to R</a> or test yourself with the practice module <a href="https://practice.datacamp.com/p/2">Introduction to R</a>.</p>
</div>
</div>
<div id="data-preprocessing" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Data preprocessing</h2>
<p><em>Data pre-processing</em> techniques typically involve addition, deletion or transformation of training set data. Preparing the data in a good way can make or break the predictive ability of a model. How the predictors enter the model is important. By transforming the data before feeding them to the model one may reduce the impact of data skewness or outliers. This can signifcantly improve the model’s performance. The need for data pre-processing may depend on the method you are using. A tree-based model is notably insensitive to the characteristics of the predictor data, while a linear regression is not.</p>
<p>How the predictors are encoded, called <em>Feature engineering</em>, can also have a huge impact on the model performance. There are many ways to encode the same information. Combining multiple predictors can sometimes be more effective than using the individual ones. For example, using the <em>body mass index</em> (BMI = weight/length<span class="math inline">\({}^2\)</span>) instead of using length and weight as separate predictors. The modeler’s understanding of the problem can often help in choosing the most effective encoding.</p>
<p>Different ways of encoding a date predictor can be</p>
<ul>
<li>The number of days since a reference date</li>
<li>Using the month, year and day of the week as separate predictors</li>
<li>The numeric day of the year (julian day)</li>
<li>Whether the date was within the school year (as opposed to holiday)</li>
</ul>
</div>
<div id="case-study" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Case study</h2>
<p>We will use the same example as <span class="citation"><a href="#ref-kuhn2016" role="doc-biblioref">Kuhn and Johnson</a> (<a href="#ref-kuhn2016" role="doc-biblioref">2016</a>)</span> from the R package</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="lecture1.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;AppliedPredictiveModeling&quot;</span>)</span></code></pre></div>
<div id="data-transformations-for-individual-predictors" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Data transformations for individual predictors</h3>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="lecture1.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(AppliedPredictiveModeling)</span>
<span id="cb7-2"><a href="lecture1.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;segmentationOriginal&quot;</span>)</span>
<span id="cb7-3"><a href="lecture1.html#cb7-3" aria-hidden="true" tabindex="-1"></a>segData <span class="ot">&lt;-</span> <span class="fu">subset</span>(segmentationOriginal, Case <span class="sc">==</span> <span class="st">&quot;Train&quot;</span>)</span></code></pre></div>
</div>
<div id="centering-and-scaling" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Centering and scaling</h3>
<p>The most straightforward and common transformation of data is to center and scale the predictor variables. Let <span class="math inline">\(\textbf{x}\)</span> be the predictor, <span class="math inline">\(\overline{x}\)</span> its average value and <span class="math inline">\(sd(x)\)</span> it’s standard deviation. Centering means to substract the average predictor value from all the predictor values (<span class="math inline">\(\textbf{e} = \textbf{x}-\overline{x}\)</span>) and scaling means dividing all values by the empirical standard deviation of the predictor (<span class="math inline">\(\textbf{e}/sd(x)\)</span>). By centering the predictor, the predictor has zero mean and by scaling, you coerce the value to have a common standard deviation of one. A common term for a centered and scaled predictor is a <em>standarized</em> predictor, indicating that if you do this to all predictors they are all on a common scale. In the Figure <a href="lecture1.html#fig:scalingfig">1.1</a> below, we have simulated 1000 normally distributed observations with mean 22 and standard deviation 4 and plotted a histogram of them (left panel) and a histogram of them standardized (right panel). As you can see, the distribution looks the same, but the scale on the x-axes has changed.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="lecture1.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-2"><a href="lecture1.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb8-3"><a href="lecture1.html#cb8-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="at">mean =</span> <span class="dv">22</span>, <span class="at">sd =</span> <span class="dv">4</span>)</span>
<span id="cb8-4"><a href="lecture1.html#cb8-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(x, (x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">/</span><span class="fu">sd</span>(x)),</span>
<span id="cb8-5"><a href="lecture1.html#cb8-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">mean =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">22</span>, <span class="dv">0</span>),<span class="at">each =</span> <span class="dv">1000</span>),</span>
<span id="cb8-6"><a href="lecture1.html#cb8-6" aria-hidden="true" tabindex="-1"></a>                 <span class="at">type =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Original&quot;</span>, <span class="st">&quot;Standardized&quot;</span>), <span class="at">each =</span> <span class="dv">1000</span>))</span>
<span id="cb8-7"><a href="lecture1.html#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span> df,<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> type)) <span class="sc">+</span> </span>
<span id="cb8-8"><a href="lecture1.html#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>() <span class="sc">+</span> </span>
<span id="cb8-9"><a href="lecture1.html#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>type, <span class="at">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="sc">+</span></span>
<span id="cb8-10"><a href="lecture1.html#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> mean), <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb8-11"><a href="lecture1.html#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">expand =</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb8-12"><a href="lecture1.html#cb8-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.title =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb8-13"><a href="lecture1.html#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)<span class="sc">+</span></span>
<span id="cb8-14"><a href="lecture1.html#cb8-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:scalingfig"></span>
<img src="STAT623-compendium_files/figure-html/scalingfig-1.png" alt="Simulated normally distributed variables on original- and standardized scale." width="672" />
<p class="caption">
Figure1.1: Simulated normally distributed variables on original- and standardized scale.
</p>
</div>
</div>
<div id="tranformations-to-resolve-skewness" class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Tranformations to resolve skewness</h3>
<p>As noted above, standardizing preserves the distribution, but sometimes we need to remove distributional skewness. An un-skewed distribution is roughly symmetric around the mean. A right-skewed distribution has a larger probability of falling on the left side (i.e. small values) than on the right side (large values). We have illustrated left-, right- and un-skewed normal distributions in Figure <a href="lecture1.html#fig:skewedDist">1.2</a>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="lecture1.html#cb9-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="at">by =</span> .<span class="dv">1</span>)</span>
<span id="cb9-2"><a href="lecture1.html#cb9-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">0</span>,<span class="dv">5</span>)</span>
<span id="cb9-3"><a href="lecture1.html#cb9-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="st">&quot;x&quot;</span> <span class="ot">=</span> x,<span class="st">&quot;alpha&quot;</span> <span class="ot">=</span> alpha)</span>
<span id="cb9-4"><a href="lecture1.html#cb9-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">case =</span> <span class="fu">ifelse</span>(alpha <span class="sc">==</span> <span class="sc">-</span><span class="dv">5</span>, <span class="st">&quot;Left skewed&quot;</span>,</span>
<span id="cb9-5"><a href="lecture1.html#cb9-5" aria-hidden="true" tabindex="-1"></a>                                  <span class="fu">ifelse</span>(alpha <span class="sc">==</span><span class="dv">5</span>, <span class="st">&quot;Right skewed&quot;</span>, </span>
<span id="cb9-6"><a href="lecture1.html#cb9-6" aria-hidden="true" tabindex="-1"></a>                                         <span class="st">&quot;Unskewed&quot;</span>)),</span>
<span id="cb9-7"><a href="lecture1.html#cb9-7" aria-hidden="true" tabindex="-1"></a>                    <span class="at">delta =</span> alpha<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span><span class="sc">+</span>alpha<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb9-8"><a href="lecture1.html#cb9-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">omega =</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1-2</span><span class="sc">*</span>delta<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>pi),</span>
<span id="cb9-9"><a href="lecture1.html#cb9-9" aria-hidden="true" tabindex="-1"></a>                    <span class="at">dens =</span> sn<span class="sc">::</span><span class="fu">dsn</span>(x,<span class="at">xi =</span> <span class="sc">-</span>omega<span class="sc">*</span>delta<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">/</span>pi),  <span class="at">alpha =</span>alpha, <span class="at">omega =</span> omega))</span>
<span id="cb9-10"><a href="lecture1.html#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x,<span class="at">y =</span> dens)) <span class="sc">+</span> </span>
<span id="cb9-11"><a href="lecture1.html#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb9-12"><a href="lecture1.html#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>( <span class="sc">~</span>case, <span class="at">scales =</span> <span class="st">&quot;fixed&quot;</span>, <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">strip.position =</span> <span class="st">&quot;right&quot;</span>) <span class="sc">+</span></span>
<span id="cb9-13"><a href="lecture1.html#cb9-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb9-14"><a href="lecture1.html#cb9-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:skewedDist"></span>
<img src="STAT623-compendium_files/figure-html/skewedDist-1.png" alt="Skewed normal distributions with mean 0 and standard deviation 1." width="672" />
<p class="caption">
Figure1.2: Skewed normal distributions with mean 0 and standard deviation 1.
</p>
</div>
<p>A rule of thumb to consider is that skewed data whose ratio of the highest value to the lowest value is greater than 20 have significant skewness. One can calculate the skewness statistic and use that as a diagnostic. Is it close to zero, the distribution if roughly symmetric. Is it large, indicates right-skewness and negative values indicates left-skewness. The formula for the sample skewness statistic is given by</p>
<p><span class="math display">\[\text{skewness} = \frac{\sum_{i=1}^n (x_i-\overline{x})^3}{(n-1)v^{3/2}},\quad \text{where}\quad v = (n-1)^{-1}\sum_{i=1}^n(x_i-\overline{x})^2,\]</span></p>
<p><span class="math inline">\(x\)</span> is the predictor variable, <span class="math inline">\(n\)</span> the number of values and <span class="math inline">\(\overline{x}\)</span> is sample mean.</p>
<p>Transforming the predictor with the log, square root or inverse may help remove the skew. Alternatively, a Box-Cox transformation can be used. The Box-Cox family of transformations are indexed by a parameter denoted <span class="math inline">\(\lambda\)</span> and defined by</p>
<p><span class="math display">\[x^\star = \begin{cases}
    \frac{x^\lambda-1}\lambda, &amp; \text{if }\lambda\neq 0\\
    \log(x), &amp; \text{if }\lambda = 0.
\end{cases}\]</span></p>
<p>In addition to the log transformation, the family can identify square transformation (<span class="math inline">\(\lambda = 2\)</span>), square root (<span class="math inline">\(\lambda = 0.5\)</span>), inverse (<span class="math inline">\(\lambda = -1\)</span>) and other in-between. By using maximum likelihood estimation on the training data, <span class="math inline">\(\lambda\)</span> can be estimated for each predictor that contain values greater than zero.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-gkisser1993" class="csl-entry">
Gkisser, Seymour. 1993. <em>Predictive Inference: <span>A</span>n Introduction</em>. Chapman; Hall/CRC.
</div>
<div id="ref-kuhn2016" class="csl-entry">
Kuhn, Max, and Kjell Johnson. 2016. <em>Applied Predictive Modeling</em>. 5th ed. Springer.
</div>
<div id="ref-tidyverse" class="csl-entry">
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. <span>“Welcome to the <span class="nocase">tidyverse</span>.”</span> <em>Journal of Open Source Software</em> 4 (43): 1686. <a href="https://doi.org/10.21105/joss.01686">https://doi.org/10.21105/joss.01686</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["STAT623 compendium.pdf", "STAT623 compendium.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
